Automatically generated by Mendeley 1.5.2
Any changes to this file will be lost if it is regenerated by Mendeley.

@article{Yang2012,
abstract = {Error Correction is important for most next-generation sequencing applications because highly accurate sequenced reads will likely lead to higher quality results. Many techniques for error correction of sequencing data from next-gen platforms have been developed in the recent years. However, compared with the fast development of sequencing technologies, there is a lack of standardized evaluation procedure for different error-correction methods, making it difficult to assess their relative merits and demerits. In this article, we provide a comprehensive review of many error-correction methods, and establish a common set of benchmark data and evaluation criteria to provide a comparative assessment. We present experimental results on quality, run-time, memory usage and scalability of several error-correction methods. Apart from providing explicit recommendations useful to practitioners, the review serves to identify the current state of the art and promising directions for future research. Availability: All error-correction programs used in this article are downloaded from hosting websites. The evaluation tool kit is publicly available at: http://aluru-sun.ece.iastate.edu/doku.php?id=ecr.},
author = {Yang, X and Chockalingam, S P and Aluru, S},
doi = {10.1093/bib/bbs015},
file = {:home/gus/Documents/Mendeley Desktop/Yang, Chockalingam, Aluru\_2012\_A survey of error-correction methods for next-generation sequencing.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Yang, Chockalingam, Aluru\_2012\_A survey of error-correction methods for next-generation sequencing(2).pdf:pdf},
isbn = {1467546314774054},
issn = {14675463},
journal = {Briefings in Bioinformatics},
keywords = {error correction,next generation sequencing,sequence analysis},
pages = {bbs015--},
publisher = {Oxford University Press},
title = {{A survey of error-correction methods for next-generation sequencing}},
url = {http://bib.oxfordjournals.org/cgi/doi/10.1093/bib/bbs015 http://bib.oxfordjournals.org/content/early/2012/04/06/bib.bbs015.short},
year = {2012}
}
@article{Purcell2005,
abstract = {We present the first non-blocking hashtable based on open addressing that provides the following benefits: it combines good cache locality, accessing a single cacheline if there are no collisions, with short straight-line code; it needs no storage overhead for pointers and memory allocator schemes, having instead an overhead of two words per bucket; it does not need to periodically reorganise or replicate the table; and it does not need garbage collection, even with arbitrary-sized keys. Open problems include resizing the table and replacing, rather than erasing, entries. The result is a highly-concurrent set algorithm that approaches or outperforms the best externally- chained implementations we tested, with fixed memory costs and no need to select or fine-tune a garbage collector or locking strategy.},
author = {Purcell, Chris and Harris, Tim},
file = {:home/gus/Documents/Mendeley Desktop/Purcell, Harris\_2005\_Non-blocking hatables with open addressing.pdf:pdf},
journal = {University of Cambridge Technical Report},
keywords = {hash table,lock free},
number = {639},
title = {{Non-blocking hatables with open addressing}},
year = {2005}
}
@article{Hallett2004,
author = {Hallett, M T},
file = {:home/gus/Documents/Mendeley Desktop/Hallett\_2004\_Identifying Lateral Gene Transfers.pdf:pdf},
keywords = {blocking loop},
mendeley-tags = {blocking loop},
pages = {1--29},
title = {{Identifying Lateral Gene Transfers}},
year = {2004}
}
@article{Wetzel2011,
abstract = {Next-generation sequencing technologies allow genomes to be sequenced more quickly and less expensively than ever before. However, as sequencing technology has improved, the difficulty of de novo genome assembly has increased, due in large part to the shorter reads generated by the new technologies. The use of mated sequences (referred to as mate-pairs) is a standard means of disambiguating assemblies to obtain a more complete picture of the genome without resorting to manual finishing. Here, we examine the effectiveness of mate-pair information in resolving repeated sequences in the DNA (a paramount issue to overcome). While it has been empirically accepted that mate-pairs improve assemblies, and a variety of assemblers use mate-pairs in the context of repeat resolution, the effectiveness of mate-pairs in this context has not been systematically evaluated in previous literature.},
author = {Wetzel, Joshua and Kingsford, Carl and Pop, Mihai},
doi = {10.1186/1471-2105-12-95},
file = {:home/gus/Documents/Mendeley Desktop/Wetzel, Kingsford, Pop\_2011\_Assessing the benefits of using mate-pairs to resolve repeats in de novo short-read prokaryotic assemblies.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Bacteria,Bacteria: classification,Bacteria: genetics,Bacterial,DNA,DNA: methods,Genome, Bacterial,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
month = jan,
number = {1},
pages = {95},
pmid = {21486487},
title = {{Assessing the benefits of using mate-pairs to resolve repeats in de novo short-read prokaryotic assemblies.}},
url = {http://www.biomedcentral.com/1471-2105/12/95},
volume = {12},
year = {2011}
}
@article{Panda2010,
annote = {10.1038/nature08696},
author = {Li, Ruiqiang and Fan, Wei and Tian, Geng and Zhu, Hongmei and He, Lin and Cai, Jing and Huang, Quanfei and Cai, Qingle and Li, Bo and Bai, Yinqi and Zhang, Zhihe and Zhang, Yaping and Wang, Wen and Li, Jun and Wei, Fuwen and Li, Heng and Jian, Min and Li, Jianwen and Zhang, Zhaolei and Nielsen, Rasmus and Li, Dawei and Gu, Wanjun and Yang, Zhentao and Xuan, Zhaoling and Ryder, Oliver A and Leung, Frederick Chi-Ching and Zhou, Yan and Cao, Jianjun and Sun, Xiao and Fu, Yonggui and Fang, Xiaodong and Guo, Xiaosen and Wang, Bo and Hou, Rong and Shen, Fujun and Mu, Bo and Ni, Peixiang and Lin, Runmao and Qian, Wubin and Wang, Guodong and Yu, Chang and Nie, Wenhui and Wang, Jinhuan and Wu, Zhigang and Liang, Huiqing and Min, Jiumeng and Wu, Qi and Cheng, Shifeng and Ruan, Jue and Wang, Mingwei and Shi, Zhongbin and Wen, Ming and Liu, Binghang and Ren, Xiaoli and Zheng, Huisong and Dong, Dong and Cook, Kathleen and Shan, Gao and Zhang, Hao and Kosiol, Carolin and Xie, Xueying and Lu, Zuhong and Zheng, Hancheng and Li, Yingrui and Steiner, Cynthia C and Lam, Tommy Tsan-Yuk and Lin, Siyuan and Zhang, Qinghui and Li, Guoqing and Tian, Jing and Gong, Timing and Liu, Hongde and Zhang, Dejin and Fang, Lin and Ye, Chen and Zhang, Juanbin and Hu, Wenbo and Xu, Anlong and Ren, Yuanyuan and Zhang, Guojie and Bruford, Michael W and Li, Qibin and Ma, Lijia and Guo, Yiran and An, Na and Hu, Yujie and Zheng, Yang and Shi, Yongyong and Li, Zhiqiang and Liu, Qing and Chen, Yanling and Zhao, Jing and Qu, Ning and Zhao, Shancen and Tian, Feng and Wang, Xiaoling and Wang, Haiyin and Xu, Lizhi and Liu, Xiao and Vinar, Tomas and Wang, Yajun and Lam, Tak-Wah and Yiu, Siu-Ming and Liu, Shiping and Zhang, Hemin and Li, Desheng and Huang, Yan and Wang, Xia and Yang, Guohua and Jiang, Zhi and Wang, Junyi and Qin, Nan and Li, Li and Li, Jingxiang and Bolund, Lars and Kristiansen, Karsten and Wong, Gane Ka-Shu and Olson, Maynard and Zhang, Xiuqing and Li, Songgang and Yang, Huanming and Wang, Jian and Wang, Jun},
doi = {http://dx.doi.org/10.1038/nature08696},
issn = {0028-0836},
journal = {Nature},
number = {7279},
pages = {311--317},
publisher = {Macmillan Publishers Limited. All rights reserved},
title = {{The sequence and de novo assembly of the giant panda genome}},
volume = {463},
year = {2010}
}
@article{Gibbs2012,
author = {Gibbs, Richard A and Rogers, Jeffrey},
doi = {10.1038/483164a},
file = {:home/gus/Documents/Mendeley Desktop/Gibbs, Rogers\_2012\_Genomics Gorilla gorilla gorilla.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
month = mar,
number = {7388},
pages = {164--5},
pmid = {22398552},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nature},
title = {{Genomics: Gorilla gorilla gorilla.}},
url = {http://dx.doi.org/10.1038/483164a},
volume = {483},
year = {2012}
}
@article{Manber1990,
author = {Manber, Udi and Myers, Gene},
file = {:home/gus/Documents/Mendeley Desktop/Manber, Myers\_1990\_Suffix arrays a new method for on-line string searches.pdf:pdf},
isbn = {0-89871-251-3},
month = jan,
pages = {319--327},
title = {{Suffix arrays: a new method for on-line string searches}},
url = {http://dl.acm.org/citation.cfm?id=320176.320218},
year = {1990}
}
@article{Kim:2007uq,
abstract = {One of the main goals in genome sequencing projects is to determine a haploid consensus sequence even when clone libraries are constructed from homologous chromosomes. However, it has been noticed that haplotypes can be inferred from genome assemblies by investigating phase conservation in sequenced reads. In this study, we seek to infer haplotypes, a diploid consensus sequence, from the genome assembly of an organism, Ciona intestinalis. The Ciona intestinalis genome is an ideal resource from which haplotypes can be inferred because of the high polymorphism rate (1.2\%). The haplotype estimation scheme consists of polymorphism detection and phase estimation. The core step of our method is a Gibbs sampling procedure. The mate-pair information from two-end sequenced clone inserts is exploited to provide long-range continuity. We estimate the polymorphism rate of Ciona intestinalis to be 1.2\% and 1.5\%, according to two different polymorphism counting schemes. The distribution of heterozygosity number is well fit by a compound Poisson distribution. The N50 length of haplotype segments is 37.9 kb in our assembly, while the N50 scaffold length of the Ciona intestinalis assembly is 190 kb. We also infer diploid gene sequences from haplotype segments. According to our reconstruction, 85.4\% of predicted gene sequences are continuously covered by single haplotype segments. Our results indicate 97\% accuracy in haplotype estimation, based on a simulated data set. We conduct a comparative analysis with Ciona savignyi, and discover interesting patterns of conserved DNA elements in chordates.},
author = {Kim, Jong Hyun and Waterman, Michael S and Li, Lei M},
doi = {10.1101/gr.5894107},
file = {:home/gus/Documents/Mendeley Desktop/Kim, Waterman, Li\_2007\_Diploid genome reconstruction of Ciona intestinalis and comparative analysis with Ciona savignyi.pdf:pdf},
journal = {Genome Res},
month = jul,
number = {7},
pages = {1101--1110},
pmid = {17567986},
title = {{Diploid genome reconstruction of Ciona intestinalis and comparative analysis with Ciona savignyi}},
volume = {17},
year = {2007}
}
@article{Ilie2011b,
abstract = {High-throughput sequencing technologies produce very large amounts of data and sequencing errors constitute one of the major problems in analyzing such data. Current algorithms for correcting these errors are not very accurate and do not automatically adapt to the given data.},
author = {Ilie, Lucian and Fazayeli, Farideh and Ilie, Silvana},
file = {:home/gus/Documents/Mendeley Desktop/Ilie, Fazayeli, Ilie\_2011\_HiTEC accurate error correction in high-throughput sequencing data.pdf:pdf},
institution = {Department of Computer Science, University of Western Ontario, London, ON N6A 5B7, Canada. ilie@csd.uwo.ca},
journal = {Bioinformatics},
number = {3},
pages = {295--302},
pmid = {21115437},
title = {{HiTEC: accurate error correction in high-throughput sequencing data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21115437 http://bioinformatics.oxfordjournals.org/content/27/3/295.short},
volume = {27},
year = {2011}
}
@article{Florea:2010kx,
author = {Florea, Liliana and Souvorov, Alexander and Salzberg, Steven},
doi = {10.1186/gb-2010-11-s1-p13},
issn = {1465-6906},
journal = {Genome Biology},
number = {Suppl 1},
pages = {P13},
title = {{Genes and genomes, an imperfect world: comparison of gene annotations of two Bos taurus draft assemblies}},
url = {http://genomebiology.com/2010/11/S1/P13},
volume = {11},
year = {2010}
}
@article{Stranneheim2010,
abstract = {New generation sequencing technologies producing increasingly complex datasets demand new efficient and specialized sequence analysis algorithms. Often, it is only the 'novel' sequences in a complex dataset that are of interest and the superfluous sequences need to be removed.},
author = {Stranneheim, Henrik and K\"{a}ller, Max and Allander, Tobias and Andersson, Bj\"{o}rn and Arvestad, Lars and Lundeberg, Joakim},
doi = {10.1093/bioinformatics/btq230},
file = {:home/gus/Documents/Mendeley Desktop/Stranneheim et al.\_2010\_Classification of DNA sequences using Bloom filters.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,DNA,Genome,Humans,Metagenome,Mitochondrial,Respiratory System,Respiratory System: microbiology,Sensitivity and Specificity,Sequence Analysis},
month = jul,
number = {13},
pages = {1595--600},
pmid = {20472541},
title = {{Classification of DNA sequences using Bloom filters.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/26/13/1595},
volume = {26},
year = {2010}
}
@article{Vinson:2005kx,
abstract = {Whole-genome assembly is now used routinely to obtain high-quality draft sequence for the genomes of species with low levels of polymorphism. However, genome assembly remains extremely challenging for highly polymorphic species. The difficulty arises because two divergent haplotypes are sequenced together, making it difficult to distinguish alleles at the same locus from paralogs at different loci. We present here a method for assembling highly polymorphic diploid genomes that involves assembling the two haplotypes separately and then merging them to obtain a reference sequence. Our method was developed to assemble the genome of the sea squirt Ciona savignyi, which was sequenced to a depth of 12.7 x from a single wild individual. By comparing finished clones of the two haplotypes we determined that the sequenced individual had an extremely high heterozygosity rate, averaging 4.6\% with significant regional variation and rearrangements at all physical scales. Applied to these data, our method produced a reference assembly covering 157 Mb, with N50 contig and scaffold sizes of 47 kb and 989 kb, respectively. Alignment of ESTs indicates that 88\% of loci are present at least once and 81\% exactly once in the reference assembly. Our method represented loci in a single copy more reliably and achieved greater contiguity than a conventional whole-genome assembly method.},
author = {Vinson, Jade P and Jaffe, David B and O'Neill, Keith and Karlsson, Elinor K and Stange-Thomann, Nicole and Anderson, Scott and Mesirov, Jill P and Satoh, Nori and Satou, Yutaka and Nusbaum, Chad and Birren, Bruce and Galagan, James E and Lander, Eric S},
doi = {10.1101/gr.3722605},
journal = {Genome Res},
month = aug,
number = {8},
pages = {1127--1135},
pmid = {16077012},
title = {{Assembly of polymorphic genomes: algorithms and application to Ciona savignyi}},
volume = {15},
year = {2005}
}
@misc{TheMendeleySupportTeam2011a,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:home/gus/Documents/Mendeley Desktop/The Mendeley Support Team\_2011\_Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@article{Salzberg2011,
abstract = {New sequencing technology has dramatically altered the landscape of whole-genome sequencing, allowing scientists to initiate numerous projects to decode the genomes of previously unsequenced organisms. The lowest-cost technology can generate deep coverage of most species, including mammals, in just a few days. The sequence data generated by one of these projects consists of millions or billions of short DNA sequences (reads) that range from 50-150 nucleotides in length. These sequences must then be assembled de novo before most genome analyses can begin. Unfortunately, genome assembly remains a very difficult problem, made more difficult by shorter reads and unreliable long-range linking information. In this study, we evaluated several of the leading de novo assembly algorithms on four different short-read data sets, all generated by Illumina sequencers. Our results describe the relative performance of the different assemblers as well as other significant differences in assembly difficulty that appear to be inherent in the genomes themselves. Three overarching conclusions are apparent: first, that data quality, rather than the assembler itself, has a dramatic affect on the quality of an assembled genome; second, that the degree of contiguity of an assembly varies enormously among different assemblers and different genomes; and third, that the correctness of an assembly also varies widely, and is not well correlated with statistics on contiguity. In order to enable others to replicate our results, all of our data and methods are freely available, as are all assemblers used in this study.},
author = {Salzberg, S. L. and Phillippy, A. M. and Zimin, A. V. and Puiu, D. and Magoc, T. and Koren, S. and Treangen, T. and Schatz, M. C. and Delcher, A. L. and Roberts, M. and Marcais, G. and Pop, M. and Yorke, J. A.},
doi = {10.1101/gr.131383.111},
file = {:home/gus/Documents/Mendeley Desktop/Salzberg et al.\_2011\_GAGE A critical evaluation of genome assemblies and assembly algorithms(2).pdf:pdf},
issn = {1088-9051},
journal = {Genome Research},
month = dec,
pages = {gr.131383.111--},
title = {{GAGE: A critical evaluation of genome assemblies and assembly algorithms}},
url = {http://genome.cshlp.org/cgi/content/abstract/gr.131383.111v1},
year = {2011}
}
@inproceedings{Kanevsky,
abstract = {Given a graph G with n vertices and m edges, a k-connectivity query for vertices v' and v" of G asks whether there exist k disjoint paths between v' and v". The authors consider the problem of performing k-connectivity queries for k\&les;4. First, they present a static data structure that answers such queries in O(1) time. Next, they consider the problem of performing queries intermixed with online updates that insert vertices and edges. For triconnected graphs they give a dynamic data structure that supports queries and updates in time O($\alpha$( l,n)) amortized, where n is the current number of vertices of the graph and l is the total number of operations performed ($\alpha$(l, n) denotes the slowly growing Ackermann function inverse). For general graphs, a sequence of l operations takes total time O(n log n +l). All of the above data structures use space O(n), proportional to the number of vertices of the graph. The results also yield an efficient algorithm for testing whether graph G is four-connected that runs in O(n$\alpha$(n, n)+m) time using O(n+m) space},
author = {Kanevsky, A. and Tamassia, R. and {Di Battista}, G. and Chen, J.},
booktitle = {[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science},
doi = {10.1109/SFCS.1991.185451},
isbn = {0-8186-2445-0},
pages = {793--801},
publisher = {IEEE Comput. Soc. Press},
title = {{On-line maintenance of the four-connected components of a graph}},
url = {http://ieeexplore.ieee.org/xpl/freeabs\_all.jsp?arnumber=185451}
}
@article{Jin,
abstract = {Phylogenies-the evolutionary histories of groups of organisms-play a major role in representing the interrelationships among biological entities. Many methods for reconstructing and studying such phylogenies have been proposed, almost all of which assume that the underlying history of a given set of species can be represented by a binary tree. Although many biological processes can be effectively modeled and summarized in this fashion, others cannot: recombination, hybrid speciation, and horizontal gene transfer result in networks of relationships rather than trees of relationships. In previous works, we formulated a maximum parsimony (MP) criterion for reconstructing and evaluating phylogenetic networks, and demonstrated its quality on biological as well as synthetic data sets. In this paper, we provide further theoretical results as well as a very fast heuristic algorithm for the MP criterion of phylogenetic networks. In particular, we provide a novel combinatorial definition of phylogenetic networks in terms of "forbidden cycles," and provide detailed hardness and hardness of approximation proofs for the "small" MP problem. We demonstrate the performance of our heuristic in terms of time and accuracy on both biological and synthetic data sets. Finally, we explain the difference between our model and a similar one formulated by Nguyen et al., and describe the implications of this difference on the hardness and approximation results.},
author = {Jin, Guohua and Nakhleh, Luay and Snir, Sagi and Tuller, Tamir},
doi = {10.1109/TCBB.2008.119},
file = {:home/gus/Documents/Mendeley Desktop/Jin et al.\_Unknown\_Parsimony score of phylogenetic networks hardness results and a linear-time heuristic.pdf:pdf},
issn = {1557-9964},
journal = {IEEE/ACM transactions on computational biology and bioinformatics / IEEE, ACM},
keywords = {Algorithms,Archaea,Archaea: genetics,Cyclooxygenase 2,Cyclooxygenase 2: genetics,Genetic,Models,Phylogeny,Plant Proteins,Plant Proteins: genetics,Plants,Plants: genetics,Ribosomal Proteins,Ribosomal Proteins: genetics,blocking loop},
mendeley-tags = {blocking loop},
number = {3},
pages = {495--505},
pmid = {19644176},
title = {{Parsimony score of phylogenetic networks: hardness results and a linear-time heuristic.}},
url = {http://ieeexplore.ieee.org/xpl/freeabs\_all.jsp?arnumber=4668337},
volume = {6}
}
@article{Schatz2010,
abstract = {Second-generation sequencing technology can now be used to sequence an entire human genome in a matter of days and at low cost. Sequence read lengths, initially very short, have rapidly increased since the technology first appeared, and we now are seeing a growing number of efforts to sequence large genomes de novo from these short reads. In this Perspective, we describe the issues associated with short-read assembly, the different types of data produced by second-gen sequencers, and the latest assembly algorithms designed for these data. We also review the genomes that have been assembled recently from short reads and make recommendations for sequencing strategies that will yield a high-quality assembly.},
author = {Schatz, Michael C and Delcher, Arthur L and Salzberg, Steven L},
doi = {10.1101/gr.101360.109},
file = {:home/gus/Documents/Mendeley Desktop/Schatz, Delcher, Salzberg\_2010\_Assembly of large genomes using second-generation sequencing.pdf:pdf},
issn = {1549-5469},
journal = {Genome research},
keywords = {Algorithms,Base Sequence,Genome, Human,Genomics,Genomics: methods,Humans,Sequence Analysis, DNA,Sequence Analysis, DNA: methods},
month = sep,
number = {9},
pages = {1165--73},
pmid = {20508146},
title = {{Assembly of large genomes using second-generation sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2928494\&tool=pmcentrez\&rendertype=abstract},
volume = {20},
year = {2010}
}
@article{Zerbino2009a,
abstract = {Despite the short length of their reads, micro-read sequencing technologies have shown their usefulness for de novo sequencing. However, especially in eukaryotic genomes, complex repeat patterns are an obstacle to large assemblies.},
author = {Zerbino, Daniel R and McEwen, Gayle K and Margulies, Elliott H and Birney, Ewan},
doi = {10.1371/journal.pone.0008407},
file = {:home/gus/Documents/Mendeley Desktop/Zerbino, McEwen\_2009\_Pebble and rock band heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algorithms,Animals,Chromosomes, Artificial, Bacterial,Chromosomes, Artificial, Bacterial: genetics,Computer Simulation,Ferrets,Ferrets: genetics,Pseudomonas syringae,Pseudomonas syringae: genetics,Repetitive Sequences, Nucleic Acid,Repetitive Sequences, Nucleic Acid: genetics,Sequence Analysis, DNA,Sequence Analysis, DNA: instrumentation,Sequence Analysis, DNA: methods},
month = jan,
number = {12},
pages = {e8407},
pmid = {20027311},
title = {{Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2793427\&tool=pmcentrez\&rendertype=abstract},
volume = {4},
year = {2009}
}
@article{Lander1988,
abstract = {Results from physical mapping projects have recently been reported for the genomes of Escherichia coli, Saccharomyces cerevisiae, and Caenorhabditis elegans, and similar projects are currently being planned for other organisms. In such projects, the physical map is assembled by first "fingerprinting" a large number of clones chosen at random from a recombinant library and then inferring overlaps between clones with sufficiently similar fingerprints. Although the basic approach is the same, there are many possible choices for the fingerprint used to characterize the clones and the rules for declaring overlap. In this paper, we derive simple formulas showing how the progress of a physical mapping project is affected by the nature of the fingerprinting scheme. Using these formulas, we discuss the analytic considerations involved in selecting an appropriate fingerprinting scheme for a particular project.},
author = {Lander, E S and Waterman, M S},
file = {:home/gus/Documents/Mendeley Desktop/Lander, Waterman\_1988\_Genomic mapping by fingerprinting random clones a mathematical analysis.pdf:pdf},
issn = {0888-7543},
journal = {Genomics},
keywords = {Animals,Caenorhabditis,Caenorhabditis: genetics,Chromosome Mapping,Cloning,Escherichia coli,Escherichia coli: genetics,Mathematics,Molecular,Nucleotide Mapping,Saccharomyces cerevisiae,Saccharomyces cerevisiae: genetics},
month = apr,
number = {3},
pages = {231--9},
pmid = {3294162},
title = {{Genomic mapping by fingerprinting random clones: a mathematical analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3294162},
volume = {2},
year = {1988}
}
@article{Miller1998,
author = {Miller, Peter},
file = {:home/gus/Documents/Mendeley Desktop/Miller\_1998\_Recursive make considered harmful.pdf:pdf},
journal = {AUUGN Journal of AUUG Inc},
number = {1},
pages = {14--25},
title = {{Recursive make considered harmful}},
url = {http://www.unix-ag.uni-kl.de/svn/kbibtex/kbibtex/tags/release-0.1/admin/unsermake/doc/auug97.pdf},
volume = {19},
year = {1998}
}
@article{Manber1990,
annote = {        From Duplicate 1 (                           Suffix arrays: a new method for on-line string searches                         - Manber, Udi; Myers, Gene )
                
        
        
      },
author = {Manber, Udi and Myers, Gene},
file = {:home/gus/Documents/Mendeley Desktop/Manber, Myers\_1990\_Suffix arrays a new method for on-line string searches(2).pdf:pdf;:home/gus/Documents/Mendeley Desktop/Manber, Myers\_1990\_Suffix arrays a new method for on-line string searches.pdf:pdf},
isbn = {0-89871-251-3},
journal = {Proceedings of the first annual ACM-SIAM \ldots},
month = jan,
pages = {319--327},
title = {{Suffix arrays: a new method for on-line string searches}},
url = {http://portal.acm.org/citation.cfm?id=320218 http://dl.acm.org/citation.cfm?id=320176.320218},
year = {1990}
}
@article{Rhrissorrakrai2011,
abstract = {ABSTRACT: BACKGROUND: Graphical models of network associations are useful for both visualizing and integrating multiple types of association data. Identifying modules, or groups of functionally related gene products, is an important challenge in analyzing biological networks. However, existing tools to identify modules are insufficient when applied to dense networks of experimentally derived interaction data. To address this problem, we have developed an agglomerative clustering method that is able to identify highly modular sets of gene products within highly interconnected molecular interaction networks. RESULTS: MINE outperforms MCODE, CFinder, NEMO, SPICi, and MCL in identifying non-exclusive, high modularity clusters when applied to the C. elegans protein-protein interaction network. The algorithm generally achieves superior geometric accuracy and modularity for annotated functional categories. In comparison with the most closely related algorithm, MCODE, the top clusters identified by MINE are consistently of higher density and MINE is less likely to designate overlapping modules as a single unit. MINE offers a high level of granularity with a small number of adjustable parameters, enabling users to fine-tune cluster results for input networks with differing topological properties. CONCLUSIONS: MINE was created in response to the challenge of discovering high quality modules of gene products within highly interconnected biological networks. The algorithm allows a high degree of flexibility and user-customisation of results with few adjustable parameters. MINE outperforms several popular clustering algorithms in identifying modules with high modularity and obtains good overall recall and precision of functional annotations in protein-protein interaction networks from both S. cerevisiae and C. elegans.},
author = {Rhrissorrakrai, Kahn and Gunsalus, Kristin C},
doi = {10.1186/1471-2105-12-192},
issn = {1471-2105},
journal = {BMC bioinformatics},
month = may,
number = {1},
pages = {192},
pmid = {21605434},
title = {{MINE: Module Identification in NEtworks.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3123237\&tool=pmcentrez\&rendertype=abstract},
volume = {12},
year = {2011}
}
@article{Yang2011,
abstract = {High-throughput short read sequencing is revolutionizing genomics and systems biology research by enabling cost-effective deep coverage sequencing of genomes and transcriptomes. Error detection and correction are crucial to many short read sequencing applications including de novo genome sequencing, genome resequencing, and digital gene expression analysis. Short read error detection is typically carried out by counting the observed frequencies of kmers in reads and validating those with frequencies exceeding a threshold. In case of genomes with high repeat content, an erroneous kmer may be frequently observed if it has few nucleotide differences with valid kmers with multiple occurrences in the genome. Error detection and correction were mostly applied to genomes with low repeat content and this remains a challenging problem for genomes with high repeat content.},
author = {Yang, Xiao and Aluru, Srinivas and Dorman, Karin S},
doi = {10.1186/1471-2105-12-S1-S52},
file = {:home/gus/Documents/Mendeley Desktop//Yang, Aluru, Dorman\_2011\_Repeat-aware modeling and correction of short read errors.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,Genomics,Genomics: methods,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Likelihood Functions,Models,Software,Statistical},
month = jan,
number = {Suppl 1},
pages = {S52},
pmid = {21342585},
title = {{Repeat-aware modeling and correction of short read errors.}},
url = {http://www.biomedcentral.com/1471-2105/12/S1/S52},
volume = {12 Suppl 1},
year = {2011}
}
@inproceedings{Ortiz2010,
abstract = {Popular sorting algorithms do not translate well into hardware implementations. Instead, hardware-based solutions like sorting networks and linear sorters exploit parallelism to increase sorting efficiency. Linear sorters, built from identical nodes with simple control, have less area and latency than sorting networks, but they are limited in their throughput. We present a system composed of multiple linear sorters acting in parallel in order to increase throughput. Interleaving is used to increase bandwidth and allow sorting of multiple values per clock cycle, and the amount of interleaving and depth of the linear sorters can be adapted to suit specific applications. Implementation of this system into a Field Programmable Gate Array (FPGA) results in a speedup of 68 compared to quicksort running in a MicroBlaze processor.},
author = {Ortiz, Jorge and Andrews, David},
booktitle = {2010 IEEE International Symposium on Parallel \& Distributed Processing, Workshops and Phd Forum (IPDPSW)},
doi = {10.1109/IPDPSW.2010.5470730},
file = {:home/gus/Documents/Mendeley Desktop/Ortiz, Andrews\_2010\_A configurable high-throughput linear sorter system.pdf:pdf},
isbn = {978-1-4244-6533-0},
month = apr,
pages = {1--8},
publisher = {IEEE},
title = {{A configurable high-throughput linear sorter system}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5470730},
year = {2010}
}
@article{dohm2008substantial,
author = {Dohm, J.C. and Lottaz, C. and Borodina, T. and Himmelbauer, H.},
file = {:home/gus/Documents/Mendeley Desktop/Dohm et al.\_2008\_Substantial biases in ultra-short read data sets from high-throughput DNA sequencing.pdf:pdf},
journal = {Nucleic acids research},
number = {16},
pages = {e105},
publisher = {Oxford Univ Press},
title = {{Substantial biases in ultra-short read data sets from high-throughput DNA sequencing}},
url = {http://nar.oxfordjournals.org/content/36/16/e105.short},
volume = {36},
year = {2008}
}
@article{Zerbino2008,
abstract = {We have developed a new set of algorithms, collectively called "Velvet," to manipulate de Bruijn graphs for genomic sequence assembly. A de Bruijn graph is a compact representation based on short words (k-mers) that is ideal for high coverage, very short read (25-50 bp) data sets. Applying Velvet to very short reads and paired-ends information only, one can produce contigs of significant length, up to 50-kb N50 length in simulations of prokaryotic data and 3-kb N50 on simulated mammalian BACs. When applied to real Solexa data sets without read pairs, Velvet generated contigs of approximately 8 kb in a prokaryote and 2 kb in a mammalian BAC, in close agreement with our simulated results without read-pair information. Velvet represents a new approach to assembly that can leverage very short reads in combination with read pairs to produce useful assemblies.},
author = {Zerbino, Daniel R and Birney, Ewan},
doi = {10.1101/gr.074492.107},
file = {:home/gus/Documents/Mendeley Desktop/Zerbino, Birney\_2008\_Velvet algorithms for de novo short read assembly using de Bruijn graphs.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Animals,Chromosomes, Artificial, Bacterial,Computational Biology,Computational Biology: methods,Computer Simulation,Genome, Bacterial,Genome, Human,Genomics,Humans,Mammals,Mammals: genetics,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Sequence Analysis, DNA: standards,Streptococcus,Streptococcus: genetics},
month = may,
number = {5},
pages = {821--9},
pmid = {18349386},
title = {{Velvet: algorithms for de novo short read assembly using de Bruijn graphs.}},
url = {http://genome.cshlp.org/cgi/content/abstract/18/5/821},
volume = {18},
year = {2008}
}
@article{Butler:2008kx,
abstract = {New DNA sequencing technologies deliver data at dramatically lower costs but demand new analytical methods to take full advantage of the very short reads that they produce. We provide an initial, theoretical solution to the challenge of de novo assembly from whole-genome shotgun "microreads." For 11 genomes of sizes up to 39 Mb, we generated high-quality assemblies from 80x coverage by paired 30-base simulated reads modeled after real Illumina-Solexa reads. The bacterial genomes of Campylobacter jejuni and Escherichia coli assemble optimally, yielding single perfect contigs, and larger genomes yield assemblies that are highly connected and accurate. Assemblies are presented in a graph form that retains intrinsic ambiguities such as those arising from polymorphism, thereby providing information that has been absent from previous genome assemblies. For both C. jejuni and E. coli, this assembly graph is a single edge encompassing the entire genome. Larger genomes produce more complicated graphs, but the vast majority of the bases in their assemblies are present in long edges that are nearly always perfect. We describe a general method for genome assembly that can be applied to all types of DNA sequence data, not only short read data, but also conventional sequence reads.},
author = {Butler, Jonathan and MacCallum, Iain and Kleber, Michael and Shlyakhter, Ilya a and Belmonte, Matthew K and Lander, Eric S and Nusbaum, Chad and Jaffe, David B},
doi = {10.1101/gr.7337908},
file = {:home/gus/Documents/Mendeley Desktop/Butler et al.\_2008\_ALLPATHS de novo assembly of whole-genome shotgun microreads.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Bacterial,Bacterial: genetics,Campylobacter jejuni,Campylobacter jejuni: genetics,Computational Biology,Computational Biology: methods,Computer Simulation,DNA,DNA: methods,DNA: standards,Escherichia coli,Escherichia coli: genetics,Genome,Reproducibility of Results,Sequence Analysis},
month = may,
number = {5},
pages = {810--20},
pmid = {18340039},
title = {{ALLPATHS: de novo assembly of whole-genome shotgun microreads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2336810\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@article{Ilie2011a,
author = {Ilie, L},
file = {:home/gus/Documents/Mendeley Desktop/Ilie, Fazayeli, Ilie\_2011\_HiTEC accurate error correction in high-throughput sequencing data.pdf:pdf},
journal = {Bioinformatics},
title = {{HiTEC: accurate error correction in high-throughput sequencing data}},
url = {http://bioinformatics.oxfordjournals.org/content/27/3/295.short},
year = {2011}
}
@article{Drmanac2010,
abstract = {Genome sequencing of large numbers of individuals promises to advance the understanding, treatment, and prevention of human diseases, among other applications. We describe a genome sequencing platform that achieves efficient imaging and low reagent consumption with combinatorial probe anchor ligation chemistry to independently assay each base from patterned nanoarrays of self-assembling DNA nanoballs. We sequenced three human genomes with this platform, generating an average of 45- to 87-fold coverage per genome and identifying 3.2 to 4.5 million sequence variants per genome. Validation of one genome data set demonstrates a sequence accuracy of about 1 false variant per 100 kilobases. The high accuracy, affordable cost of \$4400 for sequencing consumables, and scalability of this platform enable complete human genome sequencing for the detection of rare variants in large-scale genetic studies.},
author = {Drmanac, Radoje and Sparks, Andrew B and Callow, Matthew J and Halpern, Aaron L and Burns, Norman L and Kermani, Bahram G and Carnevali, Paolo and Nazarenko, Igor and Nilsen, Geoffrey B and Yeung, George and Dahl, Fredrik and Fernandez, Andres and Staker, Bryan and Pant, Krishna P and Baccash, Jonathan and Borcherding, Adam P and Brownley, Anushka and Cedeno, Ryan and Chen, Linsu and Chernikoff, Dan and Cheung, Alex and Chirita, Razvan and Curson, Benjamin and Ebert, Jessica C and Hacker, Coleen R and Hartlage, Robert and Hauser, Brian and Huang, Steve and Jiang, Yuan and Karpinchyk, Vitali and Koenig, Mark and Kong, Calvin and Landers, Tom and Le, Catherine and Liu, Jia and McBride, Celeste E and Morenzoni, Matt and Morey, Robert E and Mutch, Karl and Perazich, Helena and Perry, Kimberly and Peters, Brock A and Peterson, Joe and Pethiyagoda, Charit L and Pothuraju, Kaliprasad and Richter, Claudia and Rosenbaum, Abraham M and Roy, Shaunak and Shafto, Jay and Sharanhovich, Uladzislau and Shannon, Karen W and Sheppy, Conrad G and Sun, Michel and Thakuria, Joseph V and Tran, Anne and Vu, Dylan and Zaranek, Alexander Wait and Wu, Xiaodi and Drmanac, Snezana and Oliphant, Arnold R and Banyai, William C and Martin, Bruce and Ballinger, Dennis G and Church, George M and Reid, Clifford A},
doi = {10.1126/science.1181498},
file = {:home/gus/Documents/Mendeley Desktop/Drmanac et al.\_2010\_Human genome sequencing using unchained base reads on self-assembling DNA nanoarrays.pdf:pdf},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
keywords = {Base Sequence,Computational Biology,Costs and Cost Analysis,DNA,DNA: chemistry,DNA: economics,DNA: genetics,DNA: instrumentation,DNA: methods,DNA: standards,Databases,Genome,Genomic Library,Genotype,Haplotypes,Human,Human Genome Project,Humans,Male,Microarray Analysis,Nanostructures,Nanotechnology,Nucleic Acid,Nucleic Acid Amplification Techniques,Polymorphism,Sequence Analysis,Single Nucleotide,Software},
month = jan,
number = {5961},
pages = {78--81},
pmid = {19892942},
title = {{Human genome sequencing using unchained base reads on self-assembling DNA nanoarrays.}},
url = {http://www.sciencemag.org/content/327/5961/78.abstract},
volume = {327},
year = {2010}
}
@article{yang2010reptile,
author = {Yang, X. and Dorman, K.S. and Aluru, S.},
file = {:home/gus/Documents/Mendeley Desktop/Yang, Dorman, Aluru\_2010\_Reptile representative tiling for short read error correction.pdf:pdf},
journal = {Bioinformatics},
number = {20},
pages = {2526},
publisher = {Oxford Univ Press},
title = {{Reptile: representative tiling for short read error correction}},
url = {http://bioinformatics.oxfordjournals.org/content/26/20/2526.short},
volume = {26},
year = {2010}
}
@article{Li2003,
abstract = {In shotgun sequencing projects, the genome or BAC length is not always known. We approach estimating genome length by first estimating the repeat structure of the genome or BAC, sometimes of interest in its own right, on the basis of a set of random reads from a genome project. Moreover, we can find the consensus for repeat families before assembly. Our methods are based on the l-tuple content of the reads.},
author = {Li, Xiaoman and Waterman, Michael S},
doi = {10.1101/gr.1251803},
file = {:home/gus/Documents/Mendeley Desktop/Li, Waterman\_2003\_Estimating the repeat structure and length of DNA sequences using L-tuples.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Artificial,Bacterial,Bacterial: genetics,Base Composition,Chromosome Mapping,Chromosome Mapping: statistics \& numerical data,Chromosomes,Computational Biology,Computational Biology: statistics \& numerical data,Computer Simulation,Computer Simulation: statistics \& numerical data,Consensus Sequence,DNA,Mathematical Computing,Nucleic Acid,Nucleic Acid: genetics,Poisson Distribution,Repetitive Sequences,Software},
month = aug,
number = {8},
pages = {1916--22},
pmid = {12902383},
title = {{Estimating the repeat structure and length of DNA sequences using L-tuples.}},
url = {http://genome.cshlp.org/cgi/content/abstract/13/8/1916},
volume = {13},
year = {2003}
}
@inproceedings{Cheriyan1991,
address = {New York, New York, USA},
author = {Cheriyan, Joseph and Thurimella, Ramakrishna},
booktitle = {Proceedings of the twenty-third annual ACM symposium on Theory of computing - STOC '91},
doi = {10.1145/103418.103460},
file = {:home/gus/Documents/Mendeley Desktop/Cheriyan, Thurimella\_1991\_Algorithms for parallel k -vertex connectivity and sparse certificates.pdf:pdf},
isbn = {0897913973},
month = jan,
pages = {391--401},
publisher = {ACM Press},
title = {{Algorithms for parallel k -vertex connectivity and sparse certificates}},
url = {http://dl.acm.org/citation.cfm?id=103418.103460},
year = {1991}
}
@article{Michael1996,
address = {New York, New York, USA},
author = {Michael, Maged M. and Scott, Michael L.},
doi = {10.1145/248052.248106},
file = {:home/gus/Documents/Mendeley Desktop/Michael, Scott\_1996\_Simple, fast, and practical non-blocking and blocking concurrent queue algorithms.pdf:pdf},
isbn = {0897918002},
journal = {Proceedings of the fifteenth annual ACM symposium on Principles of distributed computing - PODC '96},
keywords = {compare and swap,concurrent queue,lock-free,multiprogramming,non-blocking},
pages = {267--275},
publisher = {ACM Press},
title = {{Simple, fast, and practical non-blocking and blocking concurrent queue algorithms}},
url = {http://portal.acm.org/citation.cfm?doid=248052.248106},
year = {1996}
}
@article{Medvedev2011,
abstract = {The continuing improvements to high-throughput sequencing (HTS) platforms have begun to unfold a myriad of new applications. As a result, error correction of sequencing reads remains an important problem. Though several tools do an excellent job of correcting datasets where the reads are sampled close to uniformly, the problem of correcting reads coming from drastically non-uniform datasets, such as those from single-cell sequencing, remains open.},
author = {Medvedev, Paul and Scott, Eric and Kakaradov, Boyko and Pevzner, Pavel},
doi = {10.1093/bioinformatics/btr208},
file = {:home/gus/Documents/Mendeley Desktop/Medvedev et al.\_2011\_Error correction of high-throughput sequencing datasets with non-uniform coverage.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = jul,
number = {13},
pages = {i137--i141},
pmid = {21685062},
title = {{Error correction of high-throughput sequencing datasets with non-uniform coverage.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3117386\&tool=pmcentrez\&rendertype=abstract},
volume = {27},
year = {2011}
}
@article{Kim:2007uq,
abstract = {One of the main goals in genome sequencing projects is to determine a haploid consensus sequence even when clone libraries are constructed from homologous chromosomes. However, it has been noticed that haplotypes can be inferred from genome assemblies by investigating phase conservation in sequenced reads. In this study, we seek to infer haplotypes, a diploid consensus sequence, from the genome assembly of an organism, Ciona intestinalis. The Ciona intestinalis genome is an ideal resource from which haplotypes can be inferred because of the high polymorphism rate (1.2\%). The haplotype estimation scheme consists of polymorphism detection and phase estimation. The core step of our method is a Gibbs sampling procedure. The mate-pair information from two-end sequenced clone inserts is exploited to provide long-range continuity. We estimate the polymorphism rate of Ciona intestinalis to be 1.2\% and 1.5\%, according to two different polymorphism counting schemes. The distribution of heterozygosity number is well fit by a compound Poisson distribution. The N50 length of haplotype segments is 37.9 kb in our assembly, while the N50 scaffold length of the Ciona intestinalis assembly is 190 kb. We also infer diploid gene sequences from haplotype segments. According to our reconstruction, 85.4\% of predicted gene sequences are continuously covered by single haplotype segments. Our results indicate 97\% accuracy in haplotype estimation, based on a simulated data set. We conduct a comparative analysis with Ciona savignyi, and discover interesting patterns of conserved DNA elements in chordates.},
author = {Kim, Jong Hyun and Waterman, Michael S and Li, Lei M},
doi = {10.1101/gr.5894107},
file = {:home/gus/Documents/Mendeley Desktop/Kim, Waterman, Li\_2007\_Diploid genome reconstruction of Ciona intestinalis and comparative analysis with Ciona savignyi.pdf:pdf},
journal = {Genome Res},
month = jul,
number = {7},
pages = {1101--1110},
pmid = {17567986},
title = {{Diploid genome reconstruction of Ciona intestinalis and comparative analysis with Ciona savignyi}},
volume = {17},
year = {2007}
}
@article{Schroder2009,
abstract = {MOTIVATION: Second-generation sequencing technologies produce a massive amount of short reads in a single experiment. However, sequencing errors can cause major problems when using this approach for de novo sequencing applications. Moreover, existing error correction methods have been designed and optimized for shotgun sequencing. Therefore, there is an urgent need for the design of fast and accurate computational methods and tools for error correction of large amounts of short read data. RESULTS: We present SHREC, a new algorithm for correcting errors in short-read data that uses a generalized suffix trie on the read data as the underlying data structure. Our results show that the method can identify erroneous reads with sensitivity and specificity of over 99\% and 96\% for simulated data with error rates of up to 3\% as well as for real data. Furthermore, it achieves an error correction accuracy of over 80\% for simulated data and over 88\% for real data. These results are clearly superior to previously published approaches. SHREC is available as an efficient open-source Java implementation that allows processing of 10 million of short reads on a standard workstation.},
author = {Schr\"{o}der, Jan and Schr\"{o}der, Heiko and Puglisi, Simon J and Sinha, Ranjan and Schmidt, Bertil},
doi = {10.1093/bioinformatics/btp379},
file = {:home/gus/Documents/Mendeley Desktop/Schr\"{o}der et al.\_2009\_SHREC a short-read error correction method.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Schr\"{o}der et al.\_2009\_SHREC a short-read error correction method(2).pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,DNA,DNA: genetics,DNA: methods,Databases,Error correction,Genome,Genome: genetics,Nucleic Acid,Research Design,Sequence Analysis,Time Factors},
mendeley-tags = {Error correction},
month = sep,
number = {17},
pages = {2157--63},
pmid = {19542152},
title = {{SHREC: a short-read error correction method.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/25/17/2157},
volume = {25},
year = {2009}
}
@inproceedings{sundell2003fast,
author = {Sundell, H. and Tsigas, P.},
booktitle = {Parallel and Distributed Processing Symposium, 2003. Proceedings. International},
file = {:home/gus/Documents/Mendeley Desktop/Sundell, Tsigas\_2003\_Fast and lock-free concurrent priority queues for multi-thread systems.pdf:pdf},
pages = {11--pp},
publisher = {IEEE},
title = {{Fast and lock-free concurrent priority queues for multi-thread systems}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1213189},
year = {2003}
}
@article{Zerbino2009a,
abstract = {Despite the short length of their reads, micro-read sequencing technologies have shown their usefulness for de novo sequencing. However, especially in eukaryotic genomes, complex repeat patterns are an obstacle to large assemblies.},
author = {Zerbino, Daniel R DR and McEwen, GK Gayle K and Margulies, Elliott H and Birney, Ewan},
doi = {10.1371/journal.pone.0008407},
file = {:home/gus/Documents/Mendeley Desktop/Zerbino, McEwen\_2009\_Pebble and rock band heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algorithms,Animals,Artificial,Bacterial,Bacterial: genetics,Chromosomes,Computer Simulation,DNA,DNA: instrumentation,DNA: methods,Ferrets,Ferrets: genetics,Nucleic Acid,Nucleic Acid: genetics,Pseudomonas syringae,Pseudomonas syringae: genetics,Repetitive Sequences,Sequence Analysis},
month = jan,
number = {12},
pages = {e8407},
pmid = {20027311},
title = {{Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2793427\&tool=pmcentrez\&rendertype=abstract http://dx.plos.org/10.1371/journal.pone.0008407},
volume = {4},
year = {2009}
}
@misc{TheMendeleySupportTeam2011,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:home/gus/Documents/Mendeley Desktop/The Mendeley Support Team\_2011\_Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@article{Yang2011a,
abstract = {High-throughput short read sequencing is revolutionizing genomics and systems biology research by enabling cost-effective deep coverage sequencing of genomes and transcriptomes. Error detection and correction are crucial to many short read sequencing applications including de novo genome sequencing, genome resequencing, and digital gene expression analysis. Short read error detection is typically carried out by counting the observed frequencies of kmers in reads and validating those with frequencies exceeding a threshold. In case of genomes with high repeat content, an erroneous kmer may be frequently observed if it has few nucleotide differences with valid kmers with multiple occurrences in the genome. Error detection and correction were mostly applied to genomes with low repeat content and this remains a challenging problem for genomes with high repeat content.},
author = {Yang, Xiao and Aluru, Srinivas and Dorman, Karin S},
doi = {10.1186/1471-2105-12-S1-S52},
file = {:home/gus/Documents/Mendeley Desktop//Yang, Aluru, Dorman\_2011\_Repeat-aware modeling and correction of short read errors.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,Genomics,Genomics: methods,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Likelihood Functions,Models, Statistical,Software,Statistical},
month = jan,
pages = {S52},
pmid = {21342585},
title = {{Repeat-aware modeling and correction of short read errors.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3044310\&tool=pmcentrez\&rendertype=abstract},
volume = {12 Suppl 1},
year = {2011}
}
@article{Kim:2007uq,
abstract = {One of the main goals in genome sequencing projects is to determine a haploid consensus sequence even when clone libraries are constructed from homologous chromosomes. However, it has been noticed that haplotypes can be inferred from genome assemblies by investigating phase conservation in sequenced reads. In this study, we seek to infer haplotypes, a diploid consensus sequence, from the genome assembly of an organism, Ciona intestinalis. The Ciona intestinalis genome is an ideal resource from which haplotypes can be inferred because of the high polymorphism rate (1.2\%). The haplotype estimation scheme consists of polymorphism detection and phase estimation. The core step of our method is a Gibbs sampling procedure. The mate-pair information from two-end sequenced clone inserts is exploited to provide long-range continuity. We estimate the polymorphism rate of Ciona intestinalis to be 1.2\% and 1.5\%, according to two different polymorphism counting schemes. The distribution of heterozygosity number is well fit by a compound Poisson distribution. The N50 length of haplotype segments is 37.9 kb in our assembly, while the N50 scaffold length of the Ciona intestinalis assembly is 190 kb. We also infer diploid gene sequences from haplotype segments. According to our reconstruction, 85.4\% of predicted gene sequences are continuously covered by single haplotype segments. Our results indicate 97\% accuracy in haplotype estimation, based on a simulated data set. We conduct a comparative analysis with Ciona savignyi, and discover interesting patterns of conserved DNA elements in chordates.},
author = {Kim, Jong Hyun and Waterman, Michael S and Li, Lei M},
doi = {10.1101/gr.5894107},
journal = {Genome Res},
month = jul,
number = {7},
pages = {1101--1110},
pmid = {17567986},
title = {{Diploid genome reconstruction of Ciona intestinalis and comparative analysis with Ciona savignyi}},
volume = {17},
year = {2007}
}
@inproceedings{michael2002safe,
author = {Michael, M.M.},
booktitle = {Proceedings of the twenty-first annual symposium on Principles of distributed computing},
file = {:home/gus/Documents/Mendeley Desktop/Michael\_2002\_Safe memory reclamation for dynamic lock-free objects using atomic reads and writes.pdf:pdf},
pages = {21--30},
publisher = {ACM},
title = {{Safe memory reclamation for dynamic lock-free objects using atomic reads and writes}},
url = {http://portal.acm.org/citation.cfm?id=571829},
year = {2002}
}
@article{Langmead2009,
abstract = {Bowtie is an ultrafast, memory-efficient alignment program for aligning short DNA sequence reads to large genomes. For the human genome, Burrows-Wheeler indexing allows Bowtie to align more than 25 million reads per CPU hour with a memory footprint of approximately 1.3 gigabytes. Bowtie extends previous Burrows-Wheeler techniques with a novel quality-aware backtracking algorithm that permits mismatches. Multiple processor cores can be used simultaneously to achieve even greater alignment speeds. Bowtie is open source (http://bowtie.cbcb.umd.edu).},
author = {Langmead, Ben and Trapnell, Cole and Pop, Mihai and Salzberg, Steven L},
doi = {10.1186/gb-2009-10-3-r25},
file = {:home/gus/Documents/Mendeley Desktop/Langmead et al.\_2009\_Ultrafast and memory-efficient alignment of short DNA sequences to the human genome.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Algorithms,Base Sequence,Genome,Human,Human: genetics,Humans,Sequence Alignment,Sequence Alignment: methods},
month = jan,
number = {3},
pages = {R25},
pmid = {19261174},
title = {{Ultrafast and memory-efficient alignment of short DNA sequences to the human genome.}},
url = {http://genomebiology.com/2009/10/3/R25},
volume = {10},
year = {2009}
}
@incollection{Hart2008,
address = {New York},
author = {Hart, John P.},
booktitle = {Current Northeast Paleoethnobotany II},
chapter = {7},
editor = {Hart, John P.},
file = {:home/gus/Documents/Mendeley Desktop/Hart\_2008\_Evolving the three sisters the changing histories of maize, bean, and squash in new york and the greater northeast.pdf:pdf},
pages = {87--100},
publisher = {New York State Museum Bulletin 512},
title = {{Evolving the three sisters: the changing histories of maize, bean, and squash in new york and the greater northeast}},
year = {2008}
}
@article{Farach1997,
author = {Farach, M.},
file = {:home/gus/Documents/Mendeley Desktop/Farach\_1997\_Optimal suffix tree construction with large alphabets.pdf:pdf},
isbn = {0-8186-8197-7},
keywords = {combinatorial pattern matching,data structure,integer alphabet,integer alphabets,large alphabets,pattern matching,sorting,suffix tree},
month = oct,
pages = {137},
title = {{Optimal suffix tree construction with large alphabets}},
url = {http://dl.acm.org/citation.cfm?id=795663.796326},
year = {1997}
}
@article{Gonzalez-Porta2011,
abstract = {DNA arrays have been widely used to perform transcriptome-wide analysis of gene expression, and many methods have been developed to measure gene expression variability and to compare gene expression between conditions. Because RNA-seq is also becoming increasingly popular for transcriptome characterization, the possibility exists for further quantification of individual alternative transcript isoforms, and therefore for estimating the relative ratios of alternative splice forms within a given gene. Changes in splicing ratios, even without changes in overall gene expression, may have important phenotypic effects. Here we have developed statistical methodology to measure variability in splicing ratios within conditions, to compare it between conditions, and to identify genes with condition-specific splicing ratios. Furthermore, we have developed methodology to deconvolute the relative contribution of variability in gene expression versus variability in splicing ratios to the overall variability of transcript abundances. As a proof of concept, we have applied this methodology to estimates of transcript abundances obtained from RNA-seq experiments in lymphoblastoid cells from Caucasian and Yoruban individuals. We have found that protein-coding genes exhibit low splicing variability within populations, with many genes exhibiting constant ratios across individuals. When comparing these two populations, we have found that up to 10\% of the studied protein-coding genes exhibit population-specific splicing ratios. We estimate that 60\% of the total variability observed in the abundance of transcript isoforms can be explained by variability in transcription. A large fraction of the remaining variability can likely result from variability in splicing. Finally, we also detected that variability in splicing is uncommon without variability in transcription.},
author = {Gonz\`{a}lez-Porta, Mar and Calvo, Miquel and Sammeth, Michael and Guig\'{o}, Roderic},
doi = {10.1101/gr.121947.111},
file = {:home/gus/Documents/Mendeley Desktop/Gonz\`{a}lez-Porta et al.\_2011\_Estimation of alternative splicing variability in human populations.pdf:pdf},
issn = {1549-5469},
journal = {Genome research},
month = dec,
number = {3},
pages = {528--538},
pmid = {22113879},
title = {{Estimation of alternative splicing variability in human populations.}},
url = {http://genome.cshlp.org/cgi/content/abstract/22/3/528},
volume = {22},
year = {2011}
}
@article{Bloom1970,
author = {Bloom, Burton H.},
doi = {10.1145/362686.362692},
file = {:home/gus/Documents/Mendeley Desktop/Bloom\_1970\_Spacetime trade-offs in hash coding with allowable errors.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {Bloom filter,hash addressing,hash coding,retrieval efficiency,retrieval trade-offs,scatter storage,searching,storage efficiency,storage layout},
mendeley-tags = {Bloom filter},
month = jul,
number = {7},
pages = {422--426},
title = {{Space/time trade-offs in hash coding with allowable errors}},
url = {http://dl.acm.org/citation.cfm?id=362686.362692},
volume = {13},
year = {1970}
}
@inproceedings{Gao2004,
author = {Gao, H. and Groote, J.F. F and Hesselink, W.H. H},
booktitle = {18th International Parallel and Distributed Processing Symposium},
doi = {10.1109/IPDPS.2004.1302969},
file = {:home/gus/Documents/Mendeley Desktop/Gao, Groote, Hesselink\_Unknown\_Almost wait-free resizable hashtables.pdf:pdf},
isbn = {0769521320},
number = {C},
pages = {50--58},
publisher = {Ieee},
title = {{Almost wait-free resizable hashtables}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1302969},
volume = {00},
year = {2004}
}
@article{nagarajan2009parametric,
author = {Nagarajan, N. and Pop, M.},
file = {:home/gus/Documents/Mendeley Desktop/Nagarajan, Pop\_2009\_Parametric complexity of sequence assembly theory and applications to next generation sequencing.pdf:pdf},
journal = {Journal of computational biology},
number = {7},
pages = {897--908},
publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
title = {{Parametric complexity of sequence assembly: theory and applications to next generation sequencing}},
url = {http://www.liebertonline.com/doi/abs/10.1089/cmb.2009.0005},
volume = {16},
year = {2009}
}
@article{Kawrykow2012,
abstract = {Background
          Comparative genomics, or the study of the relationships of genome structure and function across different species, offers a powerful tool for studying evolution, annotating genomes, and understanding the causes of various genetic disorders. However, aligning multiple sequences of DNA, an essential intermediate step for most types of analyses, is a difficult computational task. In parallel, citizen science, an approach that takes advantage of the fact that the human brain is exquisitely tuned to solving specific types of problems, is becoming increasingly popular. There, instances of hard computational problems are dispatched to a crowd of non-expert human game players and solutions are sent back to a central server.
        
        
          Methodology/Principal Findings
          We introduce Phylo, a human-based computing framework applying crowd sourcing techniques to solve the Multiple Sequence Alignment (MSA) problem. The key idea of Phylo is to convert the MSA problem into a casual game that can be played by ordinary web users with a minimal prior knowledge of the biological context. We applied this strategy to improve the alignment of the promoters of disease-related genes from up to 44 vertebrate species. Since the launch in November 2010, we received more than 350,000 solutions submitted from more than 12,000 registered users. Our results show that solutions submitted contributed to improving the accuracy of up to 70\% of the alignment blocks considered.
        
        
          Conclusions/Significance
          We demonstrate that, combined with classical algorithms, crowd computing techniques can be successfully used to help improving the accuracy of MSA. More importantly, we show that an NP-hard computational problem can be embedded in casual game that can be easily played by people without significant scientific training. This suggests that citizen science approaches can be used to exploit the billions of human-brain peta-flops of computation that are spent every day playing games. Phylo is available at: http://phylo.cs.mcgill.ca.},
author = {Kawrykow, Alexander and Roumanis, Gary and Kam, Alfred and Kwak, Daniel and Leung, Clarence and Wu, Chu and Zarour, Eleyine and Sarmenta, Luis and Blanchette, Mathieu and Waldisp\"{u}hl, J\'{e}r\^{o}me},
doi = {10.1371/journal.pone.0031362},
editor = {Michalak, Pawel},
file = {:home/gus/Documents/Mendeley Desktop/Kawrykow et al.\_2012\_Phylo A Citizen Science Approach for Improving Multiple Sequence Alignment.pdf:pdf},
issn = {1932-6203},
journal = {PLoS ONE},
keywords = {Biology,Computational Biology,Computer Science,Computer applications,Computing methods,Evolutionary Biology,Genetics,Genetics and Genomics,Genomics,Organismal evolution,Research Article},
month = mar,
number = {3},
pages = {e31362},
publisher = {Public Library of Science},
title = {{Phylo: A Citizen Science Approach for Improving Multiple Sequence Alignment}},
url = {http://dx.plos.org/10.1371/journal.pone.0031362},
volume = {7},
year = {2012}
}
@article{shi2010parallel,
author = {Shi, H. and Schmidt, B. and Liu, W. and M\"{u}ller-Wittig, W.},
file = {:home/gus/Documents/Mendeley Desktop/Shi et al.\_2010\_A parallel algorithm for error correction in high-throughput short-read data on CUDA-enabled graphics hardware.pdf:pdf},
journal = {Journal of Computational Biology},
number = {4},
pages = {603--615},
publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
title = {{A parallel algorithm for error correction in high-throughput short-read data on CUDA-enabled graphics hardware}},
url = {http://www.liebertonline.com/doi/abs/10.1089/cmb.2009.0062},
volume = {17},
year = {2010}
}
@article{Marcais2011,
abstract = {Counting the number of occurrences of every k-mer (substring of length k) in a long string is a central subproblem in many applications, including genome assembly, error correction of sequencing reads, fast multiple sequence alignment and repeat detection. Recently, the deep sequence coverage generated by next-generation sequencing technologies has caused the amount of sequence to be processed during a genome project to grow rapidly, and has rendered current k-mer counting tools too slow and memory intensive. At the same time, large multicore computers have become commonplace in research facilities allowing for a new parallel computational paradigm.},
author = {Mar\c{c}ais, Guillaume and Kingsford, Carl},
doi = {10.1093/bioinformatics/btr011},
file = {:home/gus/Documents/Mendeley Desktop/Mar\c{c}ais, Kingsford\_2011\_A fast, lock-free approach for efficient parallel counting of occurrences of k-mers.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Mar\c{c}ais, Kingsford\_2011\_A fast, lock-free approach for efficient parallel counting of occurrences of k-mers(2).pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Animals,Base Sequence,Computational Biology,Computational Biology: methods,DNA,DNA: methods,Genome,Humans,Sequence Alignment,Sequence Analysis,Software},
month = mar,
number = {6},
pages = {764--70},
pmid = {21217122},
title = {{A fast, lock-free approach for efficient parallel counting of occurrences of k-mers.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/27/6/764},
volume = {27},
year = {2011}
}
@article{Maccallum:2009vn,
abstract = {We demonstrate that genome sequences approaching finished quality can be generated from short paired reads. Using 36 base (fragment) and 26 base (jumping) reads from five microbial genomes of varied GC composition and sizes up to 40 Mb, ALLPATHS2 generated assemblies with long, accurate contigs and scaffolds. Velvet and EULER-SR were less accurate. For example, for Escherichia coli, the fraction of 10-kb stretches that were perfect was 99.8\% (ALLPATHS2), 68.7\% (Velvet), and 42.1\% (EULER-SR).},
author = {Maccallum, Iain and Przybylski, Dariusz and Gnerre, Sante and Burton, Joshua and Shlyakhter, Ilya and Gnirke, Andreas and Malek, Joel and McKernan, Kevin and Ranade, Swati and Shea, Terrance P and Williams, Louise and Young, Sarah and Nusbaum, Chad and Jaffe, David B},
doi = {10.1186/gb-2009-10-10-r103},
file = {:home/gus/Documents/Mendeley Desktop//Maccallum et al.\_2009\_ALLPATHS 2 small genomes assembled accurately and with high continuity from short paired reads.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Bacteria,Bacteria: genetics,Base Pairing,Base Pairing: genetics,Fungi,Fungi: genetics,Genome,Genome: genetics,Genomics,Genomics: methods,Reproducibility of Results,Software},
month = jan,
number = {10},
pages = {R103},
pmid = {19796385},
title = {{ALLPATHS 2: small genomes assembled accurately and with high continuity from short paired reads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2784318\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Roberts2004,
abstract = {MOTIVATION: Comparison of nucleic acid and protein sequences is a fundamental tool of modern bioinformatics. A dominant method of such string matching is the 'seed-and-extend' approach, in which occurrences of short subsequences called 'seeds' are used to search for potentially longer matches in a large database of sequences. Each such potential match is then checked to see if it extends beyond the seed. To be effective, the seed-and-extend approach needs to catalogue seeds from virtually every substring in the database of search strings. Projects such as mammalian genome assemblies and large-scale protein matching, however, have such large sequence databases that the resulting list of seeds cannot be stored in RAM on a single computer. This significantly slows the matching process. RESULTS: We present a simple and elegant method in which only a small fraction of seeds, called 'minimizers', needs to be stored. Using minimizers can speed up string-matching computations by a large factor while missing only a small fraction of the matches found using all seeds.},
author = {Roberts, Michael and Hayes, Wayne and Hunt, Brian R and Mount, Stephen M and Yorke, James A},
doi = {10.1093/bioinformatics/bth408},
file = {:home/gus/Documents/Mendeley Desktop/Roberts et al.\_2004\_Reducing storage requirements for biological sequence comparison.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Roberts et al.\_2004\_Reducing storage requirements for biological sequence comparison(3).pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computer-Assisted,Databases,Genetic,Information Storage and Retrieval,Information Storage and Retrieval: methods,Numerical Analysis,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Sequence Analysis: methods},
month = dec,
number = {18},
pages = {3363--9},
pmid = {15256412},
title = {{Reducing storage requirements for biological sequence comparison.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/20/18/3363},
volume = {20},
year = {2004}
}
@article{Ilie2011c,
abstract = {Multiple spaced seeds represent the current state-of-the-art for similarity search in bioinformatics, with applications in various areas such as sequence alignment, read mapping, oligonucleotide design, etc. We present SpEED, a software program that computes highly sensitive multiple spaced seeds. SpEED can be several orders of magnitude faster and computes better seeds than the existing leading software programs.},
author = {Ilie, Lucian and Ilie, Silvana and {Mansouri Bigvand}, Anahita},
doi = {10.1093/bioinformatics/btr368},
file = {:home/gus/Documents/Mendeley Desktop/Ilie, Ilie, Mansouri Bigvand\_2011\_SpEED fast computation of sensitive spaced seeds.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = sep,
number = {17},
pages = {2433--4},
pmid = {21690104},
title = {{SpEED: fast computation of sensitive spaced seeds.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/27/17/2433},
volume = {27},
year = {2011}
}
@misc{JeffGilchrist2008,
abstract = {In this paper, we present parallel algorithms for lossless data compression based on the Burrows-Wheeler Transform (BWT) block-sorting technique. We investigate the performance of using data parallelism and task parallelism for both multi-threaded and message-passing programming. The output produced by the parallel algorithms is fully compatible with their sequential counterparts. To balance the workload among processors we develop a task scheduling strategy. An extensive set of experiments is performed with a shared memory NUMA system using up to 120 processors and on a distributed memory cluster using up to 100 processors. Our experimental results show that significant speedup can be achieved with both data parallel and task parallel methodologies. These algorithms will greatly reduce the amount of time it takes to compress large amounts of data while the compressed data remains in a form that users without access to multiple processor systems can still use.},
author = {{Jeff Gilchrist} and {Aysegul Cuhadar}},
booktitle = {International Journal of Web and Grid Services},
doi = {10.1504/IJWGS.2008.018498},
file = {:home/gus/Documents/Mendeley Desktop/Jeff Gilchrist, Aysegul Cuhadar\_2008\_Parallel lossless data compression using the Burrows-Wheeler Transform.html:html},
keywords = {BWT,Burrows-Wheeler Transform,MPI,bzip2,data parallelism,distributed computing,lossless data compression,message passing interface,multi-threading,parallel computing,task parallelism,task scheduling.,workload balance},
pages = {117--135},
title = {{Parallel lossless data compression using the Burrows-Wheeler Transform}},
url = {http://www.inderscience.com/search/index.php?action=record\&rec\_id=18498\&prevQuery=\&ps=10\&m=or},
urldate = {11/10/11},
volume = {4},
year = {2008}
}
@article{Anders2010,
abstract = {High-throughput sequencing assays such as RNA-Seq, ChIP-Seq or barcode counting provide quantitative readouts in the form of count data. To infer differential signal in such data correctly and with good statistical power, estimation of data variability throughout the dynamic range and a suitable error model are required. We propose a method based on the negative binomial distribution, with variance and mean linked by local regression and present an implementation, DESeq, as an R/Bioconductor package.},
author = {Anders, Simon and Huber, Wolfgang},
doi = {10.1186/gb-2010-11-10-r106},
file = {:home/gus/Documents/Mendeley Desktop/Anders, Huber\_2010\_Differential expression analysis for sequence count data.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Animals,Binomial Distribution,Chromatin Immunoprecipitation,Chromatin Immunoprecipitation: methods,Computational Biology,Computational Biology: methods,Drosophila,Drosophila: genetics,Gene Expression Profiling,Gene Expression Profiling: methods,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Linear Models,Models, Genetic,Saccharomyces cerevisiae,Saccharomyces cerevisiae: genetics,Sequence Analysis, RNA,Sequence Analysis, RNA: methods,Stem Cells,Tissue Culture Techniques},
month = jan,
number = {10},
pages = {R106},
pmid = {20979621},
title = {{Differential expression analysis for sequence count data.}},
url = {http://genomebiology.com/2010/11/10/R106},
volume = {11},
year = {2010}
}
@article{10.1371/journal.pone.0005350,
abstract = { <p>The success of genome-wide association (GWA) studies for the detection of sequence variation affecting complex traits in human has spurred interest in the use of large-scale high-density single nucleotide polymorphism (SNP) genotyping for the identification of quantitative trait loci (QTL) and for marker-assisted selection in model and agricultural species. A cost-effective and efficient approach for the development of a custom genotyping assay interrogating 54,001 SNP loci to support GWA applications in cattle is described. A novel algorithm for achieving a compressed inter-marker interval distribution proved remarkably successful, with median interval of 37 kb and maximum predicted gap of \&lt;350 kb. The assay was tested on a panel of 576 animals from 21 cattle breeds and six outgroup species and revealed that from 39,765 to 46,492 SNP are polymorphic within individual breeds (average minor allele frequency (MAF) ranging from 0.24 to 0.27). The assay also identified 79 putative copy number variants in cattle. Utility for GWA was demonstrated by localizing known variation for coat color and the presence/absence of horns to their correct genomic locations. The combination of SNP selection and the novel spacing algorithm allows an efficient approach for the development of high-density genotyping platforms in species having full or even moderate quality draft sequence. Aspects of the approach can be exploited in species which lack an available genome sequence. The BovineSNP50 assay described here is commercially available from Illumina and provides a robust platform for mapping disease genes and QTL in cattle.</p> },
author = {K., Lakshmi and T., Lawley Cynthia and D., Schnabel Robert and F., Taylor Jeremy and F., Allan Mark and P., Heaton Michael and Jeff, O'Connell and S., Moore Stephen and L., Smith Timothy P and S., Sonstegard Tad and Matukumalli, Van Tassell Curtis P},
doi = {10.1371/journal.pone.0005350},
journal = {PLoS ONE},
number = {4},
pages = {e5350},
publisher = {Public Library of Science},
title = {{Development and Characterization of a High Density SNP Genotyping Assay for Cattle}},
url = {http://dx.doi.org/10.1371/journal.pone.0005350},
volume = {4},
year = {2009}
}
@article{Dean2008,
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
file = {:home/gus/Documents/Mendeley Desktop/Dean, Ghemawat\_2008\_MapReduce.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = jan,
number = {1},
pages = {107},
title = {{MapReduce}},
url = {http://dl.acm.org/citation.cfm?id=1327452.1327492},
volume = {51},
year = {2008}
}
@article{Hernandez2008,
abstract = {Novel high-throughput DNA sequencing technologies allow researchers to characterize a bacterial genome during a single experiment and at a moderate cost. However, the increase in sequencing throughput that is allowed by using such platforms is obtained at the expense of individual sequence read length, which must be assembled into longer contigs to be exploitable. This study focuses on the Illumina sequencing platform that produces millions of very short sequences that are 35 bases in length. We propose a de novo assembler software that is dedicated to process such data. Based on a classical overlap graph representation and on the detection of potentially spurious reads, our software generates a set of accurate contigs of several kilobases that cover most of the bacterial genome. The assembly results were validated by comparing data sets that were obtained experimentally for Staphylococcus aureus strain MW2 and Helicobacter acinonychis strain Sheeba with that of their published genomes acquired by conventional sequencing of 1.5- to 3.0-kb fragments. We also provide indications that the broad coverage achieved by high-throughput sequencing might allow for the detection of clonal polymorphisms in the set of DNA molecules being sequenced.},
author = {Hernandez, David and Fran\c{c}ois, Patrice and Farinelli, Laurent and Oster\aa s, Magne and Schrenzel, Jacques},
doi = {10.1101/gr.072033.107},
file = {:home/gus/Documents/Mendeley Desktop/Hernandez et al.\_2008\_De novo bacterial genome sequencing millions of very short reads assembled on a desktop computer.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Chromosome Mapping,Genome, Bacterial,Genome, Bacterial: genetics,Helicobacter,Helicobacter: classification,Helicobacter: genetics,Microcomputers,Polymorphism, Genetic,Reproducibility of Results,Sequence Analysis, DNA,Sequence Analysis, DNA: economics,Sequence Analysis, DNA: instrumentation,Sequence Analysis, DNA: methods,Software,Staphylococcus aureus,Staphylococcus aureus: classification,Staphylococcus aureus: genetics},
month = may,
number = {5},
pages = {802--9},
pmid = {18332092},
title = {{De novo bacterial genome sequencing: millions of very short reads assembled on a desktop computer.}},
url = {http://genome.cshlp.org/cgi/content/abstract/18/5/802},
volume = {18},
year = {2008}
}
@article{Schatz:2010uq,
abstract = {Second-generation sequencing technology can now be used to sequence an entire human genome in a matter of days and at low cost. Sequence read lengths, initially very short, have rapidly increased since the technology first appeared, and we now are seeing a growing number of efforts to sequence large genomes de novo from these short reads. In this Perspective, we describe the issues associated with short-read assembly, the different types of data produced by second-gen sequencers, and the latest assembly algorithms designed for these data. We also review the genomes that have been assembled recently from short reads and make recommendations for sequencing strategies that will yield a high-quality assembly.},
author = {Schatz, Michael C and Delcher, Arthur L and Salzberg, Steven L},
doi = {10.1101/gr.101360.109},
file = {:home/gus/Documents/Mendeley Desktop/Schatz, Delcher, Salzberg\_2010\_Assembly of large genomes using second-generation sequencing.pdf:pdf},
issn = {1549-5469},
journal = {Genome research},
keywords = {Algorithms,Base Sequence,DNA,DNA: methods,Genome,Genomics,Genomics: methods,Human,Humans,Sequence Analysis},
month = sep,
number = {9},
pages = {1165--73},
pmid = {20508146},
title = {{Assembly of large genomes using second-generation sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2928494\&tool=pmcentrez\&rendertype=abstract},
volume = {20},
year = {2010}
}
@article{Grabherr2011,
abstract = {Massively parallel sequencing of cDNA has enabled deep and efficient probing of transcriptomes. Current approaches for transcript reconstruction from such data often rely on aligning reads to a reference genome, and are thus unsuitable for samples with a partial or missing reference genome. Here we present the Trinity method for de novo assembly of full-length transcripts and evaluate it on samples from fission yeast, mouse and whitefly, whose reference genome is not yet available. By efficiently constructing and analyzing sets of de Bruijn graphs, Trinity fully reconstructs a large fraction of transcripts, including alternatively spliced isoforms and transcripts from recently duplicated genes. Compared with other de novo transcriptome assemblers, Trinity recovers more full-length transcripts across a broad range of expression levels, with a sensitivity similar to methods that rely on genome alignments. Our approach provides a unified solution for transcriptome reconstruction in any sample, especially in the absence of a reference genome.},
author = {Grabherr, Manfred G and Haas, Brian J and Yassour, Moran and Levin, Joshua Z and Thompson, Dawn A and Amit, Ido and Adiconis, Xian and Fan, Lin and Raychowdhury, Raktima and Zeng, Qiandong and Chen, Zehua and Mauceli, Evan and Hacohen, Nir and Gnirke, Andreas and Rhind, Nicholas and di Palma, Federica and Birren, Bruce W and Nusbaum, Chad and Lindblad-Toh, Kerstin and Friedman, Nir and Regev, Aviv},
doi = {10.1038/nbt.1883},
file = {:home/gus/Documents/Mendeley Desktop/Grabherr et al.\_2011\_Full-length transcriptome assembly from RNA-Seq data without a reference genome.pdf:pdf},
issn = {1546-1696},
journal = {Nature biotechnology},
month = may,
number = {7},
pages = {644--652},
pmid = {21572440},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nat Biotech},
title = {{Full-length transcriptome assembly from RNA-Seq data without a reference genome.}},
url = {http://dx.doi.org/10.1038/nbt.1883},
volume = {29},
year = {2011}
}
@article{Ruffalo2011,
abstract = {Motivation: The advent of next-generation sequencing (NGS) techniques presents many novel opportunities for many applications in life sciences. The vast number of short reads produced by these techniques, however, pose significant computational challenges. The first step in many types of genomic analysis is the mapping of short reads to a reference genome, and several groups have developed dedicated algorithms and software packages to perform this function. As the developers of these packages optimize their algorithms with respect to various considerations, the relative merits of different software packages remain unclear. However, for scientists who generate and use NGS data for their specific research projects, an important consideration is choosing the software that is most suitable for their application. Results: With a view to comparing existing short read alignment software, we develop a simulation and evaluation suite, SEAL, which simulates NGS runs for different configurations of various factors, including sequencing error, indels and coverage. We also develop criteria to compare the performances of software with disparate output structure (e.g. some packages return a single alignment while some return multiple possible alignments). Using these criteria, we comprehensively evaluate the performances of Bowtie, BWA, mr- and mrsFAST, Novoalign, SHRiMP and SOAPv2, with regard to accuracy and runtime. Conclusion: We expect that the results presented here will be useful to investigators in choosing the alignment software that is most suitable for their specific research aims. Our results also provide insights into the factors that should be considered to use alignment results effectively. SEAL can also be used to evaluate the performance of algorithms that use deep sequencing data for various purposes (e.g. identification of genomic variants). Availability: SEAL is available as open source at http://compbio.case.edu/seal/. Contact: matthew.ruffalo@case.edu Supplementary information: Supplementary data are available at Bioinformatics online.},
author = {Ruffalo, M. and LaFramboise, T. and Koyuturk, M.},
doi = {10.1093/bioinformatics/btr477},
file = {:home/gus/Documents/Mendeley Desktop/Ruffalo, LaFramboise, Koyuturk\_2011\_Comparative analysis of algorithms for next-generation sequencing read alignment.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = aug,
number = {20},
pages = {2790--2796},
title = {{Comparative analysis of algorithms for next-generation sequencing read alignment}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/27/20/2790},
volume = {27},
year = {2011}
}
@article{Almeida2007,
author = {Almeida, Paulo S\'{e}rgio and Baquero, Carlos and Pregui\c{c}a, Nuno and Hutchison, David},
issn = {0020-0190},
journal = {Information Processing Letters},
keywords = {Bloom filters,Data structures,Distributed systems,Randomized algorithms},
month = mar,
number = {6},
pages = {255--261},
title = {{Scalable Bloom Filters}},
url = {http://www.sciencedirect.com/science/article/pii/S0020019006003127},
volume = {101},
year = {2007}
}
@inproceedings{Chazelle:2004:BFE:982792.982797,
address = {Philadelphia, PA, USA},
author = {Chazelle, Bernard and Kilian, Joe and Rubinfeld, Ronitt and Tal, Ayellet},
booktitle = {Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms},
isbn = {0-89871-558-X},
pages = {30--39},
publisher = {Society for Industrial and Applied Mathematics},
series = {SODA '04},
title = {{The Bloomier filter: an efficient data structure for static support lookup tables}},
url = {http://dl.acm.org/citation.cfm?id=982792.982797},
year = {2004}
}
@article{Schatz:2010uq,
abstract = {Second-generation sequencing technology can now be used to sequence an entire human genome in a matter of days and at low cost. Sequence read lengths, initially very short, have rapidly increased since the technology first appeared, and we now are seeing a growing number of efforts to sequence large genomes de novo from these short reads. In this Perspective, we describe the issues associated with short-read assembly, the different types of data produced by second-gen sequencers, and the latest assembly algorithms designed for these data. We also review the genomes that have been assembled recently from short reads and make recommendations for sequencing strategies that will yield a high-quality assembly.},
author = {Schatz, Michael C and Delcher, Arthur L and Salzberg, Steven L},
doi = {10.1101/gr.101360.109},
journal = {Genome Res},
month = sep,
number = {9},
pages = {1165--1173},
pmid = {20508146},
title = {{Assembly of large genomes using second-generation sequencing}},
volume = {20},
year = {2010}
}
@article{Zimin2009,
abstract = {The genome of the domestic cow, Bos taurus, was sequenced using a mixture of hierarchical and whole-genome shotgun sequencing methods.},
author = {Zimin, Aleksey V and Delcher, Arthur L and Florea, Liliana and Kelley, David R and Schatz, Michael C and Puiu, Daniela and Hanrahan, Finnian and Pertea, Geo and {Van Tassell}, Curtis P and Sonstegard, Tad S and Mar\c{c}ais, Guillaume and Roberts, Michael and Subramanian, Poorani and Yorke, James a and Salzberg, Steven L},
doi = {10.1186/gb-2009-10-4-r42},
file = {:home/gus/Documents/Mendeley Desktop/Zimin et al.\_2009\_A whole-genome assembly of the domestic cow, Bos taurus.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Animals,Cattle,Cattle: genetics,Chromosome Mapping,Female,Genome,Genome, Human,Genome, Human: genetics,Genome: genetics,Genomics,Humans,Male,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Sequence Analysis, DNA: statistics \& numerical dat,Synteny,Y Chromosome,Y Chromosome: genetics},
month = jan,
number = {4},
pages = {R42},
pmid = {19393038},
title = {{A whole-genome assembly of the domestic cow, Bos taurus.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2688933\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Gao,
author = {Gao, H. and Groote, J.F. and Hesselink, W.H.},
doi = {10.1109/IPDPS.2004.1302969},
file = {:home/gus/Documents/Mendeley Desktop/Gao, Groote, Hesselink\_Unknown\_Almost wait-free resizable hashtables.pdf:pdf},
isbn = {0-7695-2132-0},
journal = {18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.},
pages = {50--58},
publisher = {Ieee},
title = {{Almost wait-free resizable hashtables}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1302969}
}
@article{medvedev2007computability,
author = {Medvedev, P. and Georgiou, K. and Myers, G. and Brudno, M.},
file = {:home/gus/Documents/Mendeley Desktop/Medvedev et al.\_2007\_Computability of models for sequence assembly.pdf:pdf},
journal = {Algorithms in Bioinformatics},
pages = {289--301},
publisher = {Springer},
title = {{Computability of models for sequence assembly}},
url = {http://www.springerlink.com/index/H711368771048H21.pdf},
year = {2007}
}
@article{Butler2008,
abstract = {New DNA sequencing technologies deliver data at dramatically lower costs but demand new analytical methods to take full advantage of the very short reads that they produce. We provide an initial, theoretical solution to the challenge of de novo assembly from whole-genome shotgun "microreads." For 11 genomes of sizes up to 39 Mb, we generated high-quality assemblies from 80x coverage by paired 30-base simulated reads modeled after real Illumina-Solexa reads. The bacterial genomes of Campylobacter jejuni and Escherichia coli assemble optimally, yielding single perfect contigs, and larger genomes yield assemblies that are highly connected and accurate. Assemblies are presented in a graph form that retains intrinsic ambiguities such as those arising from polymorphism, thereby providing information that has been absent from previous genome assemblies. For both C. jejuni and E. coli, this assembly graph is a single edge encompassing the entire genome. Larger genomes produce more complicated graphs, but the vast majority of the bases in their assemblies are present in long edges that are nearly always perfect. We describe a general method for genome assembly that can be applied to all types of DNA sequence data, not only short read data, but also conventional sequence reads.},
author = {Butler, Jonathan and MacCallum, Iain and Kleber, Michael and Shlyakhter, Ilya a and Belmonte, Matthew K and Lander, Eric S and Nusbaum, Chad and Jaffe, David B},
doi = {10.1101/gr.7337908},
file = {:home/gus/Documents/Mendeley Desktop/Butler et al.\_2008\_ALLPATHS de novo assembly of whole-genome shotgun microreads.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Campylobacter jejuni,Campylobacter jejuni: genetics,Computational Biology,Computational Biology: methods,Computer Simulation,Escherichia coli,Escherichia coli: genetics,Genome, Bacterial,Genome, Bacterial: genetics,Reproducibility of Results,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Sequence Analysis, DNA: standards},
month = may,
number = {5},
pages = {810--20},
pmid = {18340039},
title = {{ALLPATHS: de novo assembly of whole-genome shotgun microreads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2336810\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@article{Miller:2010ys,
abstract = {The emergence of next-generation sequencing platforms led to resurgence of research in whole-genome shotgun assembly algorithms and software. DNA sequencing data from the Roche 454, Illumina/Solexa, and ABI SOLiD platforms typically present shorter read lengths, higher coverage, and different error profiles compared with Sanger sequencing data. Since 2005, several assembly software packages have been created or revised specifically for de novo assembly of next-generation sequencing data. This review summarizes and compares the published descriptions of packages named SSAKE, SHARCGS, VCAKE, Newbler, Celera Assembler, Euler, Velvet, ABySS, AllPaths, and SOAPdenovo. More generally, it compares the two standard methods known as the de Bruijn graph approach and the overlap/layout/consensus approach to assembly.},
author = {Miller, Jason R and Koren, Sergey and Sutton, Granger},
doi = {10.1016/j.ygeno.2010.03.001},
journal = {Genomics},
month = jun,
number = {6},
pages = {315--327},
pmid = {20211242},
title = {{Assembly algorithms for next-generation sequencing data}},
volume = {95},
year = {2010}
}
@misc{TheMendeleySupportTeam2011a,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:home/gus/Documents/Mendeley Desktop/The Mendeley Support Team\_2011\_Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@article{Reed2004,
abstract = {A family of probability densities, which has proved useful in modelling the size distributions of various phenomens, including incomes and earnings, human settlement sizes, oil-field volumes and particle sizes, is introduced. The distribution, named herein as the double Pareto-lognormal or dPlN distribution, arises as that of the state of a geometric Brownian motion (GBM), with lognormally distributed initial state, after an exponentially distributed length of time (or equivalently as the distribution of the killed state of such a GBM with constant killing rate). A number of phenomena can be viewed as resulting from such a process (e.g., incomes, settlement sizes), which explains the good fit. Properties of the distribution are derived and estimation methods discussed. The distribution exhibits Paretian power-law) behaviour in both tails, and when plotted on logarithmic axes, its density exhibits hyperbolic-type behaviour.},
author = {Reed, William J. and Jorgensen, Murray},
file = {:home/gus/Documents/Mendeley Desktop/Reed, Jorgensen\_2004\_The double Pareto-lognormal distributiona new parametric model for size distributions.pdf:pdf},
journal = {Communications in Statistics-Theory \ldots},
number = {8},
pages = {1733--1753},
title = {{The double Pareto-lognormal distributiona new parametric model for size distributions}},
url = {http://www.tandfonline.com/doi/abs/10.1081/STA-120037438},
volume = {33},
year = {2004}
}
@article{michael2004hazard,
author = {Michael, M.M.},
file = {:home/gus/Documents/Mendeley Desktop/Michael\_2004\_Hazard pointers Safe memory reclamation for lock-free objects.pdf:pdf},
journal = {Parallel and Distributed Systems, IEEE Transactions on},
number = {6},
pages = {491--504},
publisher = {IEEE},
title = {{Hazard pointers: Safe memory reclamation for lock-free objects}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1291819},
volume = {15},
year = {2004}
}
@article{Phenix2011,
abstract = {Inferring regulatory and metabolic network models from quantitative genetic interaction data remains a major challenge in systems biology. Here, we present a novel quantitative model for interpreting epistasis within pathways responding to an external signal. The model provides the basis of an experimental method to determine the architecture of such pathways, and establishes a new set of rules to infer the order of genes within them. The method also allows the extraction of quantitative parameters enabling a new level of information to be added to genetic network models. It is applicable to any system where the impact of combinatorial loss-of-function mutations can be quantified with sufficient accuracy. We test the method by conducting a systematic analysis of a thoroughly characterized eukaryotic gene network, the galactose utilization pathway in Saccharomyces cerevisiae. For this purpose, we quantify the effects of single and double gene deletions on two phenotypic traits, fitness and reporter gene expression. We show that applying our method to fitness traits reveals the order of metabolic enzymes and the effects of accumulating metabolic intermediates. Conversely, the analysis of expression traits reveals the order of transcriptional regulatory genes, secondary regulatory signals and their relative strength. Strikingly, when the analyses of the two traits are combined, the method correctly infers 80\% of the known relationships without any false positives.},
author = {Phenix, Hilary and Morin, Katy and Batenchuk, Cory and Parker, Jacob and Abedi, Vida and Yang, Liu and Tepliakova, Lioudmila and Perkins, Theodore J and K\ae rn, Mads},
doi = {10.1371/journal.pcbi.1002048},
editor = {Ohler, Uwe},
file = {:home/gus/Documents/Mendeley Desktop/Phenix et al.\_2011\_Quantitative epistasis analysis and pathway inference from genetic interaction data.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Biology,Computational Biology,Functional genomics,Gene expression,Gene regulation,Genomics,Metabolic networks,Molecular genetics,Regulatory networks,Research Article,Signaling networks,Systems biology},
month = may,
number = {5},
pages = {e1002048},
pmid = {21589890},
publisher = {Public Library of Science},
title = {{Quantitative epistasis analysis and pathway inference from genetic interaction data.}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1002048},
volume = {7},
year = {2011}
}
@article{Pugh1990,
author = {Pugh, William},
doi = {10.1145/78973.78977},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {data structures,searching,trees},
month = jun,
number = {6},
pages = {668--676},
title = {{Skip lists: a probabilistic alternative to balanced trees}},
url = {http://dl.acm.org/citation.cfm?id=78973.78977},
volume = {33},
year = {1990}
}
@inproceedings{Fomitchev2004,
address = {New York, New York, USA},
annote = {http://drdobbs.com/parallel/208801371},
author = {Fomitchev, Mikhail and Ruppert, Eric},
booktitle = {Proceedings of the twenty-third annual ACM symposium on Principles of distributed computing - PODC '04},
doi = {10.1145/1011767.1011776},
file = {:home/gus/Documents/Mendeley Desktop/Fomitchev, Ruppert\_2004\_Lock-free linked lists and skip lists.pdf:pdf},
isbn = {1581138024},
keywords = {amortized analysis,analysis,distributed,efficient,fault-tolerant,linked list,lock-free,skip list},
month = jul,
pages = {50},
publisher = {ACM Press},
title = {{Lock-free linked lists and skip lists}},
url = {http://dl.acm.org/citation.cfm?id=1011767.1011776},
year = {2004}
}
@article{Au2010,
abstract = {Alternative splicing is a prevalent post-transcriptional process, which is not only important to normal cellular function but is also involved in human diseases. The newly developed second generation sequencing technique provides high-throughput data (RNA-seq data) to study alternative splicing events in different types of cells. Here, we present a computational method, SpliceMap, to detect splice junctions from RNA-seq data. This method does not depend on any existing annotation of gene structures and is capable of finding novel splice junctions with high sensitivity and specificity. It can handle long reads (50-100 nt) and can exploit paired-read information to improve mapping accuracy. Several parameters are included in the output to indicate the reliability of the predicted junction and help filter out false predictions. We applied SpliceMap to analyze 23 million paired 50-nt reads from human brain tissue. The results show at this depth of sequencing, RNA-seq can support reliable detection of splice junctions except for those that are present at very low level. Compared to current methods, SpliceMap can achieve 12\% higher sensitivity without sacrificing specificity.},
author = {Au, Kin Fai and Jiang, Hui and Lin, Lan and Xing, Yi and Wong, Wing Hung},
doi = {10.1093/nar/gkq211},
file = {:home/gus/Documents/Mendeley Desktop/Au et al.\_2010\_Detection of splice junctions from paired-end RNA-seq data by SpliceMap.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Au et al.\_2010\_Detection of splice junctions from paired-end RNA-seq data by SpliceMap(2).pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Algorithms,Alternative Splicing,Computational Biology,Computational Biology: methods,Humans,Polymerase Chain Reaction,RNA,RNA Splice Sites,Sequence Analysis,Software},
month = aug,
number = {14},
pages = {4570--8},
pmid = {20371516},
title = {{Detection of splice junctions from paired-end RNA-seq data by SpliceMap.}},
url = {http://nar.oxfordjournals.org/cgi/content/abstract/gkq211v1},
volume = {38},
year = {2010}
}
@inproceedings{Lin2010,
address = {New York, New York, USA},
author = {Lin, Jimmy and Schatz, Michael},
booktitle = {Proceedings of the Eighth Workshop on Mining and Learning with Graphs - MLG '10},
doi = {10.1145/1830252.1830263},
file = {:home/gus/Documents/Mendeley Desktop/Lin, Schatz\_2010\_Design patterns for efficient graph algorithms in MapReduce.pdf:pdf},
isbn = {9781450302142},
month = jul,
pages = {78--85},
publisher = {ACM Press},
title = {{Design patterns for efficient graph algorithms in MapReduce}},
url = {http://dl.acm.org/citation.cfm?id=1830252.1830263},
year = {2010}
}
@book{Colden1755,
address = {London},
author = {Colden, Cadwallader},
edition = {3rd editio},
file = {:home/gus/Documents/Mendeley Desktop/Colden\_1755\_The History of the Five Indian Nations of Canada.jpeg:jpeg},
pages = {Vol. II 18--24},
title = {{The History of the Five Indian Nations of Canada}},
url = {http://america.eb.com/america/article?articleId=385137\&query=Iroquois},
year = {1755}
}
@article{Zerbino2008,
abstract = {We have developed a new set of algorithms, collectively called "Velvet," to manipulate de Bruijn graphs for genomic sequence assembly. A de Bruijn graph is a compact representation based on short words (k-mers) that is ideal for high coverage, very short read (25-50 bp) data sets. Applying Velvet to very short reads and paired-ends information only, one can produce contigs of significant length, up to 50-kb N50 length in simulations of prokaryotic data and 3-kb N50 on simulated mammalian BACs. When applied to real Solexa data sets without read pairs, Velvet generated contigs of approximately 8 kb in a prokaryote and 2 kb in a mammalian BAC, in close agreement with our simulated results without read-pair information. Velvet represents a new approach to assembly that can leverage very short reads in combination with read pairs to produce useful assemblies.},
author = {Zerbino, Daniel R and Birney, Ewan},
doi = {10.1101/gr.074492.107},
file = {:home/gus/Documents/Mendeley Desktop/Zerbino, Birney\_2008\_Velvet algorithms for de novo short read assembly using de Bruijn graphs.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Animals,Artificial,Bacterial,Chromosomes,Computational Biology,Computational Biology: methods,Computer Simulation,DNA,DNA: methods,DNA: standards,Genome,Genomics,Human,Humans,Mammals,Mammals: genetics,Sequence Analysis,Streptococcus,Streptococcus: genetics},
month = may,
number = {5},
pages = {821--9},
pmid = {18349386},
title = {{Velvet: algorithms for de novo short read assembly using de Bruijn graphs}},
url = {http://genome.cshlp.org/content/18/5/821.short http://genome.cshlp.org/cgi/content/abstract/18/5/821},
volume = {18},
year = {2008}
}
@article{Kao2011,
abstract = {Developing accurate, scalable algorithms to improve data quality is an important computational challenge associated with recent advances in high-throughput sequencing technology. In this study, a novel error-correction algorithm, called ECHO, is introduced for correcting base-call errors in short-reads, without the need of a reference genome. Unlike most previous methods, ECHO does not require the user to specify parameters of which optimal values are typically unknown a priori. ECHO automatically sets the parameters in the assumed model and estimates error characteristics specific to each sequencing run, while maintaining a running time that is within the range of practical use. ECHO is based on a probabilistic model and is able to assign a quality score to each corrected base. Furthermore, it explicitly models heterozygosity in diploid genomes and provides a reference-free method for detecting bases that originated from heterozygous sites. On both real and simulated data, ECHO is able to improve the accuracy of previous error-correction methods by several folds to an order of magnitude, depending on the sequence coverage depth and the position in the read. The improvement is most pronounced toward the end of the read, where previous methods become noticeably less effective. Using a whole-genome yeast data set, it is demonstrated here that ECHO is capable of coping with nonuniform coverage. Also, it is shown that using ECHO to perform error correction as a preprocessing step considerably facilitates de novo assembly, particularly in the case of low-to-moderate sequence coverage depth.},
author = {Kao, Wei-Chun and Chan, Andrew H and Song, Yun S},
doi = {10.1101/gr.111351.110},
file = {:home/gus/Documents/Mendeley Desktop/Kao, Chan, Song\_2011\_ECHO A reference-free short-read error correction algorithm.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Kao, Chan, Song\_2011\_ECHO A reference-free short-read error correction algorithm(2).pdf:pdf},
issn = {1549-5469},
journal = {Genome research},
month = may,
number = {7},
pages = {1181--1192},
pmid = {21482625},
title = {{ECHO: A reference-free short-read error correction algorithm.}},
url = {http://genome.cshlp.org/cgi/content/abstract/21/7/1181},
volume = {21},
year = {2011}
}
@article{Chin2011,
abstract = {Recent advances in genome technologies and the ensuing outpouring of genomic information related to cancer have accelerated the convergence of discovery science and clinical medicine. Successful examples of translating cancer genomics into therapeutics and diagnostics reinforce its potential to make possible personalized cancer medicine. However, the bottlenecks along the path of converting a genome discovery into a tangible clinical endpoint are numerous and formidable. In this Perspective, we emphasize the importance of establishing the biological relevance of a cancer genomic discovery in realizing its clinical potential and discuss some of the major obstacles to moving from the bench to the bedside.},
author = {Chin, Lynda and Andersen, Jannik N and Futreal, P Andrew},
doi = {10.1038/nm.2323},
file = {:home/gus/Documents/Mendeley Desktop/Chin, Andersen, Futreal\_2011\_Cancer genomics from discovery science to personalized medicine.pdf:pdf},
issn = {1546-170X},
journal = {Nature medicine},
keywords = {Genomics,Humans,Individualized Medicine,Neoplasms,Neoplasms: genetics,Neoplasms: therapy},
language = {en},
month = mar,
number = {3},
pages = {297--303},
pmid = {21383744},
publisher = {Nature Publishing Group},
title = {{Cancer genomics: from discovery science to personalized medicine.}},
url = {http://www.nature.com/nm/journal/v17/n3/full/nm.2323.html?WT.ec\_id=NM-201103},
volume = {17},
year = {2011}
}
@article{Zhao2011,
author = {Zhao, Z and Yin, J and Li, Y},
file = {:home/gus/Documents/Mendeley Desktop/Zhao, Yin, Li\_2011\_An efficient hybrid approach to correcting errors in short reads.pdf:pdf},
journal = {Modeling Decision for Artificial \ldots},
title = {{An efficient hybrid approach to correcting errors in short reads}},
url = {http://www.springerlink.com/index/P53757J416772662.pdf},
year = {2011}
}
@article{Li01082009,
abstract = {Summary: SOAP2 is a significantly improved version of the short oligonucleotide alignment program that both reduces computer memory usage and increases alignment speed at an unprecedented rate. We used a Burrows Wheeler Transformation (BWT) compression index to substitute the seed strategy for indexing the reference sequence in the main memory. We tested it on the whole human genome and found that this new algorithm reduced memory usage from 14.7 to 5.4 GB and improved alignment speed by 20\{\"{A}\}\{\`{\i}\}30 times. SOAP2 is compatible with both single- and paired-end reads. Additionally, this tool now supports multiple text and compressed file formats. A consensus builder has also been developed for consensus assembly and SNP detection from alignment of short reads on a reference genome.Availability: http://soap.genomics.org.cnContact: soap@genomics.org.cn},
author = {Li, Ruiqiang and Yu, Chang and Li, Yingrui and Lam, Tak-Wah and Yiu, Siu-Ming and Kristiansen, Karsten and Wang, Jun},
doi = {10.1093/bioinformatics/btp336},
file = {:home/gus/Documents/Mendeley Desktop/Li et al.\_2009\_SOAP2 an improved ultrafast tool for short read alignment.pdf:pdf},
journal = {Bioinformatics},
number = {15},
pages = {1966--1967},
title = {{SOAP2: an improved ultrafast tool for short read alignment}},
url = {http://bioinformatics.oxfordjournals.org/content/25/15/1966.abstract},
volume = {25},
year = {2009}
}
@inproceedings{ferragina2004alphabet,
author = {Ferragina, P. and Manzini, G. and M\"{a}kinen, V. and Navarro, G.},
booktitle = {String Processing and Information Retrieval},
file = {:home/gus/Documents/Mendeley Desktop/Ferragina et al.\_2004\_An alphabet-friendly FM-index.pdf:pdf},
pages = {228--228},
publisher = {Springer},
title = {{An alphabet-friendly FM-index}},
url = {http://www.springerlink.com/index/P95U67K0XV0X382E.pdf},
year = {2004}
}
@article{Roberts12122004,
abstract = {Motivation: Comparison of nucleic acid and protein sequences is a fundamental tool of modern bioinformatics. A dominant method of such string matching is the seed-and-extend approach, in which occurrences of short subsequences called seeds are used to search for potentially longer matches in a large database of sequences. Each such potential match is then checked to see if it extends beyond the seed. To be effective, the seed-and-extend approach needs to catalogue seeds from virtually every substring in the database of search strings. Projects such as mammalian genome assemblies and large-scale protein matching, however, have such large sequence databases that the resulting list of seeds cannot be stored in RAM on a single computer. This significantly slows the matching process.Results: We present a simple and elegant method in which only a small fraction of seeds, called minimizers, needs to be stored. Using minimizers can speed up string-matching computations by a large factor while missing only a small fraction of the matches found using all seeds.},
author = {Roberts, Michael and Hayes, Wayne and Hunt, Brian R and Mount, Stephen M and Yorke, James A},
doi = {10.1093/bioinformatics/bth408},
journal = {Bioinformatics},
number = {18},
pages = {3363--3369},
title = {{Reducing storage requirements for biological sequence comparison}},
url = {http://bioinformatics.oxfordjournals.org/content/20/18/3363.abstract},
volume = {20},
year = {2004}
}
@article{Richter2008,
abstract = {The new research field of metagenomics is providing exciting insights into various, previously unclassified ecological systems. Next-generation sequencing technologies are producing a rapid increase of environmental data in public databases. There is great need for specialized software solutions and statistical methods for dealing with complex metagenome data sets.},
author = {Richter, Daniel C and Ott, Felix and Auch, Alexander F and Schmid, Ramona and Huson, Daniel H},
doi = {10.1371/journal.pone.0003373},
editor = {Field, Dawn},
file = {:home/gus/Documents/Mendeley Desktop/Richter et al.\_2008\_MetaSim a sequencing simulator for genomics and metagenomics.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Databases, Genetic,Genomics,Models, Theoretical,User-Computer Interface},
month = jan,
number = {10},
pages = {e3373},
pmid = {18841204},
publisher = {Public Library of Science},
title = {{MetaSim: a sequencing simulator for genomics and metagenomics.}},
url = {http://dx.plos.org/10.1371/journal.pone.0003373},
volume = {3},
year = {2008}
}
@article{Brown2012,
abstract = {Deep shotgun sequencing and analysis of genomes, transcriptomes, amplified single-cell genomes, and metagenomes enable the sensitive investigation of a wide range of biological phenomena. However, it is increasingly difficult to deal with the volume of data emerging from deep short-read sequencers, in part because of random and systematic sampling variation as well as a high sequencing error rate. These challenges have led to the development of entire new classes of short-read mapping tools, as well as new \{$\backslash$em de novo\} assemblers. Even newer assembly strategies for dealing with transcriptomes, single-cell genomes, and metagenomes have also emerged. Despite these advances, algorithms and compute capacity continue to be challenged by the continued improvements in sequencing technology throughput. We here describe an approach we term digital normalization, a single-pass computational algorithm that discards redundant data and both sampling variation and the number of errors present in deep sequencing data sets. Digital normalization substantially reduces the size of data sets and accordingly decreases the memory and time requirements for \{$\backslash$em de novo\} sequence assembly, all without significantly impacting content of the generated contigs. In doing so, it converts high random coverage to low systematic coverage. Digital normalization is an effective and efficient approach to normalizing coverage, removing errors, and reducing data set size for shotgun sequencing data sets. It is particularly useful for reducing the compute requirements for \{$\backslash$em de novo\} sequence assembly. We demonstrate this for the assembly of microbial genomes, amplified single-cell genomic data, and transcriptomic data. The software is freely available for use and modification.},
annote = {I am skeptical about this paper:

        
- As they mention at the end of the paper, they only tested on bacterial genome with low amount of repeats. This is conceptually a big drawback for a paper whose purpose is to reduce the amount of data to tackle large projects. Assembling E. coli is not a challenge anymore.
- Given that the coverage of most part of the genome is now made uniform, we lost a big indicator to detect repeat regions. The Celera assembler and others will likely collapse many copies of repeat regions and have many more mis-assemblies as a result. Which does not show up in the E. coli dataset.

        
- I disagree with the claim that "Digital normalization retains information while discarding both data and errors" is beneficial. When looking at their own data (Table 1.) I see the following trend. For a mer length of k, one base error creates usually around k erroneous k-mers. Hence the error rate can be estimated as the number of k-mers in the reads minus the number of true k-mers, divided by the number of reads. For E. coli in Table 1 they kept a tenth of the reads while reducing the number of erroneous k-mers by only half. Hence the error rate in the reads that are kept is approximately 5 times HIGHER.  This trend of higher error rate in the kept reads is even worse in the three-pass procedure (Table 2.).

        
They reduced the number of erroneous k-mers in absolute value but increased it in proportion. Is it really a good operation?

        
Actually, based on this, I would go the other way around. I would count k-mers in the remaining 90\% of the reads (the discarded ones), which have a  lower error rate (approximately half the original error rate). I would then used these k-mers to do error correction on ALL of the reads.},
archivePrefix = {arXiv},
arxivId = {1203.4802},
author = {Brown, C. Titus and Howe, Adina and Zhang, Qingpeng and Pyrkosz, Alexis B. and Brom, Timothy H.},
eprint = {1203.4802},
file = {:home/gus/Documents/Mendeley Desktop/Brown et al.\_2012\_A single pass approach to reducing sampling variation, removing errors, and scaling \{em de novo\} assembly of shotgun sequences.pdf:pdf},
month = mar,
title = {{A single pass approach to reducing sampling variation, removing errors, and scaling \{$\backslash$em de novo\} assembly of shotgun sequences}},
url = {http://arxiv.org/abs/1203.4802},
year = {2012}
}
@article{Pugh1990,
author = {Pugh, William},
doi = {10.1145/78973.78977},
file = {:home/gus/Documents/Mendeley Desktop//Pugh\_1990\_Skip lists a probabilistic alternative to balanced trees.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {data structures,searching,trees},
month = jun,
number = {6},
pages = {668--676},
publisher = {ACM},
title = {{Skip lists: a probabilistic alternative to balanced trees}},
url = {http://dl.acm.org/citation.cfm?id=78973.78977 http://dl.acm.org/citation.cfm?id=78977,},
volume = {33},
year = {1990}
}
@misc{TheMendeleySupportTeam2011,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:home/gus/Documents/Mendeley Desktop/The Mendeley Support Team\_2011\_Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@article{Li2009,
abstract = {The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.},
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
file = {:home/gus/Documents/Mendeley Desktop/Li, Durbin\_2009\_Fast and accurate short read alignment with Burrows-Wheeler transform.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Genomics,Genomics: methods,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
month = jul,
number = {14},
pages = {1754--60},
pmid = {19451168},
title = {{Fast and accurate short read alignment with Burrows-Wheeler transform.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/25/14/1754},
volume = {25},
year = {2009}
}
@article{Maccallum2009,
abstract = {We demonstrate that genome sequences approaching finished quality can be generated from short paired reads. Using 36 base (fragment) and 26 base (jumping) reads from five microbial genomes of varied GC composition and sizes up to 40 Mb, ALLPATHS2 generated assemblies with long, accurate contigs and scaffolds. Velvet and EULER-SR were less accurate. For example, for Escherichia coli, the fraction of 10-kb stretches that were perfect was 99.8\% (ALLPATHS2), 68.7\% (Velvet), and 42.1\% (EULER-SR).},
author = {Maccallum, Iain and Przybylski, Dariusz and Gnerre, Sante and Burton, Joshua and Shlyakhter, Ilya and Gnirke, Andreas and Malek, Joel and McKernan, Kevin and Ranade, Swati and Shea, Terrance P and Williams, Louise and Young, Sarah and Nusbaum, Chad and Jaffe, David B},
doi = {10.1186/gb-2009-10-10-r103},
file = {:home/gus/Documents/Mendeley Desktop//Maccallum et al.\_2009\_ALLPATHS 2 small genomes assembled accurately and with high continuity from short paired reads.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Bacteria,Bacteria: genetics,Base Pairing,Base Pairing: genetics,Fungi,Fungi: genetics,Genome,Genome: genetics,Genomics,Genomics: methods,Reproducibility of Results,Software},
month = jan,
number = {10},
pages = {R103},
pmid = {19796385},
title = {{ALLPATHS 2: small genomes assembled accurately and with high continuity from short paired reads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2784318\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Heyn2012,
author = {Heyn, Holger and Li, Ning and Ferreira, Humberto J and Moran, Sebastian and Pisano, David G and Gomez, Antonio and Diez, Javier},
doi = {10.1073/pnas.1120658109},
file = {:home/gus/Documents/Mendeley Desktop/Heyn et al.\_2012\_Distinct DNA methylomes of newborns and centenarians.pdf:pdf},
journal = {PNAS},
title = {{Distinct DNA methylomes of newborns and centenarians}},
year = {2012}
}
@article{PETER1991,
abstract = {We extend the notion of randomness (in the version introduced by Schnorr) to computable Probability Spaces and compare it to a dynamical notion of randomness: typicality. Roughly, a point is typical for some dynamic, if it follows the statistical behavior of the system (Birkhos pointwise ergodic theorem). We prove that a point is Schnorr random if and only if it is typical for every mixing computable dynamics. To prove the result we develop some tools for the theory of computable probability spaces (for example, morphisms) that are expected to have other applications.},
author = {PETER, GACS and HOYRUP, MATHIEU and CRISTOBAL, ROJAS},
file = {:home/gus/Documents/Mendeley Desktop/PETER, HOYRUP, CRISTOBAL\_1991\_Randomness on computable probability spacesa dynamical point of view.pdf:pdf},
journal = {Theory of Computing Systems},
keywords = {Birkhos ergodic theorem,Computability,Schnorr Randomness,computable measures},
mendeley-tags = {Computability},
pages = {1--18},
title = {{Randomness on computable probability spacesa dynamical point of view}},
url = {http://www.springerlink.com/index/3q774874623m0nm6.pdf},
year = {1991}
}
@article{Ding2012,
author = {Ding, Li and Ley, Timothy J. and Larson, David E. and Miller, Christopher A. and Koboldt, Daniel C. and Welch, John S. and Ritchey, Julie K. and Young, Margaret A. and Lamprecht, Tamara and McLellan, Michael D. and McMichael, Joshua F. and Wallis, John W. and Lu, Charles and Shen, Dong and Harris, Christopher C. and Dooling, David J. and Fulton, Robert S. and Fulton, Lucinda L. and Chen, Ken and Schmidt, Heather and Kalicki-Veizer, Joelle and Magrini, Vincent J. and Cook, Lisa and McGrath, Sean D. and Vickery, Tammi L. and Wendl, Michael C. and Heath, Sharon and Watson, Mark A. and Link, Daniel C. and Tomasson, Michael H. and Shannon, William D. and Payton, Jacqueline E. and Kulkarni, Shashikant and Westervelt, Peter and Walter, Matthew J. and Graubert, Timothy A. and Mardis, Elaine R. and Wilson, Richard K. and DiPersio, John F.},
doi = {10.1038/nature10738},
file = {:home/gus/Documents/Mendeley Desktop/Ding et al.\_2012\_Clonal evolution in relapsed acute myeloid leukaemia revealed by whole-genome sequencing(2).pdf:pdf;:home/gus/Documents/Mendeley Desktop/Ding et al.\_2012\_Clonal evolution in relapsed acute myeloid leukaemia revealed by whole-genome sequencing.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
month = jan,
number = {7382},
pages = {506--510},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nature},
title = {{Clonal evolution in relapsed acute myeloid leukaemia revealed by whole-genome sequencing}},
url = {http://dx.doi.org/10.1038/nature10738},
volume = {481},
year = {2012}
}
@article{Jaffe2003,
abstract = {We previously described the whole-genome assembly program Arachne, presenting assemblies of simulated data for small to mid-sized genomes. Here we describe algorithmic adaptations to the program, allowing for assembly of mammalian-size genomes, and also improving the assembly of smaller genomes. Three principal changes were simultaneously made and applied to the assembly of the mouse genome, during a six-month period of development: (1) Supercontigs (scaffolds) were iteratively broken and rejoined using several criteria, yielding a 64-fold increase in length (N50), and apparent elimination of all global misjoins; (2) gaps between contigs in supercontigs were filled (partially or completely) by insertion of reads, as suggested by pairing within the supercontig, increasing the N50 contig length by 50\%; (3) memory usage was reduced fourfold. The outcome of this mouse assembly and its analysis are described in (Mouse Genome Sequencing Consortium 2002).},
author = {Jaffe, David B and Butler, Jonathan and Gnerre, Sante and Mauceli, Evan and Lindblad-Toh, Kerstin and Mesirov, Jill P and Zody, Michael C and Lander, Eric S},
doi = {10.1101/gr.828403},
file = {:home/gus/Documents/Mendeley Desktop/Jaffe et al.\_2003\_Whole-genome sequence assembly for mammalian genomes Arachne 2.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Animals,Computational Biology,Computational Biology: methods,Contig Mapping,Contig Mapping: methods,Genome,Humans,Mice,Software},
month = jan,
number = {1},
pages = {91--6},
pmid = {12529310},
title = {{Whole-genome sequence assembly for mammalian genomes: Arachne 2.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=430950\&tool=pmcentrez\&rendertype=abstract},
volume = {13},
year = {2003}
}
@article{Koren2007,
abstract = {Protein-protein interaction (PPI) networks have become prevalent recently as high-throughput experiment data has become available. Many network growth models have been proposed to reproduce the properties of the networks. We present a stochastic sampling method for capturing network topology properties and use it to infer the model that best matches the observed topology. We apply the method to three real networks, Caenorhabditis elegans, Drosophila melanogaster, and Saccharomyces cerevisiae. We show that using the current protein-interaction networks, the models are not consistent across organisms.},
author = {Koren, Sergey and Mar\c{c}ais, Guillaume},
file = {:home/gus/Documents/Mendeley Desktop/Koren, Mar\c{c}ais\_2007\_Network Comparison Using Stochastic Sampling and its Applications.pdf:pdf},
keywords = {PPI,Random walks},
pages = {1--8},
title = {{Network Comparison Using Stochastic Sampling and its Applications}},
year = {2007}
}
@article{Peterlongo2011,
abstract = {Background: The analysis of next-generation sequencing data from large genomes is a timely research topic. Sequencers are producing billions of short sequence fragments from newly sequenced organisms. Computational methods for reconstructing sequences (whole-genome assemblers) are typically employed to process such data. However, one of the main drawback of these methods is the high memory requirement. Results: We present Mapsembler, an iterative targeted assembler which processes large datasets of reads on commodity hardware. Mapsembler checks for the presence of given regions of interest in the reads and reconstructs their neighborhood, either as a plain sequence (consensus) or as a graph (full sequence structure). We introduce new algorithms to retrieve homologues of a sequence from reads and construct an extension graph. Conclusions: Mapsembler is the rst software that enables de novo discovery around a region of interest of gene homologues, SNPs, exon skipping as well as other structural events, directly from raw sequencing reads. Compared to traditional assembly software, memory requirement and execution time of Mapsembler are considerably lower, as data indexing is localized. Mapsembler can be used at http://mapsembler.genouest.org},
author = {Peterlongo, Pierre and Chikhi, Rayan},
file = {:home/gus/Documents/Mendeley Desktop/Peterlongo, Chikhi\_2011\_Mapsembler, targeted assembly of larges genomes on a desktop computer.pdf:pdf},
keywords = {NGS,algorithms,bioinformatics,genome,light memory usage,targeted assembly},
month = mar,
title = {{Mapsembler, targeted assembly of larges genomes on a desktop computer}},
url = {http://hal.inria.fr/inria-00577218/ http://hal.inria.fr/docs/00/57/72/18/PDF/RR-7565.pdf},
year = {2011}
}
@article{Nagaraj2007,
abstract = {Expressed sequence tag (EST) sequencing projects are underway for numerous organisms, generating millions of short, single-pass nucleotide sequence reads, accumulating in EST databases. Extensive computational strategies have been developed to organize and analyse both small- and large-scale EST data for gene discovery, transcript and single nucleotide polymorphism analysis as well as functional annotation of putative gene products. We provide an overview of the significance of ESTs in the genomic era, their properties and the applications of ESTs. Methods adopted for each step of EST analysis by various research groups have been compared. Challenges that lie ahead in organizing and analysing the ever increasing EST data have also been identified. The most appropriate software tools for EST pre-processing, clustering and assembly, database matching and functional annotation have been compiled (available online from http://biolinfo.org/EST). We propose a road map for EST analysis to accelerate the effective analyses of EST data sets. An investigation of EST analysis platforms reveals that they all terminate prior to downstream functional annotation including gene ontologies, motif/pattern analysis and pathway mapping.},
author = {Nagaraj, Shivashankar H and Gasser, Robin B and Ranganathan, Shoba},
doi = {10.1093/bib/bbl015},
file = {:home/gus/Documents/Mendeley Desktop/Nagaraj, Gasser, Ranganathan\_2007\_A hitchhiker's guide to expressed sequence tag (EST) analysis.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Nagaraj, Gasser, Ranganathan\_2007\_A hitchhiker's guide to expressed sequence tag (EST) analysis(2).pdf:pdf},
issn = {1467-5463},
journal = {Briefings in bioinformatics},
keywords = {Animals,Complementary,Complementary: chemistry,Consensus Sequence,Consensus Sequence: genetics,DNA,Databases,Expressed Sequence Tags,Genetic,Genetic: standards,Genomics,Genomics: methods,Humans,Nucleic Acid,Nucleic Acid: standards,RNA,Software,Untranslated Regions,Untranslated Regions: chemistry},
mendeley-tags = {RNA},
month = jan,
number = {1},
pages = {6--21},
pmid = {16772268},
title = {{A hitchhiker's guide to expressed sequence tag (EST) analysis.}},
url = {http://bib.oxfordjournals.org/cgi/content/abstract/8/1/6},
volume = {8},
year = {2007}
}
@article{Maccallum:2009vn,
abstract = {We demonstrate that genome sequences approaching finished quality can be generated from short paired reads. Using 36 base (fragment) and 26 base (jumping) reads from five microbial genomes of varied GC composition and sizes up to 40 Mb, ALLPATHS2 generated assemblies with long, accurate contigs and scaffolds. Velvet and EULER-SR were less accurate. For example, for Escherichia coli, the fraction of 10-kb stretches that were perfect was 99.8\% (ALLPATHS2), 68.7\% (Velvet), and 42.1\% (EULER-SR).},
author = {Maccallum, Iain and Przybylski, Dariusz and Gnerre, Sante and Burton, Joshua and Shlyakhter, Ilya and Gnirke, Andreas and Malek, Joel and McKernan, Kevin and Ranade, Swati and Shea, Terrance P and Williams, Louise and Young, Sarah and Nusbaum, Chad and Jaffe, David B},
doi = {10.1186/gb-2009-10-10-r103},
file = {:home/gus/Documents/Mendeley Desktop//Maccallum et al.\_2009\_ALLPATHS 2 small genomes assembled accurately and with high continuity from short paired reads.pdf:pdf},
journal = {Genome Biol},
number = {10},
pages = {R103},
pmid = {19796385},
title = {{ALLPATHS 2: small genomes assembled accurately and with high continuity from short paired reads}},
volume = {10},
year = {2009}
}
@article{Yang2011,
abstract = {High-throughput short read sequencing is revolutionizing genomics and systems biology research by enabling cost-effective deep coverage sequencing of genomes and transcriptomes. Error detection and correction are crucial to many short read sequencing applications including de novo genome sequencing, genome resequencing, and digital gene expression analysis. Short read error detection is typically carried out by counting the observed frequencies of kmers in reads and validating those with frequencies exceeding a threshold. In case of genomes with high repeat content, an erroneous kmer may be frequently observed if it has few nucleotide differences with valid kmers with multiple occurrences in the genome. Error detection and correction were mostly applied to genomes with low repeat content and this remains a challenging problem for genomes with high repeat content.},
author = {Yang, Xiao and Aluru, Srinivas and Dorman, Karin S},
doi = {10.1186/1471-2105-12-S1-S52},
file = {:home/gus/Documents/Mendeley Desktop//Yang, Aluru, Dorman\_2011\_Repeat-aware modeling and correction of short read errors.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,Genomics,Genomics: methods,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Likelihood Functions,Models,Software,Statistical},
month = jan,
number = {Suppl 1},
pages = {S52},
pmid = {21342585},
title = {{Repeat-aware modeling and correction of short read errors.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3044310\&tool=pmcentrez\&rendertype=abstract http://www.biomedcentral.com/1471-2105/12/S1/S52},
volume = {12 Suppl 1},
year = {2011}
}
@incollection{springerlink:10.1007/978-3-642-23038-7_21,
annote = {10.1007/978-3-642-23038-7\_21},
author = {Patro, Rob and Sefer, Emre and Malin, Justin and Mar\c{c}ais, Guillaume and Navlakha, Saket and Kingsford, Carl},
booktitle = {Algorithms in Bioinformatics},
editor = {Przytycka, Teresa and Sagot, Marie-France},
file = {:home/gus/Documents/Mendeley Desktop/Patro et al.\_2011\_Parsimonious Reconstruction of Network Evolution.pdf:pdf},
isbn = {978-3-642-23037-0},
pages = {237--249},
publisher = {Springer Berlin / Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Parsimonious Reconstruction of Network Evolution}},
url = {http://dx.doi.org/10.1007/978-3-642-23038-7\_21},
volume = {6833},
year = {2011}
}
@article{Rumble2009,
abstract = {The development of Next Generation Sequencing technologies, capable of sequencing hundreds of millions of short reads (25-70 bp each) in a single run, is opening the door to population genomic studies of non-model species. In this paper we present SHRiMP - the SHort Read Mapping Package: a set of algorithms and methods to map short reads to a genome, even in the presence of a large amount of polymorphism. Our method is based upon a fast read mapping technique, separate thorough alignment methods for regular letter-space as well as AB SOLiD (color-space) reads, and a statistical model for false positive hits. We use SHRiMP to map reads from a newly sequenced Ciona savignyi individual to the reference genome. We demonstrate that SHRiMP can accurately map reads to this highly polymorphic genome, while confirming high heterozygosity of C. savignyi in this second individual. SHRiMP is freely available at http://compbio.cs.toronto.edu/shrimp.},
author = {Rumble, Stephen M and Lacroute, Phil and Dalca, Adrian V and Fiume, Marc and Sidow, Arend and Brudno, Michael},
doi = {10.1371/journal.pcbi.1000386},
file = {:home/gus/Documents/Mendeley Desktop/Rumble et al.\_2009\_SHRiMP accurate mapping of short color-space reads.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Algorithms,Animals,Chromosome Mapping,Chromosome Mapping: methods,Computer Simulation,Models, Statistical,Reproducibility of Results,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software,Urochordata,Urochordata: genetics},
month = may,
number = {5},
pages = {e1000386},
pmid = {19461883},
title = {{SHRiMP: accurate mapping of short color-space reads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2678294\&tool=pmcentrez\&rendertype=abstract},
volume = {5},
year = {2009}
}
@article{Simpson2010,
abstract = {MOTIVATION: Sequence assembly is a difficult problem whose importance has grown again recently as the cost of sequencing has dramatically dropped. Most new sequence assembly software has started by building a de Bruijn graph, avoiding the overlap-based methods used previously because of the computational cost and complexity of these with very large numbers of short reads. Here, we show how to use suffix array-based methods that have formed the basis of recent very fast sequence mapping algorithms to find overlaps and generate assembly string graphs asymptotically faster than previously described algorithms. RESULTS: Standard overlap assembly methods have time complexity O(N(2)), where N is the sum of the lengths of the reads. We use the Ferragina-Manzini index (FM-index) derived from the Burrows-Wheeler transform to find overlaps of length at least tau among a set of reads. As well as an approach that finds all overlaps then implements transitive reduction to produce a string graph, we show how to output directly only the irreducible overlaps, significantly shrinking memory requirements and reducing compute time to O(N), independent of depth. Overlap-based assembly methods naturally handle mixed length read sets, including capillary reads or long reads promised by the third generation sequencing technologies. The algorithms we present here pave the way for overlap-based assembly approaches to be developed that scale to whole vertebrate genome de novo assembly.},
author = {Simpson, Jared T and Durbin, Richard},
doi = {10.1093/bioinformatics/btq217},
file = {:home/gus/Documents/Mendeley Desktop/Simpson, Durbin\_2010\_Efficient construction of an assembly string graph using the FM-index.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Genomics,Genomics: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
month = jun,
number = {12},
pages = {i367--73},
pmid = {20529929},
title = {{Efficient construction of an assembly string graph using the FM-index.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/26/12/i367},
volume = {26},
year = {2010}
}
@article{Kashtan2005,
abstract = {Biological networks have an inherent simplicity: they are modular with a design that can be separated into units that perform almost independently. Furthermore, they show reuse of recurring patterns termed network motifs. Little is known about the evolutionary origin of these properties. Current models of biological evolution typically produce networks that are highly nonmodular and lack understandable motifs. Here, we suggest a possible explanation for the origin of modularity and network motifs in biology. We use standard evolutionary algorithms to evolve networks. A key feature in this study is evolution under an environment (evolutionary goal) that changes in a modular fashion. That is, we repeatedly switch between several goals, each made of a different combination of subgoals. We find that such "modularly varying goals" lead to the spontaneous evolution of modular network structure and network motifs. The resulting networks rapidly evolve to satisfy each of the different goals. Such switching between related goals may represent biological evolution in a changing environment that requires different combinations of a set of basic biological functions. The present study may shed light on the evolutionary forces that promote structural simplicity in biological networks and offers ways to improve the evolutionary design of engineered systems.},
author = {Kashtan, Nadav and Alon, Uri},
doi = {10.1073/pnas.0503610102},
file = {:home/gus/Documents/Mendeley Desktop/Kashtan, Alon\_2005\_Spontaneous evolution of modularity and network motifs.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Algorithms,Biological Evolution,Computer Simulation,Neural Networks (Computer)},
month = sep,
number = {39},
pages = {13773--8},
pmid = {16174729},
title = {{Spontaneous evolution of modularity and network motifs.}},
url = {http://www.pnas.org/cgi/content/abstract/102/39/13773},
volume = {102},
year = {2005}
}
@article{Li2011,
author = {Li, Yingrui and Zheng, Hancheng and Luo, Ruibang and Wu, Honglong and Zhu, Hongmei and Li, Ruiqiang and Cao, Hongzhi and Wu, Boxin and Huang, Shujia and Shao, Haojing and Ma, Hanzhou and Zhang, Fan and Feng, Shuijian and Zhang, Wei and Du, Hongli and Tian, Geng and Li, Jingxiang and Zhang, Xiuqing and Li, Songgang and Bolund, Lars and Kristiansen, Karsten and de Smith, Adam J and Blakemore, Alexandra I F and Coin, Lachlan J M and Yang, Huanming and Wang, Jian and Wang, Jun},
doi = {10.1038/nbt.1904},
file = {:home/gus/Documents/Mendeley Desktop/Li et al.\_2011\_Structural variation in two human genomes mapped at single-nucleotide resolution by whole genome de novo assembly.pdf:pdf},
issn = {1087-0156},
journal = {Nature Biotechnology},
month = jul,
number = {8},
pages = {723--730},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nat Biotech},
title = {{Structural variation in two human genomes mapped at single-nucleotide resolution by whole genome de novo assembly}},
url = {http://dx.doi.org/10.1038/nbt.1904},
volume = {29},
year = {2011}
}
@article{Gnerre:2011zr,
abstract = {Massively parallel DNA sequencing technologies are revolutionizing genomics by making it possible to generate billions of relatively short (\~{}100-base) sequence reads at very low cost. Whereas such data can be readily used for a wide range of biomedical applications, it has proven difficult to use them to generate high-quality de novo genome assemblies of large, repeat-rich vertebrate genomes. To date, the genome assemblies generated from such data have fallen far short of those obtained with the older (but much more expensive) capillary-based sequencing approach. Here, we report the development of an algorithm for genome assembly, ALLPATHS-LG, and its application to massively parallel DNA sequence data from the human and mouse genomes, generated on the Illumina platform. The resulting draft genome assemblies have good accuracy, short-range contiguity, long-range connectivity, and coverage of the genome. In particular, the base accuracy is high (99.95\%) and the scaffold sizes (N50 size = 11.5 Mb for human and 7.2 Mb for mouse) approach those obtained with capillary-based sequencing. The combination of improved sequencing technology and improved computational methods should now make it possible to increase dramatically the de novo sequencing of large genomes. The ALLPATHS-LG program is available at http://www.broadinstitute.org/science/programs/genome-biology/crd.},
annote = {        From Duplicate 2 (                           High-quality draft assemblies of mammalian genomes from massively parallel sequence data                         - Gnerre, Sante; Maccallum, Iain; Przybylski, Dariusz; Ribeiro, Filipe J; Burton, Joshua N; Walker, Bruce J; Sharpe, Ted; Hall, Giles; Shea, Terrance P; Sykes, Sean; Berlin, Aaron M; Aird, Daniel; Costello, Maura; Daza, Riza; Williams, Louise; Nicol, Robert; Gnirke, Andreas; Nusbaum, Chad; Lander, Eric S; Jaffe, David B )
                
        
        
      },
author = {Gnerre, Sante and Maccallum, Iain and Przybylski, Dariusz and Ribeiro, Filipe J and Burton, Joshua N and Walker, Bruce J and Sharpe, Ted and Hall, Giles and Shea, Terrance P and Sykes, Sean and Berlin, Aaron M and Aird, Daniel and Costello, Maura and Daza, Riza and Williams, Louise and Nicol, Robert and Gnirke, Andreas and Nusbaum, Chad and Lander, Eric S and Jaffe, David B},
doi = {10.1073/pnas.1017351108},
file = {:home/gus/Documents/Mendeley Desktop/Gnerre et al.\_2011\_High-quality draft assemblies of mammalian genomes from massively parallel sequence data.pdf:pdf},
issn = {1091-6490},
journal = {Proc Natl Acad Sci U S A},
keywords = {Algorithms,Animals,DNA,DNA: methods,Genome,Genome: genetics,Genomics,Genomics: methods,Humans,Internet,Mice,Reproducibility of Results,Sequence Analysis,Software},
month = jan,
number = {4},
pages = {1513--1518},
pmid = {21187386},
title = {{High-quality draft assemblies of mammalian genomes from massively parallel sequence data}},
url = {http://www.pnas.org/cgi/content/abstract/108/4/1513},
volume = {108},
year = {2011}
}
@article{Chaisson2009,
abstract = {Increasing read length is currently viewed as the crucial condition for fragment assembly with next-generation sequencing technologies. However, introducing mate-paired reads (separated by a gap of length, GapLength) opens a possibility to transform short mate-pairs into long mate-reads of length approximately GapLength, and thus raises the question as to whether the read length (as opposed to GapLength) even matters. We describe a new tool, EULER-USR, for assembling mate-paired short reads and use it to analyze the question of whether the read length matters. We further complement the ongoing experimental efforts to maximize read length by a new computational approach for increasing the effective read length. While the common practice is to trim the error-prone tails of the reads, we present an approach that substitutes trimming with error correction using repeat graphs. An important and counterintuitive implication of this result is that one may extend sequencing reactions that degrade with length "past their prime" to where the error rate grows above what is normally acceptable for fragment assembly.},
author = {Chaisson, Mark J and Brinza, Dumitru and Pevzner, Pavel A},
doi = {10.1101/gr.079053.108},
file = {:home/gus/Documents/Mendeley Desktop/Chaisson, Brinza, Pevzner\_2009\_De novo fragment assembly with short mate-paired reads Does the read length matter.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Bacterial,Base Pairing,Base Pairing: physiology,Base Sequence,Base Sequence: physiology,Bias (Epidemiology),Biological,Chromosome Mapping,Chromosome Mapping: methods,Computational Biology,Computational Biology: methods,DNA,DNA: methods,Escherichia coli,Escherichia coli: genetics,Genetic Markers,Genome,Models,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis},
month = feb,
number = {2},
pages = {336--46},
pmid = {19056694},
title = {{De novo fragment assembly with short mate-paired reads: Does the read length matter?}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2652199\&tool=pmcentrez\&rendertype=abstract},
volume = {19},
year = {2009}
}
@article{Maccallum:2009vn,
abstract = {We demonstrate that genome sequences approaching finished quality can be generated from short paired reads. Using 36 base (fragment) and 26 base (jumping) reads from five microbial genomes of varied GC composition and sizes up to 40 Mb, ALLPATHS2 generated assemblies with long, accurate contigs and scaffolds. Velvet and EULER-SR were less accurate. For example, for Escherichia coli, the fraction of 10-kb stretches that were perfect was 99.8\% (ALLPATHS2), 68.7\% (Velvet), and 42.1\% (EULER-SR).},
author = {Maccallum, Iain and Przybylski, Dariusz and Gnerre, Sante and Burton, Joshua and Shlyakhter, Ilya and Gnirke, Andreas and Malek, Joel and McKernan, Kevin and Ranade, Swati and Shea, Terrance P and Williams, Louise and Young, Sarah and Nusbaum, Chad and Jaffe, David B},
doi = {10.1186/gb-2009-10-10-r103},
file = {:home/gus/Documents/Mendeley Desktop//Maccallum et al.\_2009\_ALLPATHS 2 small genomes assembled accurately and with high continuity from short paired reads.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Bacteria,Bacteria: genetics,Base Pairing,Base Pairing: genetics,Fungi,Fungi: genetics,Genome,Genome: genetics,Genomics,Genomics: methods,Reproducibility of Results,Software},
month = jan,
number = {10},
pages = {R103},
pmid = {19796385},
title = {{ALLPATHS 2: small genomes assembled accurately and with high continuity from short paired reads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2784318\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Ilie2011,
abstract = {High-throughput sequencing technologies produce very large amounts of data and sequencing errors constitute one of the major problems in analyzing such data. Current algorithms for correcting these errors are not very accurate and do not automatically adapt to the given data.},
author = {Ilie, Lucian and Fazayeli, Farideh and Ilie, Silvana},
institution = {Department of Computer Science, University of Western Ontario, London, ON N6A 5B7, Canada. ilie@csd.uwo.ca},
journal = {Bioinformatics},
number = {3},
pages = {295--302},
pmid = {21115437},
title = {{HiTEC: accurate error correction in high-throughput sequencing data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21115437},
volume = {27},
year = {2011}
}
@article{Butler:2008kx,
abstract = {New DNA sequencing technologies deliver data at dramatically lower costs but demand new analytical methods to take full advantage of the very short reads that they produce. We provide an initial, theoretical solution to the challenge of de novo assembly from whole-genome shotgun "microreads." For 11 genomes of sizes up to 39 Mb, we generated high-quality assemblies from 80x coverage by paired 30-base simulated reads modeled after real Illumina-Solexa reads. The bacterial genomes of Campylobacter jejuni and Escherichia coli assemble optimally, yielding single perfect contigs, and larger genomes yield assemblies that are highly connected and accurate. Assemblies are presented in a graph form that retains intrinsic ambiguities such as those arising from polymorphism, thereby providing information that has been absent from previous genome assemblies. For both C. jejuni and E. coli, this assembly graph is a single edge encompassing the entire genome. Larger genomes produce more complicated graphs, but the vast majority of the bases in their assemblies are present in long edges that are nearly always perfect. We describe a general method for genome assembly that can be applied to all types of DNA sequence data, not only short read data, but also conventional sequence reads.},
author = {Butler, Jonathan and MacCallum, Iain and Kleber, Michael and Shlyakhter, Ilya a and Belmonte, Matthew K and Lander, Eric S and Nusbaum, Chad and Jaffe, David B},
doi = {10.1101/gr.7337908},
file = {:home/gus/Documents/Mendeley Desktop/Butler et al.\_2008\_ALLPATHS de novo assembly of whole-genome shotgun microreads.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Bacterial,Bacterial: genetics,Campylobacter jejuni,Campylobacter jejuni: genetics,Computational Biology,Computational Biology: methods,Computer Simulation,DNA,DNA: methods,DNA: standards,Escherichia coli,Escherichia coli: genetics,Genome,Reproducibility of Results,Sequence Analysis},
month = may,
number = {5},
pages = {810--20},
pmid = {18340039},
title = {{ALLPATHS: de novo assembly of whole-genome shotgun microreads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2336810\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@incollection{CLR,
author = {Cormen, T. and Leiserson, C. and Rivest, R.},
chapter = {12},
publisher = {MIT Press},
title = {{Introduction to Algorithms}},
year = {1990}
}
@article{Gilchrist2007,
author = {Gilchrist, Jeff},
doi = {10.1109/AINA.2007.109},
file = {:home/gus/Documents/Mendeley Desktop/Gilchrist\_2007\_Parallel lossless data compression based on the burrows-wheeler transform.pdf:pdf},
isbn = {0-7695-2846-5},
issn = {1550-445X},
journal = {21st International Conference on Advanced Networking and Applications (AINA '07)},
month = may,
pages = {877--884},
publisher = {Ieee},
title = {{Parallel lossless data compression based on the burrows-wheeler transform}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4220984 http://www.computer.org/portal/web/csdl/doi/10.1109/AINA.2007.109},
year = {2007}
}
@article{Baker2012,
author = {Baker, Monya},
doi = {10.1038/nmeth.1858},
file = {:home/gus/Documents/Mendeley Desktop/Baker\_2012\_Structural variation the genome's hidden architecture.pdf:pdf},
issn = {1548-7091},
journal = {Nature Methods},
month = jan,
number = {2},
pages = {133--137},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nat Meth},
title = {{Structural variation: the genome's hidden architecture}},
url = {http://dx.doi.org/10.1038/nmeth.1858},
volume = {9},
year = {2012}
}
@article{Huttenhower2006,
abstract = {MOTIVATION: The diverse microarray datasets that have become available over the past several years represent a rich opportunity and challenge for biological data mining. Many supervised and unsupervised methods have been developed for the analysis of individual microarray datasets. However, integrated analysis of multiple datasets can provide a broader insight into genetic regulation of specific biological pathways under a variety of conditions. RESULTS: To aid in the analysis of such large compendia of microarray experiments, we present Microarray Experiment Functional Integration Technology (MEFIT), a scalable Bayesian framework for predicting functional relationships from integrated microarray datasets. Furthermore, MEFIT predicts these functional relationships within the context of specific biological processes. All results are provided in the context of one or more specific biological functions, which can be provided by a biologist or drawn automatically from catalogs such as the Gene Ontology (GO). Using MEFIT, we integrated 40 Saccharomyces cerevisiae microarray datasets spanning 712 unique conditions. In tests based on 110 biological functions drawn from the GO biological process ontology, MEFIT provided a 5\% or greater performance increase for 54 functions, with a 5\% or more decrease in performance in only two functions.},
annote = {MEFIT. Better correlation / aggregation for micro-array data.},
author = {Huttenhower, Curtis and Hibbs, Matt and Myers, Chad and Troyanskaya, Olga G},
doi = {10.1093/bioinformatics/btl492},
file = {:home/gus/Documents/Mendeley Desktop/Huttenhower et al.\_2006\_A scalable method for integration and functional analysis of multiple microarray datasets.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Bayes Theorem,Databases, Genetic,Gene Expression Profiling,Gene Expression Profiling: methods,Information Storage and Retrieval,Information Storage and Retrieval: methods,Saccharomyces cerevisiae,Saccharomyces cerevisiae Proteins,Saccharomyces cerevisiae Proteins: metabolism,Saccharomyces cerevisiae: metabolism,Systems Integration},
month = dec,
number = {23},
pages = {2890--7},
pmid = {17005538},
title = {{A scalable method for integration and functional analysis of multiple microarray datasets.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/22/23/2890},
volume = {22},
year = {2006}
}
@article{Melsted2011,
abstract = {BACKGROUND:Counting k-mers (substrings of length k in DNA sequence data) is an essential component of many methods in bioinformatics, including for genome and transcriptome assembly, for metagenomic sequencing, and for error correction of sequence reads. Although simple in principle, counting k-mers in large modern sequence data sets can easily overwhelm the memory capacity of standard computers. In current data sets, a large fraction - often more than 50\% - of the storage capacity may be spent on storing k-mers that contain sequencing errors and which are typically observed only a single time in the data. These singleton k-mers are uninformative for many algorithms without some kind of error correction.RESULTS:We present a new method that identifies all the k-mers that occur more than once in a DNA sequence data set. Our method does this using a Bloom filter, a probabilistic data structure that stores all the observed k-mers implicitly in memory with greatly reduced memory requirements. We then make a second sweep through the data to provide exact counts of all nonunique k-mers. For example data sets, we report up to 50\% savings in memory usage compared to current software, with modest costs in computational speed. This approach may reduce memory requirements for any algorithm that starts by counting k-mers in sequence data with errors.CONCLUSIONS:A reference implementation for this methodology, BFCounter, is written in C++ and is GPL licensed. It is available for free download at http://pritch.bsd.uchicago.edu/software.html},
author = {Melsted, Pall and Pritchard, Jonathan K},
doi = {10.1186/1471-2105-12-333},
file = {:home/gus/Documents/Mendeley Desktop/Melsted, Pritchard\_2011\_Efficient counting of k-mers in DNA sequences using a Bloom Filter.pdf:pdf},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {333},
title = {{Efficient counting of k-mers in DNA sequences using a Bloom Filter}},
url = {http://www.biomedcentral.com/1471-2105/12/333},
volume = {12},
year = {2011}
}
@article{salmela2011correcting,
author = {Salmela, L. and Schr\"{o}der, J.},
file = {:home/gus/Documents/Mendeley Desktop/Salmela, Schr\"{o}der\_2011\_Correcting errors in short reads by multiple alignments.pdf:pdf},
journal = {Bioinformatics},
number = {11},
pages = {1455},
publisher = {Oxford Univ Press},
title = {{Correcting errors in short reads by multiple alignments}},
url = {http://bioinformatics.oxfordjournals.org/content/27/11/1455.short},
volume = {27},
year = {2011}
}
@article{Rhrissorrakrai2011a,
abstract = {ABSTRACT: BACKGROUND: Graphical models of network associations are useful for both visualizing and integrating multiple types of association data. Identifying modules, or groups of functionally related gene products, is an important challenge in analyzing biological networks. However, existing tools to identify modules are insufficient when applied to dense networks of experimentally derived interaction data. To address this problem, we have developed an agglomerative clustering method that is able to identify highly modular sets of gene products within highly interconnected molecular interaction networks. RESULTS: MINE outperforms MCODE, CFinder, NEMO, SPICi, and MCL in identifying non-exclusive, high modularity clusters when applied to the C. elegans protein-protein interaction network. The algorithm generally achieves superior geometric accuracy and modularity for annotated functional categories. In comparison with the most closely related algorithm, MCODE, the top clusters identified by MINE are consistently of higher density and MINE is less likely to designate overlapping modules as a single unit. MINE offers a high level of granularity with a small number of adjustable parameters, enabling users to fine-tune cluster results for input networks with differing topological properties. CONCLUSIONS: MINE was created in response to the challenge of discovering high quality modules of gene products within highly interconnected biological networks. The algorithm allows a high degree of flexibility and user-customisation of results with few adjustable parameters. MINE outperforms several popular clustering algorithms in identifying modules with high modularity and obtains good overall recall and precision of functional annotations in protein-protein interaction networks from both S. cerevisiae and C. elegans.},
author = {Rhrissorrakrai, Kahn and Gunsalus, Kristin C},
doi = {10.1186/1471-2105-12-192},
file = {:home/gus/Documents/Mendeley Desktop/Rhrissorrakrai, Gunsalus\_2011\_MINE Module Identification in NEtworks.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
month = may,
number = {1},
pages = {192},
pmid = {21605434},
title = {{MINE: Module Identification in NEtworks.}},
url = {http://www.biomedcentral.com/1471-2105/12/192},
volume = {12},
year = {2011}
}
@article{Navlakha2011,
abstract = {What proteins interacted in a long-extinct ancestor of yeast? How have different members of a protein complex assembled together over time? Our ability to answer such questions has been limited by the unavailability of ancestral protein-protein interaction (PPI) networks. To overcome this limitation, we propose several novel algorithms to reconstruct the growth history of a present-day network. Our likelihood-based method finds a probable previous state of the graph by applying an assumed growth model backwards in time. This approach retains node identities so that the history of individual nodes can be tracked. Using this methodology, we estimate protein ages in the yeast PPI network that are in good agreement with sequence-based estimates of age and with structural features of protein complexes. Further, by comparing the quality of the inferred histories for several different growth models (duplication-mutation with complementarity, forest fire, and preferential attachment), we provide additional evidence that a duplication-based model captures many features of PPI network growth better than models designed to mimic social network growth. From the reconstructed history, we model the arrival time of extant and ancestral interactions and predict that complexes have significantly re-wired over time and that new edges tend to form within existing complexes. We also hypothesize a distribution of per-protein duplication rates, track the change of the network's clustering coefficient, and predict paralogous relationships between extant proteins that are likely to be complementary to the relationships inferred using sequence alone. Finally, we infer plausible parameters for the model, thereby predicting the relative probability of various evolutionary events. The success of these algorithms indicates that parts of the history of the yeast PPI are encoded in its present-day form.},
author = {Navlakha, Saket and Kingsford, Carl},
doi = {10.1371/journal.pcbi.1001119},
editor = {Bader, Joel S.},
file = {:home/gus/Documents/Mendeley Desktop/Navlakha, Kingsford\_2011\_Network archaeology uncovering ancient networks from present-day interactions.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Computational Biology/Systems Biology,Computer Science,Molecular Biology/Bioinformatics,Research Article},
month = apr,
number = {4},
pages = {e1001119},
pmid = {21533211},
publisher = {Public Library of Science},
title = {{Network archaeology: uncovering ancient networks from present-day interactions.}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1001119},
volume = {7},
year = {2011}
}
@article{Goya2010,
abstract = {Motivation: Next-generation sequencing (NGS) has enabled whole genome and transcriptome single nucleotide variant (SNV) discovery in cancer. NGS produces millions of short sequence reads that, once aligned to a reference genome sequence, can be interpreted for the presence of SNVs. Although tools exist for SNV discovery from NGS data, none are specifically suited to work with data from tumors, where altered ploidy and tumor cellularity impact the statistical expectations of SNV discovery. Results: We developed three implementations of a probabilistic Binomial mixture model, called SNVMix, designed to infer SNVs from NGS data from tumors to address this problem. The first models allelic counts as observations and infers SNVs and model parameters using an expectation maximization (EM) algorithm and is therefore capable of adjusting to deviation of allelic frequencies inherent in genomically unstable tumor genomes. The second models nucleotide and mapping qualities of the reads by probabilistically weighting the contribution of a read/nucleotide to the inference of a SNV based on the confidence we have in the base call and the read alignment. The third combines filtering out low-quality data in addition to probabilistic weighting of the qualities. We quantitatively evaluated these approaches on 16 ovarian cancer RNASeq datasets with matched genotyping arrays and a human breast cancer genome sequenced to >40 (haploid) coverage with ground truth data and show systematically that the SNVMix models outperform competing approaches. Availability: Software and data are available at http://compbio.bccrc.ca Contact: sshahbccrc.ca Supplemantary information: Supplementary data are available at Bioinformatics online.},
author = {Goya, Rodrigo and Sun, Mark G F and Morin, Ryan D and Leung, Gillian and Ha, Gavin and Wiegand, Kimberley C and Senz, Janine and Crisan, Anamaria and Marra, Marco A and Hirst, Martin and Huntsman, David and Murphy, Kevin P and Aparicio, Sam and Shah, Sohrab P},
file = {:home/gus/Documents/Mendeley Desktop/Goya et al.\_2010\_SNVMix predicting single nucleotide variants from next-generation sequencing of tumors.pdf:pdf},
institution = {Department of Molecular Oncology Breast Cancer Research Program, British Columbia Cancer Research Centre, Vancouver, BC, Canada.},
journal = {Bioinformatics},
number = {6},
pages = {730--736},
publisher = {Oxford University Press},
title = {{SNVMix: predicting single nucleotide variants from next-generation sequencing of tumors}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2832826\&tool=pmcentrez\&rendertype=abstract},
volume = {26},
year = {2010}
}
@article{trigger1978iroquois,
author = {Trigger, B.},
journal = {Pennsylvania Archaeologist},
pages = {55--65},
title = {{Iroquois matriliny}},
url = {http://scholar.google.com/scholar?hl=fr\&q=Iroquoian+Matriliny\&btnG=Rechercher\&lr=\&as\_ylo=\&as\_vis=0\#0},
volume = {48},
year = {1978}
}
@article{dalloul2010multi,
author = {Dalloul, R.A. and Long, J.A. and Zimin, A.V. and Aslam, L. and Beal, K. and Bouffard, P. and Burt, D.W. and Crasta, O. and Crooijmans, R.P.M.A. and Cooper, K. and Others},
file = {:home/gus/Documents/Mendeley Desktop/Dalloul et al.\_2010\_Multi-platform next-generation sequencing of the domestic turkey (Meleagris gallopavo) genome assembly and analysis.pdf:pdf},
journal = {PLoS biology},
number = {9},
pages = {e1000475},
title = {{Multi-platform next-generation sequencing of the domestic turkey (Meleagris gallopavo): genome assembly and analysis}},
url = {http://dx.plos.org/10.1371/journal.pbio.1000475},
volume = {8},
year = {2010}
}
@article{Li01022010,
abstract = {Next-generation massively parallel DNA sequencing technologies provide ultrahigh throughput at a substantially lower unit data cost; however, the data are very short read length sequences, making de novo assembly extremely challenging. Here, we describe a novel method for de novo assembly of large genomes from short read sequences. We successfully assembled both the Asian and African human genome sequences, achieving an N50 contig size of 7.4 and 5.9 kilobases (kb) and scaffold of 446.3 and 61.9 kb, respectively. The development of this de novo short read assembly method creates new opportunities for building reference sequences and carrying out accurate analyses of unexplored genomes in a cost-effective way.},
author = {Li, Ruiqiang and Zhu, Hongmei and Ruan, Jue and Qian, Wubin and Fang, Xiaodong and Shi, Zhongbin and Li, Yingrui and Li, Shengting and Shan, Gao and Kristiansen, Karsten and Li, Songgang and Yang, Huanming and Wang, Jian and Wang, Jun},
doi = {10.1101/gr.097261.109},
file = {:home/gus/Documents/Mendeley Desktop/Li et al.\_2010\_De novo assembly of human genomes with massively parallel short read sequencing.pdf:pdf},
journal = {Genome Research},
number = {2},
pages = {265--272},
title = {{De novo assembly of human genomes with massively parallel short read sequencing}},
url = {http://genome.cshlp.org/content/20/2/265.abstract},
volume = {20},
year = {2010}
}
@article{Zerbino2009,
author = {Zerbino, DR and McEwen, GK},
file = {:home/gus/Documents/Mendeley Desktop/Zerbino, McEwen\_2009\_Pebble and rock band heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler.pdf:pdf},
journal = {PloS one},
title = {{Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler}},
url = {http://dx.plos.org/10.1371/journal.pone.0008407},
year = {2009}
}
@inproceedings{Chazelle:2004:BFE:982792.982797,
address = {Philadelphia, PA, USA},
author = {Chazelle, Bernard and Kilian, Joe and Rubinfeld, Ronitt and Tal, Ayellet},
booktitle = {Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms},
file = {:home/gus/Documents/Mendeley Desktop/Chazelle et al.\_2004\_The Bloomier filter an efficient data structure for static support lookup tables.pdf:pdf},
isbn = {0-89871-558-X},
pages = {30--39},
publisher = {Society for Industrial and Applied Mathematics},
series = {SODA '04},
title = {{The Bloomier filter: an efficient data structure for static support lookup tables}},
url = {http://dl.acm.org/citation.cfm?id=982797 http://dl.acm.org/citation.cfm?id=982792.982797},
year = {2004}
}
@inproceedings{Gao2004,
author = {Gao, H and Groote, J F and Hesselink, W H},
booktitle = {18th International Parallel and Distributed Processing Symposium},
doi = {10.1109/IPDPS.2004.1302969},
isbn = {0769521320},
number = {C},
pages = {50--58},
publisher = {Ieee},
title = {{Almost wait-free resizable hashtables}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1302969},
volume = {00},
year = {2004}
}
@article{Butler:2008kx,
abstract = {New DNA sequencing technologies deliver data at dramatically lower costs but demand new analytical methods to take full advantage of the very short reads that they produce. We provide an initial, theoretical solution to the challenge of de novo assembly from whole-genome shotgun "microreads." For 11 genomes of sizes up to 39 Mb, we generated high-quality assemblies from 80x coverage by paired 30-base simulated reads modeled after real Illumina-Solexa reads. The bacterial genomes of Campylobacter jejuni and Escherichia coli assemble optimally, yielding single perfect contigs, and larger genomes yield assemblies that are highly connected and accurate. Assemblies are presented in a graph form that retains intrinsic ambiguities such as those arising from polymorphism, thereby providing information that has been absent from previous genome assemblies. For both C. jejuni and E. coli, this assembly graph is a single edge encompassing the entire genome. Larger genomes produce more complicated graphs, but the vast majority of the bases in their assemblies are present in long edges that are nearly always perfect. We describe a general method for genome assembly that can be applied to all types of DNA sequence data, not only short read data, but also conventional sequence reads.},
author = {Butler, Jonathan and MacCallum, Iain and Kleber, Michael and Shlyakhter, Ilya A and Belmonte, Matthew K and Lander, Eric S and Nusbaum, Chad and Jaffe, David B},
doi = {10.1101/gr.7337908},
file = {:home/gus/Documents/Mendeley Desktop/Butler et al.\_2008\_ALLPATHS de novo assembly of whole-genome shotgun microreads.pdf:pdf},
journal = {Genome Res},
month = may,
number = {5},
pages = {810--820},
pmid = {18340039},
title = {{ALLPATHS: de novo assembly of whole-genome shotgun microreads}},
volume = {18},
year = {2008}
}
@article{Dayarian:2010ly,
abstract = {High throughput sequencing (HTS) platforms produce gigabases of short read (<100 bp) data per run. While these short reads are adequate for resequencing applications, de novo assembly of moderate size genomes from such reads remains a significant challenge. These limitations could be partially overcome by utilizing mate pair technology, which provides pairs of short reads separated by a known distance along the genome.},
author = {Dayarian, Adel and Michael, Todd P and Sengupta, Anirvan M},
doi = {10.1186/1471-2105-11-345},
journal = {BMC Bioinformatics},
pages = {345},
pmid = {20576136},
title = {{SOPRA: Scaffolding algorithm for paired reads via statistical optimization}},
volume = {11},
year = {2010}
}
@article{Pevzner:2001uq,
abstract = {For the last 20 years, fragment assembly in DNA sequencing followed the "overlap-layout-consensus" paradigm that is used in all currently available assembly tools. Although this approach proved useful in assembling clones, it faces difficulties in genomic shotgun assembly. We abandon the classical "overlap-layout-consensus" approach in favor of a new euler algorithm that, for the first time, resolves the 20-year-old "repeat problem" in fragment assembly. Our main result is the reduction of the fragment assembly to a variation of the classical Eulerian path problem that allows one to generate accurate solutions of large-scale sequencing problems. euler, in contrast to the celera assembler, does not mask such repeats but uses them instead as a powerful fragment assembly tool.},
author = {Pevzner, P A and Tang, H and Waterman, M S},
doi = {10.1073/pnas.171285098},
file = {:home/gus/Documents/Mendeley Desktop/Pevzner, Tang, Waterman\_2001\_An Eulerian path approach to DNA fragment assembly.pdf:pdf},
journal = {Proc Natl Acad Sci U S A},
month = aug,
number = {17},
pages = {9748--9753},
pmid = {11504945},
title = {{An Eulerian path approach to DNA fragment assembly}},
volume = {98},
year = {2001}
}
@article{mt2006science,
author = {{Mt Pleasant}, J. and Staller, JE and Tykot, RH and Benz, BF},
journal = {Histories of Maize: Multidisciplinary approaches to the prehistory, linguistics, biogeography, domestication, and evolution of maize. Elsevier, San Diego},
pages = {529--537},
title = {{The science behind the three sisters mound system: an agronomic assessment of an indigenous agricultural system in the Northeast}},
url = {http://scholar.google.com/scholar?hl=fr\&q=The+Science+Behind+the+Three+Sisters+Mound+System:+An+Agronomic+Assessment+of+an+Indigenous+Agricultural+System+in+the+Northeast.\&btnG=Rechercher\&lr=\&as\_ylo=\&as\_vis=0\#0},
year = {2006}
}
@article{Kurtz2008,
abstract = {The challenges of accurate gene prediction and enumeration are further aggravated in large genomes that contain highly repetitive transposable elements (TEs). Yet TEs play a substantial role in genome evolution and are themselves an important subject of study. Repeat annotation, based on counting occurrences of k-mers, has been previously used to distinguish TEs from low-copy genic regions; but currently available software solutions are impractical due to high memory requirements or specialization for specific user-tasks.},
author = {Kurtz, Stefan and Narechania, Apurva and Stein, Joshua C and Ware, Doreen},
doi = {10.1186/1471-2164-9-517},
file = {:home/gus/Documents/Mendeley Desktop/Kurtz et al.\_2008\_A new method to compute K-mer frequencies and its application to annotate large repetitive plant genomes.pdf:pdf},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Computational Biology,Computational Biology: methods,DNA Transposable Elements,Genome, Plant,Genomics,Genomics: methods,Methods,Oryza sativa,Software,Sorghum,Zea mays},
month = jan,
pages = {517},
pmid = {18976482},
title = {{A new method to compute K-mer frequencies and its application to annotate large repetitive plant genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2613927\&tool=pmcentrez\&rendertype=abstract},
volume = {9},
year = {2008}
}
@article{Gnerre2011,
abstract = {Massively parallel DNA sequencing technologies are revolutionizing genomics by making it possible to generate billions of relatively short (\~{}100-base) sequence reads at very low cost. Whereas such data can be readily used for a wide range of biomedical applications, it has proven difficult to use them to generate high-quality de novo genome assemblies of large, repeat-rich vertebrate genomes. To date, the genome assemblies generated from such data have fallen far short of those obtained with the older (but much more expensive) capillary-based sequencing approach. Here, we report the development of an algorithm for genome assembly, ALLPATHS-LG, and its application to massively parallel DNA sequence data from the human and mouse genomes, generated on the Illumina platform. The resulting draft genome assemblies have good accuracy, short-range contiguity, long-range connectivity, and coverage of the genome. In particular, the base accuracy is high (99.95\%) and the scaffold sizes (N50 size = 11.5 Mb for human and 7.2 Mb for mouse) approach those obtained with capillary-based sequencing. The combination of improved sequencing technology and improved computational methods should now make it possible to increase dramatically the de novo sequencing of large genomes. The ALLPATHS-LG program is available at http://www.broadinstitute.org/science/programs/genome-biology/crd.},
author = {Gnerre, Sante and Maccallum, Iain and Przybylski, Dariusz and Ribeiro, Filipe J and Burton, Joshua N and Walker, Bruce J and Sharpe, Ted and Hall, Giles and Shea, Terrance P and Sykes, Sean and Berlin, Aaron M and Aird, Daniel and Costello, Maura and Daza, Riza and Williams, Louise and Nicol, Robert and Gnirke, Andreas and Nusbaum, Chad and Lander, Eric S and Jaffe, David B},
doi = {10.1073/pnas.1017351108},
file = {:home/gus/Documents/Mendeley Desktop/Gnerre et al.\_2011\_High-quality draft assemblies of mammalian genomes from massively parallel sequence data.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Algorithms,Animals,Genome,Genome: genetics,Genomics,Genomics: methods,Humans,Internet,Mice,Reproducibility of Results,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
month = jan,
number = {4},
pages = {1513--8},
pmid = {21187386},
title = {{High-quality draft assemblies of mammalian genomes from massively parallel sequence data.}},
url = {http://www.pnas.org/cgi/content/abstract/108/4/1513},
volume = {108},
year = {2011}
}
@article{Berger2011,
abstract = {Prostate cancer is the second most common cause of male cancer deaths in the United States. However, the full range of prostate cancer genomic alterations is incompletely characterized. Here we present the complete sequence of seven primary human prostate cancers and their paired normal counterparts. Several tumours contained complex chains of balanced (that is, 'copy-neutral') rearrangements that occurred within or adjacent to known cancer genes. Rearrangement breakpoints were enriched near open chromatin, androgen receptor and ERG DNA binding sites in the setting of the ETS gene fusion TMPRSS2-ERG, but inversely correlated with these regions in tumours lacking ETS fusions. This observation suggests a link between chromatin or transcriptional regulation and the genesis of genomic aberrations. Three tumours contained rearrangements that disrupted CADM2, and four harboured events disrupting either PTEN (unbalanced events), a prostate tumour suppressor, or MAGI2 (balanced events), a PTEN interacting protein not previously implicated in prostate tumorigenesis. Thus, genomic rearrangements may arise from transcriptional or chromatin aberrancies and engage prostate tumorigenic mechanisms.},
author = {Berger, Michael F and Lawrence, Michael S and Demichelis, Francesca and Drier, Yotam and Cibulskis, Kristian and Sivachenko, Andrey Y and Sboner, Andrea and Esgueva, Raquel and Pflueger, Dorothee and Sougnez, Carrie and Onofrio, Robert and Carter, Scott L and Park, Kyung and Habegger, Lukas and Ambrogio, Lauren and Fennell, Timothy and Parkin, Melissa and Saksena, Gordon and Voet, Douglas and Ramos, Alex H and Pugh, Trevor J and Wilkinson, Jane and Fisher, Sheila and Winckler, Wendy and Mahan, Scott and Ardlie, Kristin and Baldwin, Jennifer and Simons, Jonathan W and Kitabayashi, Naoki and MacDonald, Theresa Y and Kantoff, Philip W and Chin, Lynda and Gabriel, Stacey B and Gerstein, Mark B and Golub, Todd R and Meyerson, Matthew and Tewari, Ashutosh and Lander, Eric S and Getz, Gad and Rubin, Mark A and Garraway, Levi A},
doi = {10.1038/nature09744},
file = {:home/gus/Documents/Mendeley Desktop/Berger et al.\_2011\_The genomic complexity of primary human prostate cancer.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Berger et al.\_2011\_The genomic complexity of primary human prostate cancer(2).pdf:pdf},
isbn = {doi:10.1038/nature09744},
issn = {1476-4687},
journal = {Nature},
keywords = {Carrier Proteins,Carrier Proteins: genetics,Case-Control Studies,Cell Adhesion Molecules,Cell Adhesion Molecules: genetics,Chromatin,Chromatin: genetics,Chromatin: metabolism,Chromosome Aberrations,Chromosome Breakpoints,Epigenesis, Genetic,Epigenesis, Genetic: genetics,Gene Expression Regulation, Neoplastic,Genetic,Genetic: genetics,Genome, Human,Genome, Human: genetics,Human,Human: genetics,Humans,Male,Neoplastic,PTEN Phosphohydrolase,PTEN Phosphohydrolase: genetics,PTEN Phosphohydrolase: metabolism,Prostatic Neoplasms,Prostatic Neoplasms: genetics,Recombination, Genetic,Recombination, Genetic: genetics,Signal Transduction,Signal Transduction: genetics,Transcription, Genetic},
language = {en},
month = feb,
number = {7333},
pages = {214--20},
pmid = {21307934},
publisher = {Nature Publishing Group},
title = {{The genomic complexity of primary human prostate cancer.}},
url = {http://www.nature.com/nature/journal/v470/n7333//full/nature09744.html},
volume = {470},
year = {2011}
}
@article{Zabet2012,
abstract = {Motivation: Transcription factors (TFs) are proteins that regulate gene activity by binding to specific sites on the DNA. Understanding the way these molecules locate their target site is of great importance in understanding gene regulation. We developed a comprehensive computational model of this process and estimated the model parameters in (Zabet and Adryan, 2011). Results: GRiP is a highly versatile implementation of this model and simulates the search process in a computationally efficient way. This program aims to provide researchers in the field with a flexible and highly customisable simulation framework. Its features include representation of DNA sequence, TFs and the interaction between TFs and the DNA (facilitated diffusion mechanism), or between various TFs (cooperative behaviour). The software will record both information on the dynamics associated with the search process (locations of molecules) and also steady state results (affinity landscape, occupancy-bias, collision hotspots). Availability: http://logic.sysbiol.cam.ac.uk/grip Supplementary information: Additional information and data is presented in the Supplementary Material which is available at Bioinformatics online. Contact: n.r.zabet@gen.cam.ac.uk},
author = {Zabet, N. R. and Adryan, B.},
doi = {10.1093/bioinformatics/bts132},
file = {:home/gus/Documents/Mendeley Desktop/Zabet, Adryan\_2012\_GRiP a computational tool to simulate transcription factor binding in prokaryotes.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Zabet, Adryan\_2012\_GRiP a computational tool to simulate transcription factor binding in prokaryotes(2).pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = mar,
pages = {bts132--},
title = {{GRiP: a computational tool to simulate transcription factor binding in prokaryotes}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/bts132v1},
year = {2012}
}
@article{Ladan-Mozes2007,
author = {Ladan-Mozes, Edya and Shavit, Nir},
doi = {10.1007/s00446-007-0050-0},
file = {:home/gus/Documents/Mendeley Desktop/Ladan-Mozes, Shavit\_2007\_An optimistic approach to lock-free FIFO queues.pdf:pdf},
issn = {0178-2770},
journal = {Distributed Computing},
month = nov,
number = {5},
pages = {323--341},
title = {{An optimistic approach to lock-free FIFO queues}},
url = {http://www.springerlink.com/index/10.1007/s00446-007-0050-0},
volume = {20},
year = {2007}
}
@article{Trapnell2009,
abstract = {A new protocol for sequencing the messenger RNA in a cell, known as RNA-Seq, generates millions of short sequence fragments in a single run. These fragments, or 'reads', can be used to measure levels of gene expression and to identify novel splice variants of genes. However, current software for aligning RNA-Seq data to a genome relies on known splice junctions and cannot identify novel ones. TopHat is an efficient read-mapping algorithm designed to align reads from an RNA-Seq experiment to a reference genome without relying on known splice sites.},
author = {Trapnell, Cole and Pachter, Lior and Salzberg, Steven L},
doi = {10.1093/bioinformatics/btp120},
file = {:home/gus/Documents/Mendeley Desktop/Trapnell, Pachter, Salzberg\_2009\_TopHat discovering splice junctions with RNA-Seq.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Trapnell, Pachter, Salzberg\_2009\_TopHat discovering splice junctions with RNA-Seq(2).pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Gene Expression Profiling,Gene Expression Profiling: methods,Genetic,Messenger,Models,RNA,RNA Splicing,RNA Splicing: genetics,Sequence Alignment,Sequence Analysis,Software},
month = may,
number = {9},
pages = {1105--11},
pmid = {19289445},
title = {{TopHat: discovering splice junctions with RNA-Seq.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/25/9/1105},
volume = {25},
year = {2009}
}
@article{Simpson2009,
abstract = {Widespread adoption of massively parallel deoxyribonucleic acid (DNA) sequencing instruments has prompted the recent development of de novo short read assembly algorithms. A common shortcoming of the available tools is their inability to efficiently assemble vast amounts of data generated from large-scale sequencing projects, such as the sequencing of individual human genomes to catalog natural genetic variation. To address this limitation, we developed ABySS (Assembly By Short Sequences), a parallelized sequence assembler. As a demonstration of the capability of our software, we assembled 3.5 billion paired-end reads from the genome of an African male publicly released by Illumina, Inc. Approximately 2.76 million contigs > or =100 base pairs (bp) in length were created with an N50 size of 1499 bp, representing 68\% of the reference human genome. Analysis of these contigs identified polymorphic and novel sequences not present in the human reference assembly, which were validated by alignment to alternate human assemblies and to other primate genomes.},
author = {Simpson, Jared T and Wong, Kim and Jackman, Shaun D and Schein, Jacqueline E and Jones, Steven J M and Birol, Inan\c{c}},
doi = {10.1101/gr.089532.108},
file = {:home/gus/Documents/Mendeley Desktop/Simpson et al.\_2009\_ABySS a parallel assembler for short read sequence data.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Animals,Computational Biology,Computational Biology: methods,Contig Mapping,Escherichia coli K12,Escherichia coli K12: genetics,Genetic Variation,Genome, Human,Humans,Polymorphism, Genetic,Reproducibility of Results,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
month = jun,
number = {6},
pages = {1117--23},
pmid = {19251739},
title = {{ABySS: a parallel assembler for short read sequence data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2694472\&tool=pmcentrez\&rendertype=abstract},
volume = {19},
year = {2009}
}
@article{Chor2009,
abstract = {The empirical frequencies of DNA k-mers in whole genome sequences provide an interesting perspective on genomic complexity, and the availability of large segments of genomic sequence from many organisms means that analysis of k-mers with non-trivial lengths is now possible.},
author = {Chor, Benny and Horn, David and Goldman, Nick and Levy, Yaron and Massingham, Tim},
doi = {10.1186/gb-2009-10-10-r108},
file = {:home/gus/Documents/Mendeley Desktop/Chor et al.\_2009\_Genomic DNA k-mer spectra models and modalities.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Animals,Base Composition,Base Composition: genetics,Chickens,Chickens: genetics,Computer Simulation,CpG Islands,CpG Islands: genetics,DNA,DNA: genetics,Genome,Genome: genetics,Humans,Markov Chains,Models, Genetic,Zebrafish,Zebrafish: genetics},
month = jan,
number = {10},
pages = {R108},
pmid = {19814784},
title = {{Genomic DNA k-mer spectra: models and modalities.}},
url = {http://genomebiology.com/2009/10/10/R108},
volume = {10},
year = {2009}
}
@article{Roberts12122004,
abstract = {Motivation: Comparison of nucleic acid and protein sequences is a fundamental tool of modern bioinformatics. A dominant method of such string matching is the seed-and-extend approach, in which occurrences of short subsequences called seeds are used to search for potentially longer matches in a large database of sequences. Each such potential match is then checked to see if it extends beyond the seed. To be effective, the seed-and-extend approach needs to catalogue seeds from virtually every substring in the database of search strings. Projects such as mammalian genome assemblies and large-scale protein matching, however, have such large sequence databases that the resulting list of seeds cannot be stored in RAM on a single computer. This significantly slows the matching process.Results: We present a simple and elegant method in which only a small fraction of seeds, called minimizers, needs to be stored. Using minimizers can speed up string-matching computations by a large factor while missing only a small fraction of the matches found using all seeds.},
author = {Roberts, Michael and Hayes, Wayne and Hunt, Brian R and Mount, Stephen M and Yorke, James A},
doi = {10.1093/bioinformatics/bth408},
file = {:home/gus/Documents/Mendeley Desktop/Roberts et al.\_2004\_Reducing storage requirements for biological sequence comparison(2).pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
keywords = {Algorithms,Computer-Assisted,Databases,Genetic,Information Storage and Retrieval,Information Storage and Retrieval: methods,Numerical Analysis,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Sequence Analysis: methods},
month = dec,
number = {18},
pages = {3363--3369},
pmid = {15256412},
title = {{Reducing storage requirements for biological sequence comparison}},
url = {http://bioinformatics.oxfordjournals.org/content/20/18/3363.short},
volume = {20},
year = {2004}
}
@inproceedings{Martinez2005,
author = {Martinez, J. and Cumplido, R. and Feregrino, C.},
booktitle = {2005 International Conference on Reconfigurable Computing and FPGAs (ReConFig'05)},
doi = {10.1109/RECONFIG.2005.9},
isbn = {0-7695-2456-7},
month = sep,
pages = {17--17},
publisher = {IEEE},
title = {{An FPGA Parallel Sorting Architecture for the Burrows Wheeler Transform}},
url = {http://dl.acm.org/citation.cfm?id=1114693.1115262},
year = {2005}
}
@article{zhao2011psaec,
author = {Zhao, Z. and Yin, J. and Zhan, Y. and Xiong, W. and Li, Y. and Liu, F.},
file = {:home/gus/Documents/Mendeley Desktop/Zhao et al.\_2011\_PSAEC An Improved Algorithm for Short Read Error Correction Using Partial Suffix Arrays.pdf:pdf},
journal = {Frontiers in Algorithmics and Algorithmic Aspects in Information and Management},
pages = {220--232},
publisher = {Springer},
title = {{PSAEC: An Improved Algorithm for Short Read Error Correction Using Partial Suffix Arrays}},
url = {http://www.springerlink.com/index/9L77826718445828.pdf},
year = {2011}
}
@article{Li2008,
abstract = {New sequencing technologies promise a new era in the use of DNA sequence. However, some of these technologies produce very short reads, typically of a few tens of base pairs, and to use these reads effectively requires new algorithms and software. In particular, there is a major issue in efficiently aligning short reads to a reference genome and handling ambiguity or lack of accuracy in this alignment. Here we introduce the concept of mapping quality, a measure of the confidence that a read actually comes from the position it is aligned to by the mapping algorithm. We describe the software MAQ that can build assemblies by mapping shotgun short reads to a reference genome, using quality scores to derive genotype calls of the consensus sequence of a diploid genome, e.g., from a human sample. MAQ makes full use of mate-pair information and estimates the error probability of each read alignment. Error probabilities are also derived for the final genotype calls, using a Bayesian statistical model that incorporates the mapping qualities, error probabilities from the raw sequence quality scores, sampling of the two haplotypes, and an empirical model for correlated errors at a site. Both read mapping and genotype calling are evaluated on simulated data and real data. MAQ is accurate, efficient, versatile, and user-friendly. It is freely available at http://maq.sourceforge.net.},
author = {Li, Heng and Ruan, Jue and Durbin, Richard},
doi = {10.1101/gr.078212.108},
file = {:home/gus/Documents/Mendeley Desktop/Li, Ruan, Durbin\_2008\_Mapping short DNA sequencing reads and calling variants using mapping quality scores.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Li, Ruan, Durbin\_2008\_Mapping short DNA sequencing reads and calling variants using mapping quality scores(2).pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Bacterial,Bacterial: genetics,Bayes Theorem,Chromosome Mapping,Chromosome Mapping: statistics \& numerical data,Computer Simulation,DNA,DNA: genetics,DNA: statistics \& numerical dat,Diploidy,Genome,Human,Humans,Polymorphism,Reproducibility of Results,Salmonella paratyphi A,Salmonella paratyphi A: genetics,Sequence Alignment,Sequence Alignment: statistics \& numerical data,Sequence Analysis,Single Nucleotide,Software},
month = nov,
number = {11},
pages = {1851--8},
pmid = {18714091},
title = {{Mapping short DNA sequencing reads and calling variants using mapping quality scores.}},
url = {http://genome.cshlp.org/cgi/content/abstract/18/11/1851},
volume = {18},
year = {2008}
}
@article{Kumar2011,
abstract = {To catalog protein-altering mutations that may drive the development of prostate cancers and their progression to metastatic disease systematically, we performed whole-exome sequencing of 23 prostate cancers derived from 16 different lethal metastatic tumors and three high-grade primary carcinomas. All tumors were propagated in mice as xenografts, designated the LuCaP series, to model phenotypic variation, such as responses to cancer-directed therapeutics. Although corresponding normal tissue was not available for most tumors, we were able to take advantage of increasingly deep catalogs of human genetic variation to remove most germline variants. On average, each tumor genome contained [\~{}]200 novel nonsynonymous variants, of which the vast majority was specific to individual carcinomas. A subset of genes was recurrently altered across tumors derived from different individuals, including TP53, DLK2, GPC6, and SDF4. Unexpectedly, three prostate cancer genomes exhibited substantially higher mutation frequencies, with 2,000-4,000 novel coding variants per exome. A comparison of castration-resistant and castration-sensitive pairs of tumor lines derived from the same prostate cancer highlights mutations in the Wnt pathway as potentially contributing to the development of castration resistance. Collectively, our results indicate that point mutations arising in coding regions of advanced prostate cancers are common but, with notable exceptions, very few genes are mutated in a substantial fraction of tumors. We also report a previously undescribed subtype of prostate cancers exhibiting "hypermutated" genomes, with potential implications for resistance to cancer therapeutics. Our results also suggest that increasingly deep catalogs of human germline variation may challenge the necessity of sequencing matched tumor-normal pairs.},
author = {Kumar, A. and White, T. A. and MacKenzie, A. P. and Clegg, N. and Lee, C. and Dumpit, R. F. and Coleman, I. and Ng, S. B. and Salipante, S. J. and Rieder, M. J. and Nickerson, D. A. and Corey, E. and Lange, P. H. and Morrissey, C. and Vessella, R. L. and Nelson, P. S. and Shendure, J.},
doi = {10.1073/pnas.1108745108},
file = {:home/gus/Documents/Mendeley Desktop/Kumar et al.\_2011\_Exome sequencing identifies a spectrum of mutation frequencies in advanced and lethal prostate cancers.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
month = sep,
number = {41},
pages = {17087--17092},
title = {{Exome sequencing identifies a spectrum of mutation frequencies in advanced and lethal prostate cancers}},
url = {http://www.pnas.org/cgi/content/abstract/108/41/17087},
volume = {108},
year = {2011}
}
@article{Pop:2009fk,
abstract = {Research into genome assembly algorithms has experienced a resurgence due to new challenges created by the development of next generation sequencing technologies. Several genome assemblers have been published in recent years specifically targeted at the new sequence data; however, the ever-changing technological landscape leads to the need for continued research. In addition, the low cost of next generation sequencing data has led to an increased use of sequencing in new settings. For example, the new field of metagenomics relies on large-scale sequencing of entire microbial communities instead of isolate genomes, leading to new computational challenges. In this article, we outline the major algorithmic approaches for genome assembly and describe recent developments in this domain.},
author = {Pop, Mihai},
doi = {10.1093/bib/bbp026},
journal = {Brief Bioinform},
month = jul,
number = {4},
pages = {354--366},
pmid = {19482960},
title = {{Genome assembly reborn: recent computational challenges}},
volume = {10},
year = {2009}
}
@article{Brummitt2012,
abstract = {Understanding how interdependence among systems affects cascading behaviors is increasingly important across many fields of science and engineering. Inspired by cascades of load shedding in coupled electric grids and other infrastructure, we study the Bak-Tang-Wiesenfeld sandpile model on modular random graphs and on graphs based on actual, interdependent power grids. Starting from two isolated networks, adding some connectivity between them is beneficial, for it suppresses the largest cascades in each system. Too much interconnectivity, however, becomes detrimental for two reasons. First, interconnections open pathways for neighboring networks to inflict large cascades. Second, as in real infrastructure, new interconnections increase capacity and total possible load, which fuels even larger cascades. Using a multitype branching process and simulations we show these effects and estimate the optimal level of interconnectivity that balances their trade-offs. Such equilibria could allow, for example, power grid owners to minimize the largest cascades in their grid. We also show that asymmetric capacity among interdependent networks affects the optimal connectivity that each prefers and may lead to an arms race for greater capacity. Our multitype branching process framework provides building blocks for better prediction of cascading processes on modular random graphs and on multitype networks in general.},
author = {Brummitt, Charles D and D'Souza, Raissa M and Leicht, E A},
doi = {10.1073/pnas.1110586109},
file = {:home/gus/Documents/Mendeley Desktop/Brummitt, D'Souza, Leicht\_2012\_Suppressing cascades of load in interdependent networks.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = feb,
number = {12},
pages = {E680--689},
pmid = {22355144},
title = {{Suppressing cascades of load in interdependent networks.}},
url = {http://www.pnas.org/cgi/content/abstract/109/12/E680},
volume = {109},
year = {2012}
}
@article{Schuenemann2011,
abstract = {Although investigations of medieval plague victims have identified Yersinia pestis as the putative etiologic agent of the pandemic, methodological limitations have prevented large-scale genomic investigations to evaluate changes in the pathogen's virulence over time. We screened over 100 skeletal remains from Black Death victims of the East Smithfield mass burial site (1348-1350, London, England). Recent methods of DNA enrichment coupled with high-throughput DNA sequencing subsequently permitted reconstruction of ten full human mitochondrial genomes (16 kb each) and the full pPCP1 (9.6 kb) virulence-associated plasmid at high coverage. Comparisons of molecular damage profiles between endogenous human and Y. pestis DNA confirmed its authenticity as an ancient pathogen, thus representing the longest contiguous genomic sequence for an ancient pathogen to date. Comparison of our reconstructed plasmid against modern Y. pestis shows identity with several isolates matching the Medievalis biovar; however, our chromosomal sequences indicate the victims were infected with a Y. pestis variant that has not been previously reported. Our data reveal that the Black Death in medieval Europe was caused by a variant of Y. pestis that may no longer exist, and genetic data carried on its pPCP1 plasmid were not responsible for the purported epidemiological differences between ancient and modern forms of Y. pestis infections.},
author = {Schuenemann, V. J. and Bos, K. and DeWitte, S. and Schmedes, S. and Jamieson, J. and Mittnik, A. and Forrest, S. and Coombes, B. K. and Wood, J. W. and Earn, D. J. D. and White, W. and Krause, J. and Poinar, H. N.},
doi = {10.1073/pnas.1105107108},
file = {:home/gus/Documents/Mendeley Desktop/Schuenemann et al.\_2011\_PNAS Plus Targeted enrichment of ancient pathogens yielding the pPCP1 plasmid of Yersinia pestis from victims of the Black Death.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
month = aug,
pages = {1105107108--},
title = {{PNAS Plus: Targeted enrichment of ancient pathogens yielding the pPCP1 plasmid of Yersinia pestis from victims of the Black Death}},
url = {http://www.pnas.org/cgi/content/abstract/1105107108v1},
year = {2011}
}
@article{Ferragina2000,
author = {Ferragina, P. and Manzini, G.},
file = {:home/gus/Documents/Mendeley Desktop/Ferragina, Manzini\_2000\_Opportunistic data structures with applications.pdf:pdf},
isbn = {0-7695-0850-2},
keywords = {Glimpse tool,computational complexity,data compression,data indexing,data set,data structures,database indexing,database theory,entropy,opportunistic data structures,query performance,search,sublinear query time complexity,sublinear space complexity,succinct suffix array,suffix array data structures,suffix tree data structures},
month = nov,
pages = {390},
title = {{Opportunistic data structures with applications}},
url = {http://dl.acm.org/citation.cfm?id=795666.796543 http://www.di.unipi.it/~ferragin/Libraries/fmindexV2/index.html},
year = {2000}
}
@article{David2011,
abstract = {We report on a major update (version 2) of the original SHort Read Mapping Program (SHRiMP). SHRiMP2 primarily targets mapping sensitivity, and is able to achieve high accuracy at a very reasonable speed. SHRiMP2 supports both letter space and color space (AB/SOLiD) reads, enables for direct alignment of paired reads and uses parallel computation to fully utilize multi-core architectures. AVAILABILITY: SHRiMP2 executables and source code are freely available at: http://compbio.cs.toronto.edu/shrimp/.},
author = {David, Matei and Dzamba, Misko and Lister, Dan and Ilie, Lucian and Brudno, Michael},
doi = {10.1093/bioinformatics/btr046},
file = {:home/gus/Documents/Mendeley Desktop/David et al.\_2011\_SHRiMP2 sensitive yet practical SHort Read Mapping.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Chromosome Mapping,DNA,Genetic,Genomics,Genomics: methods,Polymorphism,Sequence Alignment,Sequence Analysis,Software},
month = apr,
number = {7},
pages = {1011--2},
pmid = {21278192},
title = {{SHRiMP2: sensitive yet practical SHort Read Mapping.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21278192},
volume = {27},
year = {2011}
}
@article{Chong1993,
author = {Chong, Ka Wong and Lam, Tak Wah},
file = {:home/gus/Documents/Mendeley Desktop/Chong, Lam\_1993\_Finding connected components in O (log n loglog n ) time on the EREW PRAM.pdf:pdf},
isbn = {0-89871-313-7},
month = jan,
pages = {11--20},
title = {{Finding connected components in O (log n loglog n ) time on the EREW PRAM}},
url = {http://dl.acm.org/citation.cfm?id=313559.313575},
year = {1993}
}
@article{Rogers2012,
abstract = {ABSTRACT: We propose a method for predicting splice graphs that enhances curated gene models using evidence from RNA-Seq and EST alignments. Results obtained using RNA-Seq experiments in Arabidopsis thaliana show that predictions made by our SpliceGrapher method are more consistent with current gene models than predictions made by TAU and Cufflinks. Furthermore, analysis of plant and human data indicates that the machine learning approach used by SpliceGrapher is useful for discriminating between real and spurious splice sites, and can improve the reliability of detection of alternative splicing. SpliceGrapher is available for download at http://SpliceGrapher.sf.net.},
author = {Rogers, Mark F and Thomas, Julie and Reddy, Anireddy Sn and Ben-Hur, Asa},
doi = {10.1186/gb-2012-13-1-r4},
issn = {1465-6914},
journal = {Genome biology},
month = jan,
number = {1},
pages = {R4},
pmid = {22293517},
title = {{SpliceGrapher: detecting patterns of alternative splicing from RNA-Seq data in the context of gene models and EST data.}},
url = {http://genomebiology.com/2012/13/1/R4},
volume = {13},
year = {2012}
}
@article{Manber1990,
author = {Manber, Udi and Myers, Gene},
file = {:home/gus/Documents/Mendeley Desktop/Manber, Myers\_1990\_Suffix arrays a new method for on-line string searches.pdf:pdf},
isbn = {0-89871-251-3},
journal = {Proceedings of the first annual ACM-SIAM  \ldots},
month = jan,
pages = {319--327},
title = {{Suffix arrays: a new method for on-line string searches}},
url = {http://portal.acm.org/citation.cfm?id=320218 http://dl.acm.org/citation.cfm?id=320176.320218},
year = {1990}
}
@article{Venter2001,
author = {Venter, JC and Adams, MD and Myers, EW},
file = {:home/gus/Documents/Mendeley Desktop/Venter, Adams, Myers\_2001\_The sequence of the human genome.pdf:pdf},
journal = {Science's \ldots},
title = {{The sequence of the human genome}},
url = {http://stke.sciencemag.org/cgi/content/abstract/sci;291/5507/1304},
year = {2001}
}
@techreport{Drepper2007,
author = {Drepper, Ulrich},
file = {:home/gus/Documents/Mendeley Desktop/Drepper\_2007\_What Every Programmer Should Know About Memory.pdf:pdf},
keywords = {Cache,Memory,lock free},
title = {{What Every Programmer Should Know About Memory}},
year = {2007}
}
@article{Almeida2007,
author = {Almeida, Paulo S\'{e}rgio PS and Baquero, Carlos and Pregui\c{c}a, Nuno and Hutchison, David},
file = {:home/gus/Documents/Mendeley Desktop//Almeida et al.\_2007\_Scalable Bloom Filters.pdf:pdf},
issn = {0020-0190},
journal = {Information Processing Letters},
keywords = {Distributed systems,Randomized algorithms,bloom filters,data structures,dis-,expected,lower false positive rates,require more state},
month = mar,
number = {6},
pages = {255--261},
title = {{Scalable Bloom Filters}},
url = {http://www.sciencedirect.com/science/article/pii/S0020019006003127},
volume = {101},
year = {2007}
}
@article{Liu2012,
abstract = {SUMMARY: SOAP3 is the first short read alignment tool that leverages the multi-processors in a graphic processing unit (GPU) to achieve a drastic improvement in speed. We adapted the compressed full-text index (BWT) used by SOAP2 in view of the advantages and disadvantages of GPU. When tested with millions of Illumina Hiseq 2000 length-100bp reads, SOAP3 takes less than 30 seconds to align a million read pairs onto the human reference genome and is at least 7.5 and 20 times faster than BWA and Bowtie, respectively. For aligning reads with up to 4 mismatches, SOAP3 aligns slightly more reads than BWA and Bowtie; this is because SOAP3, unlike BWA and Bowtie, is not heuristic-based and always reports all answers. AVAILABILITY: SOAP3 is available at: http://www.cs.hku.hk/2bwt-tools/soap3; and http://soap.genomics.org.cn/soap3.html. CONTACT: liruiqiang@gmail.com, twlam@cs.hku.hk.},
author = {Liu, Chi-Man and Wong, Thomas and Wu, Edward and Luo, Ruibang and Yiu, Siu-Ming and Li, Yingrui and Wang, B and Yu, C and Chu, X and Zhao, K and Li, Ruiqiang and Lam, Tak-Wah},
doi = {10.1093/bioinformatics/bts061},
file = {:home/gus/Documents/Mendeley Desktop/Liu et al.\_2012\_SOAP3 Ultra-fast GPU-based parallel alignment tool for short reads.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = jan,
number = {6},
pages = {878--879},
pmid = {22285832},
title = {{SOAP3: Ultra-fast GPU-based parallel alignment tool for short reads.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/28/6/878},
volume = {28},
year = {2012}
}
@book{Menon2011,
address = {New York, New York, USA},
author = {Menon, Rohith K. and Bhat, Goutham P. and Schatz, Michael C.},
booktitle = {Proceedings of the second international workshop on MapReduce and its applications - MapReduce '11},
doi = {10.1145/1996092.1996104},
file = {:home/gus/Documents/Mendeley Desktop/Menon, Bhat, Schatz\_2011\_Rapid parallel genome indexing with MapReduce.pdf:pdf},
isbn = {9781450307000},
keywords = {DNA sequence analysis,MapReduce,burrows-wheeler transform,cloud computing,parallel suffix array construction},
month = jun,
pages = {51},
publisher = {ACM Press},
title = {{Rapid parallel genome indexing with MapReduce}},
url = {http://dl.acm.org/citation.cfm?id=1996092.1996104},
year = {2011}
}
@article{Nakamura2011,
abstract = {We identified the sequence-specific starting positions of consecutive miscalls in the mapping of reads obtained from the Illumina Genome Analyser (GA). Detailed analysis of the miscall pattern indicated that the underlying mechanism involves sequence-specific interference of the base elongation process during sequencing. The two major sequence patterns that trigger this sequence-specific error (SSE) are: (i) inverted repeats and (ii) GGC sequences. We speculate that these sequences favor dephasing by inhibiting single-base elongation, by: (i) folding single-stranded DNA and (ii) altering enzyme preference. This phenomenon is a major cause of sequence coverage variability and of the unfavorable bias observed for population-targeted methods such as RNA-seq and ChIP-seq. Moreover, SSE is a potential cause of false single-nucleotide polymorphism (SNP) calls and also significantly hinders de novo assembly. This article highlights the importance of recognizing SSE and its underlying mechanisms in the hope of enhancing the potential usefulness of the Illumina sequencers.},
author = {Nakamura, Kensuke and Oshima, Taku and Morimoto, Takuya and Ikeda, Shun and Yoshikawa, Hirofumi and Shiwa, Yuh and Ishikawa, Shu and Linak, Margaret C and Hirai, Aki and Takahashi, Hiroki and Altaf-Ul-Amin, Md and Ogasawara, Naotake and Kanaya, Shigehiko},
doi = {10.1093/nar/gkr344},
file = {:home/gus/Documents/Mendeley Desktop/Nakamura et al.\_2011\_Sequence-specific error profile of Illumina sequencers.pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
month = may,
number = {13},
pages = {e90--},
pmid = {21576222},
title = {{Sequence-specific error profile of Illumina sequencers.}},
url = {http://nar.oxfordjournals.org/cgi/content/abstract/39/13/e90},
volume = {39},
year = {2011}
}
@article{Edgar2004,
abstract = {We describe MUSCLE, a new computer program for creating multiple alignments of protein sequences. Elements of the algorithm include fast distance estimation using kmer counting, progressive alignment using a new profile function we call the log-expectation score, and refinement using tree-dependent restricted partitioning. The speed and accuracy of MUSCLE are compared with T-Coffee, MAFFT and CLUSTALW on four test sets of reference alignments: BAliBASE, SABmark, SMART and a new benchmark, PREFAB. MUSCLE achieves the highest, or joint highest, rank in accuracy on each of these sets. Without refinement, MUSCLE achieves average accuracy statistically indistinguishable from T-Coffee and MAFFT, and is the fastest of the tested methods for large numbers of sequences, aligning 5000 sequences of average length 350 in 7 min on a current desktop computer. The MUSCLE program, source code and PREFAB test data are freely available at http://www.drive5. com/muscle.},
author = {Edgar, Robert C},
doi = {10.1093/nar/gkh340},
file = {:home/gus/Documents/Mendeley Desktop/Edgar\_2004\_MUSCLE multiple sequence alignment with high accuracy and high throughput.pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Algorithms,Amino Acid Motifs,Amino Acid Sequence,Internet,Molecular Sequence Data,Reproducibility of Results,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, Protein,Sequence Analysis, Protein: methods,Software,Time Factors},
month = jan,
number = {5},
pages = {1792--7},
pmid = {15034147},
title = {{MUSCLE: multiple sequence alignment with high accuracy and high throughput.}},
url = {http://nar.oxfordjournals.org/cgi/content/abstract/32/5/1792},
volume = {32},
year = {2004}
}
@article{Manber1990,
author = {Manber, Udi and Myers, Gene},
file = {:home/gus/Documents/Mendeley Desktop/Manber, Myers\_1990\_Suffix arrays a new method for on-line string searches(2).pdf:pdf;:home/gus/Documents/Mendeley Desktop/Manber, Myers\_1990\_Suffix arrays a new method for on-line string searches.pdf:pdf},
isbn = {0-89871-251-3},
journal = {Proceedings of the first annual ACM-SIAM \ldots},
month = jan,
pages = {319--327},
title = {{Suffix arrays: a new method for on-line string searches}},
url = {http://portal.acm.org/citation.cfm?id=320218 http://dl.acm.org/citation.cfm?id=320176.320218},
year = {1990}
}
@article{Trapnell2010,
abstract = {High-throughput mRNA sequencing (RNA-Seq) promises simultaneous transcript discovery and abundance estimation. However, this would require algorithms that are not restricted by prior gene annotations and that account for alternative transcription and splicing. Here we introduce such algorithms in an open-source software program called Cufflinks. To test Cufflinks, we sequenced and analyzed >430 million paired 75-bp RNA-Seq reads from a mouse myoblast cell line over a differentiation time series. We detected 13,692 known transcripts and 3,724 previously unannotated ones, 62\% of which are supported by independent expression data or by homologous genes in other species. Over the time series, 330 genes showed complete switches in the dominant transcription start site (TSS) or splice isoform, and we observed more subtle shifts in 1,304 other genes. These results suggest that Cufflinks can illuminate the substantial regulatory flexibility and complexity in even this well-studied model of muscle development and that it can improve transcriptome-based genome annotation.},
author = {Trapnell, Cole and Williams, Brian A and Pertea, Geo and Mortazavi, Ali and Kwan, Gordon and van Baren, Marijke J and Salzberg, Steven L and Wold, Barbara J and Pachter, Lior},
doi = {10.1038/nbt.1621},
issn = {1546-1696},
journal = {Nature biotechnology},
keywords = {Algorithms,Animals,Cell Differentiation,Cell Differentiation: genetics,Cell Line,Gene Expression Profiling,Gene Expression Profiling: methods,Genome,Messenger,Messenger: analysis,Messenger: genetics,Messenger: metabolism,Mice,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Protein Isoforms,Protein Isoforms: genetics,Protein Isoforms: metabolism,Proto-Oncogene Proteins c-myc,Proto-Oncogene Proteins c-myc: genetics,Proto-Oncogene Proteins c-myc: metabolism,RNA,RNA: methods,Sequence Analysis,Software},
month = may,
number = {5},
pages = {511--5},
pmid = {20436464},
publisher = {Nature Publishing Group},
shorttitle = {Nat Biotech},
title = {{Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation.}},
url = {http://dx.doi.org/10.1038/nbt.1621},
volume = {28},
year = {2010}
}
@article{Gnerre:2011zr,
abstract = {Massively parallel DNA sequencing technologies are revolutionizing genomics by making it possible to generate billions of relatively short (\~{}100-base) sequence reads at very low cost. Whereas such data can be readily used for a wide range of biomedical applications, it has proven difficult to use them to generate high-quality de novo genome assemblies of large, repeat-rich vertebrate genomes. To date, the genome assemblies generated from such data have fallen far short of those obtained with the older (but much more expensive) capillary-based sequencing approach. Here, we report the development of an algorithm for genome assembly, ALLPATHS-LG, and its application to massively parallel DNA sequence data from the human and mouse genomes, generated on the Illumina platform. The resulting draft genome assemblies have good accuracy, short-range contiguity, long-range connectivity, and coverage of the genome. In particular, the base accuracy is high (99.95\%) and the scaffold sizes (N50 size = 11.5 Mb for human and 7.2 Mb for mouse) approach those obtained with capillary-based sequencing. The combination of improved sequencing technology and improved computational methods should now make it possible to increase dramatically the de novo sequencing of large genomes. The ALLPATHS-LG program is available at http://www.broadinstitute.org/science/programs/genome-biology/crd.},
author = {Gnerre, Sante and Maccallum, Iain and Przybylski, Dariusz and Ribeiro, Filipe J and Burton, Joshua N and Walker, Bruce J and Sharpe, Ted and Hall, Giles and Shea, Terrance P and Sykes, Sean and Berlin, Aaron M and Aird, Daniel and Costello, Maura and Daza, Riza and Williams, Louise and Nicol, Robert and Gnirke, Andreas and Nusbaum, Chad and Lander, Eric S and Jaffe, David B},
doi = {10.1073/pnas.1017351108},
journal = {Proc Natl Acad Sci U S A},
month = jan,
number = {4},
pages = {1513--1518},
pmid = {21187386},
title = {{High-quality draft assemblies of mammalian genomes from massively parallel sequence data}},
volume = {108},
year = {2011}
}
@article{Marsaglia2003,
abstract = {Description of a class of simple, extremely fast random number generators (RNGs) with periods 2 k - 1 for k = 32, 64, 96, 128, 160, 192. These RNGs seem to pass tests of randomness very well.},
author = {Marsaglia, George},
file = {:home/gus/Documents/Mendeley Desktop/Marsaglia\_2003\_Xorshift RNGs.pdf:pdf},
issn = {15487660},
journal = {Journal Of Statistical Software},
number = {14},
pages = {1--6},
publisher = {American Statistical Association},
title = {{Xorshift RNGs}},
url = {http://www.jstatsoft.org/v08/i14/paper},
volume = {8},
year = {2003}
}
@article{herlihy2002repeat,
author = {Herlihy, M. and Luchangco, V. and Moir, M.},
file = {:home/gus/Documents/Mendeley Desktop/Herlihy, Luchangco, Moir\_2002\_The repeat offender problem A mechanism for supporting dynamic-sized lock-free data structures.pdf:pdf},
publisher = {Sun Microsystems, Inc.},
title = {{The repeat offender problem: A mechanism for supporting dynamic-sized lock-free data structures}},
url = {http://portal.acm.org/citation.cfm?id=1698158},
year = {2002}
}
@article{Kelley:2010fk,
abstract = {ABSTRACT : We introduce Quake, a program to detect and correct errors in DNA sequencing reads. Using a maximum likelihood approach incorporating quality values and nucleotide specific miscall rates, Quake achieves the highest accuracy on realistically simulated reads. We further demonstrate substantial improvements in de novo assembly and SNP detection after using Quake. Quake can be used for any size project, including more than one billion human reads, and is freely available as open source software from http://www.cbcb.umd.edu/software/quake.},
author = {Kelley, David R and Schatz, Michael C and Salzberg, Steven L},
doi = {10.1186/gb-2010-11-11-r116},
file = {:home/gus/Documents/Mendeley Desktop/Kelley, Schatz, Salzberg\_2010\_Quake quality-aware detection and correction of sequencing errors.pdf:pdf},
journal = {Genome Biol},
number = {11},
pages = {R116},
pmid = {21114842},
title = {{Quake: quality-aware detection and correction of sequencing errors}},
volume = {11},
year = {2010}
}
@article{Qi2008,
abstract = {The yeast synthetic lethal genetic interaction network contains rich information about underlying pathways and protein complexes as well as new genetic interactions yet to be discovered. We have developed a graph diffusion kernel as a unified framework for inferring complex/pathway membership analogous to "friends" and genetic interactions analogous to "enemies" from the genetic interaction network. When applied to the Saccharomyces cerevisiae synthetic lethal genetic interaction network, we can achieve a precision around 50\% with 20\% to 50\% recall in the genome-wide prediction of new genetic interactions, supported by experimental validation. The kernels show significant improvement over previous best methods for predicting genetic interactions and protein co-complex membership from genetic interaction data.},
author = {Qi, Yan and Suhail, Yasir and Lin, Yu-yi and Boeke, Jef D and Bader, Joel S},
doi = {10.1101/gr.077693.108},
file = {:home/gus/Documents/Mendeley Desktop/Qi et al.\_2008\_Finding friends and enemies in an enemies-only network a graph diffusion kernel for predicting novel genetic interactions and co-complex membership from yeast genetic interactions.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Fungal,Gene Expression Regulation,Genes,Genetic,Genomics,Genomics: methods,Models,Protein Interaction Mapping,Protein Interaction Mapping: methods,Reproducibility of Results,Saccharomyces cerevisiae,Saccharomyces cerevisiae Proteins,Saccharomyces cerevisiae Proteins: genetics,Saccharomyces cerevisiae Proteins: metabolism,Saccharomyces cerevisiae: genetics,Saccharomyces cerevisiae: metabolism,Statistical},
month = dec,
number = {12},
pages = {1991--2004},
pmid = {18832443},
title = {{Finding friends and enemies in an enemies-only network: a graph diffusion kernel for predicting novel genetic interactions and co-complex membership from yeast genetic interactions.}},
url = {http://genome.cshlp.org/cgi/content/abstract/18/12/1991},
volume = {18},
year = {2008}
}
@misc{TheMendeleySupportTeam2011,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:home/gus/Documents/Mendeley Desktop/The Mendeley Support Team\_2011\_Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@article{Chaisson2008,
abstract = {In the last year, high-throughput sequencing technologies have progressed from proof-of-concept to production quality. While these methods produce high-quality reads, they have yet to produce reads comparable in length to Sanger-based sequencing. Current fragment assembly algorithms have been implemented and optimized for mate-paired Sanger-based reads, and thus do not perform well on short reads produced by short read technologies. We present a new Eulerian assembler that generates nearly optimal short read assemblies of bacterial genomes and describe an approach to assemble reads in the case of the popular hybrid protocol when short and long Sanger-based reads are combined.},
author = {Chaisson, Mark J and Pevzner, Pavel A},
doi = {10.1101/gr.7088808},
file = {:home/gus/Documents/Mendeley Desktop/Chaisson, Pevzner\_2008\_Short read fragment assembly of bacterial genomes.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Bacterial,Bacterial: genetics,Contig Mapping,Contig Mapping: methods,DNA,DNA: methods,Genome,Genomics,Genomics: methods,Research Design,Sequence Analysis},
month = feb,
number = {2},
pages = {324--30},
pmid = {18083777},
title = {{Short read fragment assembly of bacterial genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2203630\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@article{Scally2012,
author = {Scally, Aylwyn and Dutheil, Julien Y. and Hillier, LaDeana W. and Jordan, Gregory E. and Goodhead, Ian and Herrero, Javier and Hobolth, Asger and Lappalainen, Tuuli and Mailund, Thomas and Marques-Bonet, Tomas and McCarthy, Shane and Montgomery, Stephen H. and Schwalie, Petra C. and Tang, Y. Amy and Ward, Michelle C. and Xue, Yali and Yngvadottir, Bryndis and Alkan, Can and Andersen, Lars N. and Ayub, Qasim and Ball, Edward V. and Beal, Kathryn and Bradley, Brenda J. and Chen, Yuan and Clee, Chris M. and Fitzgerald, Stephen and Graves, Tina A. and Gu, Yong and Heath, Paul and Heger, Andreas and Karakoc, Emre and Kolb-Kokocinski, Anja and Laird, Gavin K. and Lunter, Gerton and Meader, Stephen and Mort, Matthew and Mullikin, James C. and Munch, Kasper and OConnor, Timothy D. and Phillips, Andrew D. and Prado-Martinez, Javier and Rogers, Anthony S. and Sajjadian, Saba and Schmidt, Dominic and Shaw, Katy and Simpson, Jared T. and Stenson, Peter D. and Turner, Daniel J. and Vigilant, Linda and Vilella, Albert J. and Whitener, Weldon and Zhu, Baoli and Cooper, David N. and de Jong, Pieter and Dermitzakis, Emmanouil T. and Eichler, Evan E. and Flicek, Paul and Goldman, Nick and Mundy, Nicholas I. and Ning, Zemin and Odom, Duncan T. and Ponting, Chris P. and Quail, Michael A. and Ryder, Oliver A. and Searle, Stephen M. and Warren, Wesley C. and Wilson, Richard K. and Schierup, Mikkel H. and Rogers, Jane and Tyler-Smith, Chris and Durbin, Richard},
doi = {10.1038/nature10842},
file = {:home/gus/Documents/Mendeley Desktop/Scally et al.\_2012\_Insights into hominid evolution from the gorilla genome sequence.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Scally et al.\_2012\_Insights into hominid evolution from the gorilla genome sequence(3).pdf:pdf},
issn = {0028-0836},
journal = {Nature},
month = mar,
number = {7388},
pages = {169--175},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nature},
title = {{Insights into hominid evolution from the gorilla genome sequence}},
url = {http://dx.doi.org/10.1038/nature10842},
volume = {483},
year = {2012}
}
@article{Richter2008,
abstract = {The new research field of metagenomics is providing exciting insights into various, previously unclassified ecological systems. Next-generation sequencing technologies are producing a rapid increase of environmental data in public databases. There is great need for specialized software solutions and statistical methods for dealing with complex metagenome data sets.},
author = {Richter, Daniel C and Ott, Felix and Auch, Alexander F and Schmid, Ramona and Huson, Daniel H},
doi = {10.1371/journal.pone.0003373},
editor = {Field, Dawn},
file = {:home/gus/Documents/Mendeley Desktop/Richter et al.\_2008\_MetaSim a sequencing simulator for genomics and metagenomics.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Databases, Genetic,Genomics,Models, Theoretical,User-Computer Interface},
month = jan,
number = {10},
pages = {e3373},
pmid = {18841204},
publisher = {Public Library of Science},
title = {{MetaSim: a sequencing simulator for genomics and metagenomics.}},
url = {http://dx.plos.org/10.1371/journal.pone.0003373},
volume = {3},
year = {2008}
}
@article{Peterlongo2011,
abstract = {Background: The analysis of next-generation sequencing data from large genomes is a timely research topic. Sequencers are producing billions of short sequence fragments from newly sequenced organisms. Computational methods for reconstructing sequences (whole-genome assemblers) are typically employed to process such data. However, one of the main drawback of these methods is the high memory requirement. Results: We present Mapsembler, an iterative targeted assembler which processes large datasets of reads on commodity hardware. Mapsembler checks for the presence of given regions of interest in the reads and reconstructs their neighborhood, either as a plain sequence (consensus) or as a graph (full sequence structure). We introduce new algorithms to retrieve homologues of a sequence from reads and construct an extension graph. Conclusions: Mapsembler is the rst software that enables de novo discovery around a region of interest of gene homologues, SNPs, exon skipping as well as other structural events, directly from raw sequencing reads. Compared to traditional assembly software, memory requirement and execution time of Mapsembler are considerably lower, as data indexing is localized. Mapsembler can be used at http://mapsembler.genouest.org},
author = {Peterlongo, Pierre and Chikhi, Rayan},
file = {:home/gus/Documents/Mendeley Desktop/Peterlongo, Chikhi\_2011\_Mapsembler, targeted assembly of larges genomes on a desktop computer.pdf:pdf},
keywords = {NGS,algorithms,bioinformatics,genome,light memory usage,targeted assembly},
month = mar,
title = {{Mapsembler, targeted assembly of larges genomes on a desktop computer}},
url = {http://hal.inria.fr/inria-00577218/ http://hal.inria.fr/docs/00/57/72/18/PDF/RR-7565.pdf},
year = {2011}
}
@article{michael2004hazard,
author = {Michael, M.M.},
file = {:home/gus/Documents/Mendeley Desktop/Michael\_2004\_Hazard pointers Safe memory reclamation for lock-free objects.pdf:pdf},
journal = {Parallel and Distributed Systems, IEEE Transactions on},
number = {6},
pages = {491--504},
publisher = {IEEE},
title = {{Hazard pointers: Safe memory reclamation for lock-free objects}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1291819},
volume = {15},
year = {2004}
}
@article{Yang2011a,
abstract = {High-throughput short read sequencing is revolutionizing genomics and systems biology research by enabling cost-effective deep coverage sequencing of genomes and transcriptomes. Error detection and correction are crucial to many short read sequencing applications including de novo genome sequencing, genome resequencing, and digital gene expression analysis. Short read error detection is typically carried out by counting the observed frequencies of kmers in reads and validating those with frequencies exceeding a threshold. In case of genomes with high repeat content, an erroneous kmer may be frequently observed if it has few nucleotide differences with valid kmers with multiple occurrences in the genome. Error detection and correction were mostly applied to genomes with low repeat content and this remains a challenging problem for genomes with high repeat content.},
author = {Yang, Xiao and Aluru, Srinivas and Dorman, Karin S},
doi = {10.1186/1471-2105-12-S1-S52},
file = {:home/gus/Documents/Mendeley Desktop//Yang, Aluru, Dorman\_2011\_Repeat-aware modeling and correction of short read errors.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,Genomics,Genomics: methods,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Likelihood Functions,Models,Software,Statistical},
month = jan,
pages = {S52},
pmid = {21342585},
title = {{Repeat-aware modeling and correction of short read errors.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3044310\&tool=pmcentrez\&rendertype=abstract},
volume = {12 Suppl 1},
year = {2011}
}
@article{Roberts12122004,
abstract = {Motivation: Comparison of nucleic acid and protein sequences is a fundamental tool of modern bioinformatics. A dominant method of such string matching is the seed-and-extend approach, in which occurrences of short subsequences called seeds are used to search for potentially longer matches in a large database of sequences. Each such potential match is then checked to see if it extends beyond the seed. To be effective, the seed-and-extend approach needs to catalogue seeds from virtually every substring in the database of search strings. Projects such as mammalian genome assemblies and large-scale protein matching, however, have such large sequence databases that the resulting list of seeds cannot be stored in RAM on a single computer. This significantly slows the matching process.Results: We present a simple and elegant method in which only a small fraction of seeds, called minimizers, needs to be stored. Using minimizers can speed up string-matching computations by a large factor while missing only a small fraction of the matches found using all seeds.},
author = {Roberts, Michael and Hayes, Wayne and Hunt, Brian R and Mount, Stephen M and Yorke, James A},
doi = {10.1093/bioinformatics/bth408},
file = {:home/gus/Documents/Mendeley Desktop/Roberts et al.\_2004\_Reducing storage requirements for biological sequence comparison(2).pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
keywords = {Algorithms,Computer-Assisted,Databases,Genetic,Information Storage and Retrieval,Information Storage and Retrieval: methods,Numerical Analysis,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Sequence Analysis: methods},
month = dec,
number = {18},
pages = {3363--3369},
pmid = {15256412},
title = {{Reducing storage requirements for biological sequence comparison}},
url = {http://bioinformatics.oxfordjournals.org/content/20/18/3363.short},
volume = {20},
year = {2004}
}
@article{Zhao2011,
author = {Zhao, Z and Yin, J and Li, Y},
file = {:home/gus/Documents/Mendeley Desktop/Zhao, Yin, Li\_2011\_An efficient hybrid approach to correcting errors in short reads.pdf:pdf},
journal = {Modeling Decision for Artificial \ldots},
title = {{An efficient hybrid approach to correcting errors in short reads}},
url = {http://www.springerlink.com/index/P53757J416772662.pdf},
year = {2011}
}
@article{Scally2012,
author = {Scally, Aylwyn and Dutheil, Julien Y. and Hillier, LaDeana W. and Jordan, Gregory E. and Goodhead, Ian and Herrero, Javier and Hobolth, Asger and Lappalainen, Tuuli and Mailund, Thomas and Marques-Bonet, Tomas and McCarthy, Shane and Montgomery, Stephen H. and Schwalie, Petra C. and Tang, Y. Amy and Ward, Michelle C. and Xue, Yali and Yngvadottir, Bryndis and Alkan, Can and Andersen, Lars N. and Ayub, Qasim and Ball, Edward V. and Beal, Kathryn and Bradley, Brenda J. and Chen, Yuan and Clee, Chris M. and Fitzgerald, Stephen and Graves, Tina A. and Gu, Yong and Heath, Paul and Heger, Andreas and Karakoc, Emre and Kolb-Kokocinski, Anja and Laird, Gavin K. and Lunter, Gerton and Meader, Stephen and Mort, Matthew and Mullikin, James C. and Munch, Kasper and OConnor, Timothy D. and Phillips, Andrew D. and Prado-Martinez, Javier and Rogers, Anthony S. and Sajjadian, Saba and Schmidt, Dominic and Shaw, Katy and Simpson, Jared T. and Stenson, Peter D. and Turner, Daniel J. and Vigilant, Linda and Vilella, Albert J. and Whitener, Weldon and Zhu, Baoli and Cooper, David N. and de Jong, Pieter and Dermitzakis, Emmanouil T. and Eichler, Evan E. and Flicek, Paul and Goldman, Nick and Mundy, Nicholas I. and Ning, Zemin and Odom, Duncan T. and Ponting, Chris P. and Quail, Michael A. and Ryder, Oliver A. and Searle, Stephen M. and Warren, Wesley C. and Wilson, Richard K. and Schierup, Mikkel H. and Rogers, Jane and Tyler-Smith, Chris and Durbin, Richard},
doi = {10.1038/nature10842},
file = {:home/gus/Documents/Mendeley Desktop/Scally et al.\_2012\_Insights into hominid evolution from the gorilla genome sequence.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
month = mar,
number = {7388},
pages = {169--175},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nature},
title = {{Insights into hominid evolution from the gorilla genome sequence}},
url = {http://dx.doi.org/10.1038/nature10842},
volume = {483},
year = {2012}
}
@article{Pevzner:2001uq,
abstract = {For the last 20 years, fragment assembly in DNA sequencing followed the "overlap-layout-consensus" paradigm that is used in all currently available assembly tools. Although this approach proved useful in assembling clones, it faces difficulties in genomic shotgun assembly. We abandon the classical "overlap-layout-consensus" approach in favor of a new euler algorithm that, for the first time, resolves the 20-year-old "repeat problem" in fragment assembly. Our main result is the reduction of the fragment assembly to a variation of the classical Eulerian path problem that allows one to generate accurate solutions of large-scale sequencing problems. euler, in contrast to the celera assembler, does not mask such repeats but uses them instead as a powerful fragment assembly tool.},
author = {Pevzner, P A and Tang, H and Waterman, M S},
doi = {10.1073/pnas.171285098},
file = {:home/gus/Documents/Mendeley Desktop/Pevzner, Tang, Waterman\_2001\_An Eulerian path approach to DNA fragment assembly.pdf:pdf},
journal = {Proc Natl Acad Sci U S A},
month = aug,
number = {17},
pages = {9748--9753},
pmid = {11504945},
title = {{An Eulerian path approach to DNA fragment assembly}},
volume = {98},
year = {2001}
}
@article{nagarajan2009parametric,
author = {Nagarajan, N. and Pop, M.},
file = {:home/gus/Documents/Mendeley Desktop/Nagarajan, Pop\_2009\_Parametric complexity of sequence assembly theory and applications to next generation sequencing.pdf:pdf},
journal = {Journal of computational biology},
number = {7},
pages = {897--908},
publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
title = {{Parametric complexity of sequence assembly: theory and applications to next generation sequencing}},
url = {http://www.liebertonline.com/doi/abs/10.1089/cmb.2009.0005},
volume = {16},
year = {2009}
}
@inproceedings{Fomitchev2004,
address = {New York, New York, USA},
annote = {http://drdobbs.com/parallel/208801371},
author = {Fomitchev, Mikhail and Ruppert, Eric},
booktitle = {Proceedings of the twenty-third annual ACM symposium on Principles of distributed computing - PODC '04},
doi = {10.1145/1011767.1011776},
file = {:home/gus/Documents/Mendeley Desktop/Fomitchev, Ruppert\_2004\_Lock-free linked lists and skip lists.pdf:pdf},
isbn = {1581138024},
keywords = {amortized analysis,analysis,distributed,efficient,fault-tolerant,linked list,lock-free,skip list},
month = jul,
pages = {50},
publisher = {ACM Press},
title = {{Lock-free linked lists and skip lists}},
url = {http://dl.acm.org/citation.cfm?id=1011767.1011776},
year = {2004}
}
@article{Trapnell2009,
abstract = {A new protocol for sequencing the messenger RNA in a cell, known as RNA-Seq, generates millions of short sequence fragments in a single run. These fragments, or 'reads', can be used to measure levels of gene expression and to identify novel splice variants of genes. However, current software for aligning RNA-Seq data to a genome relies on known splice junctions and cannot identify novel ones. TopHat is an efficient read-mapping algorithm designed to align reads from an RNA-Seq experiment to a reference genome without relying on known splice sites.},
author = {Trapnell, Cole and Pachter, Lior and Salzberg, Steven L},
doi = {10.1093/bioinformatics/btp120},
file = {:home/gus/Documents/Mendeley Desktop/Trapnell, Pachter, Salzberg\_2009\_TopHat discovering splice junctions with RNA-Seq(2).pdf:pdf;:home/gus/Documents/Mendeley Desktop/Trapnell, Pachter, Salzberg\_2009\_TopHat discovering splice junctions with RNA-Seq.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Gene Expression Profiling,Gene Expression Profiling: methods,Genetic,Messenger,Models,RNA,RNA Splicing,RNA Splicing: genetics,Sequence Alignment,Sequence Analysis,Software},
month = may,
number = {9},
pages = {1105--11},
pmid = {19289445},
title = {{TopHat: discovering splice junctions with RNA-Seq.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/25/9/1105},
volume = {25},
year = {2009}
}
@article{Gnerre:2011zr,
abstract = {Massively parallel DNA sequencing technologies are revolutionizing genomics by making it possible to generate billions of relatively short (\~{}100-base) sequence reads at very low cost. Whereas such data can be readily used for a wide range of biomedical applications, it has proven difficult to use them to generate high-quality de novo genome assemblies of large, repeat-rich vertebrate genomes. To date, the genome assemblies generated from such data have fallen far short of those obtained with the older (but much more expensive) capillary-based sequencing approach. Here, we report the development of an algorithm for genome assembly, ALLPATHS-LG, and its application to massively parallel DNA sequence data from the human and mouse genomes, generated on the Illumina platform. The resulting draft genome assemblies have good accuracy, short-range contiguity, long-range connectivity, and coverage of the genome. In particular, the base accuracy is high (99.95\%) and the scaffold sizes (N50 size = 11.5 Mb for human and 7.2 Mb for mouse) approach those obtained with capillary-based sequencing. The combination of improved sequencing technology and improved computational methods should now make it possible to increase dramatically the de novo sequencing of large genomes. The ALLPATHS-LG program is available at http://www.broadinstitute.org/science/programs/genome-biology/crd.},
annote = {
        From Duplicate 2 ( 
        
        
          High-quality draft assemblies of mammalian genomes from massively parallel sequence data
        
        
         - Gnerre, Sante; Maccallum, Iain; Przybylski, Dariusz; Ribeiro, Filipe J; Burton, Joshua N; Walker, Bruce J; Sharpe, Ted; Hall, Giles; Shea, Terrance P; Sykes, Sean; Berlin, Aaron M; Aird, Daniel; Costello, Maura; Daza, Riza; Williams, Louise; Nicol, Robert; Gnirke, Andreas; Nusbaum, Chad; Lander, Eric S; Jaffe, David B )

        
        

        

        

      },
author = {Gnerre, Sante and Maccallum, Iain and Przybylski, Dariusz and Ribeiro, Filipe J and Burton, Joshua N and Walker, Bruce J and Sharpe, Ted and Hall, Giles and Shea, Terrance P and Sykes, Sean and Berlin, Aaron M and Aird, Daniel and Costello, Maura and Daza, Riza and Williams, Louise and Nicol, Robert and Gnirke, Andreas and Nusbaum, Chad and Lander, Eric S and Jaffe, David B},
doi = {10.1073/pnas.1017351108},
file = {:home/gus/Documents/Mendeley Desktop/Gnerre et al.\_2011\_High-quality draft assemblies of mammalian genomes from massively parallel sequence data.pdf:pdf;::},
issn = {1091-6490},
journal = {Proc Natl Acad Sci U S A},
keywords = {Algorithms,Animals,DNA,DNA: methods,Genome,Genome: genetics,Genomics,Genomics: methods,Humans,Internet,Mice,Reproducibility of Results,Sequence Analysis,Software},
month = jan,
number = {4},
pages = {1513--1518},
pmid = {21187386},
title = {{High-quality draft assemblies of mammalian genomes from massively parallel sequence data}},
url = {http://www.pnas.org/cgi/content/abstract/108/4/1513},
volume = {108},
year = {2011}
}
@article{zhao2011psaec,
author = {Zhao, Z. and Yin, J. and Zhan, Y. and Xiong, W. and Li, Y. and Liu, F.},
file = {:home/gus/Documents/Mendeley Desktop/Zhao et al.\_2011\_PSAEC An Improved Algorithm for Short Read Error Correction Using Partial Suffix Arrays.pdf:pdf},
journal = {Frontiers in Algorithmics and Algorithmic Aspects in Information and Management},
pages = {220--232},
publisher = {Springer},
title = {{PSAEC: An Improved Algorithm for Short Read Error Correction Using Partial Suffix Arrays}},
url = {http://www.springerlink.com/index/9L77826718445828.pdf},
year = {2011}
}
@article{Richter2008,
abstract = {The new research field of metagenomics is providing exciting insights into various, previously unclassified ecological systems. Next-generation sequencing technologies are producing a rapid increase of environmental data in public databases. There is great need for specialized software solutions and statistical methods for dealing with complex metagenome data sets.},
author = {Richter, Daniel C and Ott, Felix and Auch, Alexander F and Schmid, Ramona and Huson, Daniel H},
doi = {10.1371/journal.pone.0003373},
editor = {Field, Dawn},
file = {:home/gus/Documents/Mendeley Desktop/Richter et al.\_2008\_MetaSim a sequencing simulator for genomics and metagenomics.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Databases,Genetic,Genomics,Models,Theoretical,User-Computer Interface},
month = jan,
number = {10},
pages = {e3373},
pmid = {18841204},
publisher = {Public Library of Science},
title = {{MetaSim: a sequencing simulator for genomics and metagenomics.}},
url = {http://dx.plos.org/10.1371/journal.pone.0003373},
volume = {3},
year = {2008}
}
@article{Farach1997,
author = {Farach, M.},
file = {:home/gus/Documents/Mendeley Desktop/Farach\_1997\_Optimal suffix tree construction with large alphabets.pdf:pdf},
isbn = {0-8186-8197-7},
keywords = {combinatorial pattern matching,data structure,integer alphabet,integer alphabets,large alphabets,pattern matching,sorting,suffix tree},
month = oct,
pages = {137},
title = {{Optimal suffix tree construction with large alphabets}},
url = {http://dl.acm.org/citation.cfm?id=795663.796326},
year = {1997}
}
@article{yang2010reptile,
author = {Yang, X. and Dorman, K.S. and Aluru, S.},
file = {:home/gus/Documents/Mendeley Desktop/Yang, Dorman, Aluru\_2010\_Reptile representative tiling for short read error correction.pdf:pdf},
journal = {Bioinformatics},
number = {20},
pages = {2526},
publisher = {Oxford Univ Press},
title = {{Reptile: representative tiling for short read error correction}},
url = {http://bioinformatics.oxfordjournals.org/content/26/20/2526.short},
volume = {26},
year = {2010}
}
@article{Chaisson2008,
abstract = {In the last year, high-throughput sequencing technologies have progressed from proof-of-concept to production quality. While these methods produce high-quality reads, they have yet to produce reads comparable in length to Sanger-based sequencing. Current fragment assembly algorithms have been implemented and optimized for mate-paired Sanger-based reads, and thus do not perform well on short reads produced by short read technologies. We present a new Eulerian assembler that generates nearly optimal short read assemblies of bacterial genomes and describe an approach to assemble reads in the case of the popular hybrid protocol when short and long Sanger-based reads are combined.},
author = {Chaisson, Mark J and Pevzner, Pavel A},
doi = {10.1101/gr.7088808},
file = {:home/gus/Documents/Mendeley Desktop/Chaisson, Pevzner\_2008\_Short read fragment assembly of bacterial genomes.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Bacterial,Bacterial: genetics,Contig Mapping,Contig Mapping: methods,DNA,DNA: methods,Genome,Genomics,Genomics: methods,Research Design,Sequence Analysis},
month = feb,
number = {2},
pages = {324--30},
pmid = {18083777},
title = {{Short read fragment assembly of bacterial genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2203630\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@inproceedings{sundell2003fast,
author = {Sundell, H. and Tsigas, P.},
booktitle = {Parallel and Distributed Processing Symposium, 2003. Proceedings. International},
file = {:home/gus/Documents/Mendeley Desktop/Sundell, Tsigas\_2003\_Fast and lock-free concurrent priority queues for multi-thread systems.pdf:pdf},
pages = {11--pp},
publisher = {IEEE},
title = {{Fast and lock-free concurrent priority queues for multi-thread systems}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1213189},
year = {2003}
}
@article{Nakamura2011,
abstract = {We identified the sequence-specific starting positions of consecutive miscalls in the mapping of reads obtained from the Illumina Genome Analyser (GA). Detailed analysis of the miscall pattern indicated that the underlying mechanism involves sequence-specific interference of the base elongation process during sequencing. The two major sequence patterns that trigger this sequence-specific error (SSE) are: (i) inverted repeats and (ii) GGC sequences. We speculate that these sequences favor dephasing by inhibiting single-base elongation, by: (i) folding single-stranded DNA and (ii) altering enzyme preference. This phenomenon is a major cause of sequence coverage variability and of the unfavorable bias observed for population-targeted methods such as RNA-seq and ChIP-seq. Moreover, SSE is a potential cause of false single-nucleotide polymorphism (SNP) calls and also significantly hinders de novo assembly. This article highlights the importance of recognizing SSE and its underlying mechanisms in the hope of enhancing the potential usefulness of the Illumina sequencers.},
author = {Nakamura, Kensuke and Oshima, Taku and Morimoto, Takuya and Ikeda, Shun and Yoshikawa, Hirofumi and Shiwa, Yuh and Ishikawa, Shu and Linak, Margaret C and Hirai, Aki and Takahashi, Hiroki and Altaf-Ul-Amin, Md and Ogasawara, Naotake and Kanaya, Shigehiko},
doi = {10.1093/nar/gkr344},
file = {:home/gus/Documents/Mendeley Desktop/Nakamura et al.\_2011\_Sequence-specific error profile of Illumina sequencers.pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
month = may,
number = {13},
pages = {e90--},
pmid = {21576222},
title = {{Sequence-specific error profile of Illumina sequencers.}},
url = {http://nar.oxfordjournals.org/cgi/content/abstract/39/13/e90},
volume = {39},
year = {2011}
}
@article{Yang2011,
abstract = {High-throughput short read sequencing is revolutionizing genomics and systems biology research by enabling cost-effective deep coverage sequencing of genomes and transcriptomes. Error detection and correction are crucial to many short read sequencing applications including de novo genome sequencing, genome resequencing, and digital gene expression analysis. Short read error detection is typically carried out by counting the observed frequencies of kmers in reads and validating those with frequencies exceeding a threshold. In case of genomes with high repeat content, an erroneous kmer may be frequently observed if it has few nucleotide differences with valid kmers with multiple occurrences in the genome. Error detection and correction were mostly applied to genomes with low repeat content and this remains a challenging problem for genomes with high repeat content.},
author = {Yang, Xiao and Aluru, Srinivas and Dorman, Karin S},
doi = {10.1186/1471-2105-12-S1-S52},
file = {:home/gus/Documents/Mendeley Desktop//Yang, Aluru, Dorman\_2011\_Repeat-aware modeling and correction of short read errors.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,Genomics,Genomics: methods,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Likelihood Functions,Models,Software,Statistical},
month = jan,
number = {Suppl 1},
pages = {S52},
pmid = {21342585},
title = {{Repeat-aware modeling and correction of short read errors.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3044310\&tool=pmcentrez\&rendertype=abstract http://www.biomedcentral.com/1471-2105/12/S1/S52},
volume = {12 Suppl 1},
year = {2011}
}
@article{Yang2011,
abstract = {High-throughput short read sequencing is revolutionizing genomics and systems biology research by enabling cost-effective deep coverage sequencing of genomes and transcriptomes. Error detection and correction are crucial to many short read sequencing applications including de novo genome sequencing, genome resequencing, and digital gene expression analysis. Short read error detection is typically carried out by counting the observed frequencies of kmers in reads and validating those with frequencies exceeding a threshold. In case of genomes with high repeat content, an erroneous kmer may be frequently observed if it has few nucleotide differences with valid kmers with multiple occurrences in the genome. Error detection and correction were mostly applied to genomes with low repeat content and this remains a challenging problem for genomes with high repeat content.},
author = {Yang, Xiao and Aluru, Srinivas and Dorman, Karin S},
doi = {10.1186/1471-2105-12-S1-S52},
file = {:home/gus/Documents/Mendeley Desktop//Yang, Aluru, Dorman\_2011\_Repeat-aware modeling and correction of short read errors.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,Genomics,Genomics: methods,High-Throughput Nucleotide Sequencing,High-Throughput Nucleotide Sequencing: methods,Likelihood Functions,Models,Software,Statistical},
month = jan,
number = {Suppl 1},
pages = {S52},
pmid = {21342585},
title = {{Repeat-aware modeling and correction of short read errors.}},
url = {http://www.biomedcentral.com/1471-2105/12/S1/S52},
volume = {12 Suppl 1},
year = {2011}
}
@article{Salzberg2011,
abstract = {New sequencing technology has dramatically altered the landscape of whole-genome sequencing, allowing scientists to initiate numerous projects to decode the genomes of previously unsequenced organisms. The lowest-cost technology can generate deep coverage of most species, including mammals, in just a few days. The sequence data generated by one of these projects consists of millions or billions of short DNA sequences (reads) that range from 50-150 nucleotides in length. These sequences must then be assembled de novo before most genome analyses can begin. Unfortunately, genome assembly remains a very difficult problem, made more difficult by shorter reads and unreliable long-range linking information. In this study, we evaluated several of the leading de novo assembly algorithms on four different short-read data sets, all generated by Illumina sequencers. Our results describe the relative performance of the different assemblers as well as other significant differences in assembly difficulty that appear to be inherent in the genomes themselves. Three overarching conclusions are apparent: first, that data quality, rather than the assembler itself, has a dramatic affect on the quality of an assembled genome; second, that the degree of contiguity of an assembly varies enormously among different assemblers and different genomes; and third, that the correctness of an assembly also varies widely, and is not well correlated with statistics on contiguity. In order to enable others to replicate our results, all of our data and methods are freely available, as are all assemblers used in this study.},
author = {Salzberg, S. L. and Phillippy, A. M. and Zimin, A. V. and Puiu, D. and Magoc, T. and Koren, S. and Treangen, T. and Schatz, M. C. and Delcher, A. L. and Roberts, M. and Marcais, G. and Pop, M. and Yorke, J. A.},
doi = {10.1101/gr.131383.111},
file = {:home/gus/Documents/Mendeley Desktop/Salzberg et al.\_2011\_GAGE A critical evaluation of genome assemblies and assembly algorithms.pdf:pdf},
issn = {1088-9051},
journal = {Genome Research},
month = dec,
pages = {gr.131383.111--},
title = {{GAGE: A critical evaluation of genome assemblies and assembly algorithms}},
url = {http://genome.cshlp.org/cgi/content/abstract/gr.131383.111v1},
year = {2011}
}
@article{Ferragina2000,
author = {Ferragina, P. and Manzini, G.},
file = {:home/gus/Documents/Mendeley Desktop/Ferragina, Manzini\_2000\_Opportunistic data structures with applications.pdf:pdf},
isbn = {0-7695-0850-2},
keywords = {Glimpse tool,computational complexity,data compression,data indexing,data set,data structures,database indexing,database theory,entropy,opportunistic data structures,query performance,search,sublinear query time complexity,sublinear space complexity,succinct suffix array,suffix array data structures,suffix tree data structures},
month = nov,
pages = {390},
title = {{Opportunistic data structures with applications}},
url = {http://dl.acm.org/citation.cfm?id=795666.796543 http://www.di.unipi.it/~ferragin/Libraries/fmindexV2/index.html},
year = {2000}
}
@article{Ilie2011b,
abstract = {High-throughput sequencing technologies produce very large amounts of data and sequencing errors constitute one of the major problems in analyzing such data. Current algorithms for correcting these errors are not very accurate and do not automatically adapt to the given data.},
author = {Ilie, Lucian and Fazayeli, Farideh and Ilie, Silvana},
file = {:home/gus/Documents/Mendeley Desktop/Ilie, Fazayeli, Ilie\_2011\_HiTEC accurate error correction in high-throughput sequencing data.pdf:pdf},
institution = {Department of Computer Science, University of Western Ontario, London, ON N6A 5B7, Canada. ilie@csd.uwo.ca},
journal = {Bioinformatics},
number = {3},
pages = {295--302},
pmid = {21115437},
title = {{HiTEC: accurate error correction in high-throughput sequencing data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21115437 http://bioinformatics.oxfordjournals.org/content/27/3/295.short},
volume = {27},
year = {2011}
}
@article{Langmead2009,
abstract = {Bowtie is an ultrafast, memory-efficient alignment program for aligning short DNA sequence reads to large genomes. For the human genome, Burrows-Wheeler indexing allows Bowtie to align more than 25 million reads per CPU hour with a memory footprint of approximately 1.3 gigabytes. Bowtie extends previous Burrows-Wheeler techniques with a novel quality-aware backtracking algorithm that permits mismatches. Multiple processor cores can be used simultaneously to achieve even greater alignment speeds. Bowtie is open source (http://bowtie.cbcb.umd.edu).},
author = {Langmead, Ben and Trapnell, Cole and Pop, Mihai and Salzberg, Steven L},
doi = {10.1186/gb-2009-10-3-r25},
file = {:home/gus/Documents/Mendeley Desktop/Langmead et al.\_2009\_Ultrafast and memory-efficient alignment of short DNA sequences to the human genome.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Algorithms,Base Sequence,Genome,Human,Human: genetics,Humans,Sequence Alignment,Sequence Alignment: methods},
month = jan,
number = {3},
pages = {R25},
pmid = {19261174},
title = {{Ultrafast and memory-efficient alignment of short DNA sequences to the human genome.}},
url = {http://genomebiology.com/2009/10/3/R25},
volume = {10},
year = {2009}
}
@article{Venter2001,
author = {Venter, JC and Adams, MD and Myers, EW},
file = {:home/gus/Documents/Mendeley Desktop/Venter, Adams, Myers\_2001\_The sequence of the human genome.pdf:pdf},
journal = {Science's \ldots},
title = {{The sequence of the human genome}},
url = {http://stke.sciencemag.org/cgi/content/abstract/sci;291/5507/1304},
year = {2001}
}
@article{Butler:2008kx,
abstract = {New DNA sequencing technologies deliver data at dramatically lower costs but demand new analytical methods to take full advantage of the very short reads that they produce. We provide an initial, theoretical solution to the challenge of de novo assembly from whole-genome shotgun "microreads." For 11 genomes of sizes up to 39 Mb, we generated high-quality assemblies from 80x coverage by paired 30-base simulated reads modeled after real Illumina-Solexa reads. The bacterial genomes of Campylobacter jejuni and Escherichia coli assemble optimally, yielding single perfect contigs, and larger genomes yield assemblies that are highly connected and accurate. Assemblies are presented in a graph form that retains intrinsic ambiguities such as those arising from polymorphism, thereby providing information that has been absent from previous genome assemblies. For both C. jejuni and E. coli, this assembly graph is a single edge encompassing the entire genome. Larger genomes produce more complicated graphs, but the vast majority of the bases in their assemblies are present in long edges that are nearly always perfect. We describe a general method for genome assembly that can be applied to all types of DNA sequence data, not only short read data, but also conventional sequence reads.},
author = {Butler, Jonathan and MacCallum, Iain and Kleber, Michael and Shlyakhter, Ilya a and Belmonte, Matthew K and Lander, Eric S and Nusbaum, Chad and Jaffe, David B},
doi = {10.1101/gr.7337908},
file = {:home/gus/Documents/Mendeley Desktop/Butler et al.\_2008\_ALLPATHS de novo assembly of whole-genome shotgun microreads.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Bacterial,Bacterial: genetics,Campylobacter jejuni,Campylobacter jejuni: genetics,Computational Biology,Computational Biology: methods,Computer Simulation,DNA,DNA: methods,DNA: standards,Escherichia coli,Escherichia coli: genetics,Genome,Reproducibility of Results,Sequence Analysis},
month = may,
number = {5},
pages = {810--20},
pmid = {18340039},
title = {{ALLPATHS: de novo assembly of whole-genome shotgun microreads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2336810\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@article{medvedev2007computability,
author = {Medvedev, P. and Georgiou, K. and Myers, G. and Brudno, M.},
file = {:home/gus/Documents/Mendeley Desktop/Medvedev et al.\_2007\_Computability of models for sequence assembly.pdf:pdf},
journal = {Algorithms in Bioinformatics},
pages = {289--301},
publisher = {Springer},
title = {{Computability of models for sequence assembly}},
url = {http://www.springerlink.com/index/H711368771048H21.pdf},
year = {2007}
}
@article{Manber1990,
annote = {        From Duplicate 1 (                           Suffix arrays: a new method for on-line string searches                         - Manber, Udi; Myers, Gene )
                
        
        
      },
author = {Manber, Udi and Myers, Gene},
file = {:home/gus/Documents/Mendeley Desktop/Manber, Myers\_1990\_Suffix arrays a new method for on-line string searches(2).pdf:pdf;:home/gus/Documents/Mendeley Desktop/Manber, Myers\_1990\_Suffix arrays a new method for on-line string searches.pdf:pdf},
isbn = {0-89871-251-3},
journal = {Proceedings of the first annual ACM-SIAM \ldots},
month = jan,
pages = {319--327},
title = {{Suffix arrays: a new method for on-line string searches}},
url = {http://portal.acm.org/citation.cfm?id=320218 http://dl.acm.org/citation.cfm?id=320176.320218},
year = {1990}
}
@article{herlihy2002repeat,
author = {Herlihy, M. and Luchangco, V. and Moir, M.},
file = {:home/gus/Documents/Mendeley Desktop/Herlihy, Luchangco, Moir\_2002\_The repeat offender problem A mechanism for supporting dynamic-sized lock-free data structures.pdf:pdf},
publisher = {Sun Microsystems, Inc.},
title = {{The repeat offender problem: A mechanism for supporting dynamic-sized lock-free data structures}},
url = {http://portal.acm.org/citation.cfm?id=1698158},
year = {2002}
}
@article{Zerbino2009a,
abstract = {Despite the short length of their reads, micro-read sequencing technologies have shown their usefulness for de novo sequencing. However, especially in eukaryotic genomes, complex repeat patterns are an obstacle to large assemblies.},
author = {Zerbino, Daniel R DR and McEwen, GK Gayle K and Margulies, Elliott H and Birney, Ewan},
doi = {10.1371/journal.pone.0008407},
file = {:home/gus/Documents/Mendeley Desktop/Zerbino, McEwen\_2009\_Pebble and rock band heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Algorithms,Animals,Artificial,Bacterial,Bacterial: genetics,Chromosomes,Computer Simulation,DNA,DNA: instrumentation,DNA: methods,Ferrets,Ferrets: genetics,Nucleic Acid,Nucleic Acid: genetics,Pseudomonas syringae,Pseudomonas syringae: genetics,Repetitive Sequences,Sequence Analysis},
month = jan,
number = {12},
pages = {e8407},
pmid = {20027311},
title = {{Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2793427\&tool=pmcentrez\&rendertype=abstract http://dx.plos.org/10.1371/journal.pone.0008407},
volume = {4},
year = {2009}
}
@article{Simpson2009,
abstract = {Widespread adoption of massively parallel deoxyribonucleic acid (DNA) sequencing instruments has prompted the recent development of de novo short read assembly algorithms. A common shortcoming of the available tools is their inability to efficiently assemble vast amounts of data generated from large-scale sequencing projects, such as the sequencing of individual human genomes to catalog natural genetic variation. To address this limitation, we developed ABySS (Assembly By Short Sequences), a parallelized sequence assembler. As a demonstration of the capability of our software, we assembled 3.5 billion paired-end reads from the genome of an African male publicly released by Illumina, Inc. Approximately 2.76 million contigs > or =100 base pairs (bp) in length were created with an N50 size of 1499 bp, representing 68\% of the reference human genome. Analysis of these contigs identified polymorphic and novel sequences not present in the human reference assembly, which were validated by alignment to alternate human assemblies and to other primate genomes.},
author = {Simpson, Jared T and Wong, Kim and Jackman, Shaun D and Schein, Jacqueline E and Jones, Steven J M and Birol, Inan\c{c}},
doi = {10.1101/gr.089532.108},
file = {:home/gus/Documents/Mendeley Desktop/Simpson et al.\_2009\_ABySS a parallel assembler for short read sequence data.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Animals,Computational Biology,Computational Biology: methods,Contig Mapping,Escherichia coli K12,Escherichia coli K12: genetics,Genetic Variation,Genome, Human,Humans,Polymorphism, Genetic,Reproducibility of Results,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
month = jun,
number = {6},
pages = {1117--23},
pmid = {19251739},
title = {{ABySS: a parallel assembler for short read sequence data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2694472\&tool=pmcentrez\&rendertype=abstract},
volume = {19},
year = {2009}
}
@article{Maccallum:2009vn,
abstract = {We demonstrate that genome sequences approaching finished quality can be generated from short paired reads. Using 36 base (fragment) and 26 base (jumping) reads from five microbial genomes of varied GC composition and sizes up to 40 Mb, ALLPATHS2 generated assemblies with long, accurate contigs and scaffolds. Velvet and EULER-SR were less accurate. For example, for Escherichia coli, the fraction of 10-kb stretches that were perfect was 99.8\% (ALLPATHS2), 68.7\% (Velvet), and 42.1\% (EULER-SR).},
author = {Maccallum, Iain and Przybylski, Dariusz and Gnerre, Sante and Burton, Joshua and Shlyakhter, Ilya and Gnirke, Andreas and Malek, Joel and McKernan, Kevin and Ranade, Swati and Shea, Terrance P and Williams, Louise and Young, Sarah and Nusbaum, Chad and Jaffe, David B},
doi = {10.1186/gb-2009-10-10-r103},
file = {:home/gus/Documents/Mendeley Desktop//Maccallum et al.\_2009\_ALLPATHS 2 small genomes assembled accurately and with high continuity from short paired reads.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Bacteria,Bacteria: genetics,Base Pairing,Base Pairing: genetics,Fungi,Fungi: genetics,Genome,Genome: genetics,Genomics,Genomics: methods,Reproducibility of Results,Software},
month = jan,
number = {10},
pages = {R103},
pmid = {19796385},
title = {{ALLPATHS 2: small genomes assembled accurately and with high continuity from short paired reads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2784318\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Chor2009,
abstract = {The empirical frequencies of DNA k-mers in whole genome sequences provide an interesting perspective on genomic complexity, and the availability of large segments of genomic sequence from many organisms means that analysis of k-mers with non-trivial lengths is now possible.},
author = {Chor, Benny and Horn, David and Goldman, Nick and Levy, Yaron and Massingham, Tim},
doi = {10.1186/gb-2009-10-10-r108},
file = {:home/gus/Documents/Mendeley Desktop/Chor et al.\_2009\_Genomic DNA k-mer spectra models and modalities.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Animals,Base Composition,Base Composition: genetics,Chickens,Chickens: genetics,Computer Simulation,CpG Islands,CpG Islands: genetics,DNA,DNA: genetics,Genome,Genome: genetics,Humans,Markov Chains,Models, Genetic,Zebrafish,Zebrafish: genetics},
month = jan,
number = {10},
pages = {R108},
pmid = {19814784},
title = {{Genomic DNA k-mer spectra: models and modalities.}},
url = {http://genomebiology.com/2009/10/10/R108},
volume = {10},
year = {2009}
}
@techreport{Drepper2007,
author = {Drepper, Ulrich},
file = {:home/gus/Documents/Mendeley Desktop/Drepper\_2007\_What Every Programmer Should Know About Memory.pdf:pdf},
keywords = {Cache,Memory,lock free},
title = {{What Every Programmer Should Know About Memory}},
year = {2007}
}
@article{Lander1988,
abstract = {Results from physical mapping projects have recently been reported for the genomes of Escherichia coli, Saccharomyces cerevisiae, and Caenorhabditis elegans, and similar projects are currently being planned for other organisms. In such projects, the physical map is assembled by first "fingerprinting" a large number of clones chosen at random from a recombinant library and then inferring overlaps between clones with sufficiently similar fingerprints. Although the basic approach is the same, there are many possible choices for the fingerprint used to characterize the clones and the rules for declaring overlap. In this paper, we derive simple formulas showing how the progress of a physical mapping project is affected by the nature of the fingerprinting scheme. Using these formulas, we discuss the analytic considerations involved in selecting an appropriate fingerprinting scheme for a particular project.},
author = {Lander, E S and Waterman, M S},
file = {:home/gus/Documents/Mendeley Desktop/Lander, Waterman\_1988\_Genomic mapping by fingerprinting random clones a mathematical analysis.pdf:pdf},
issn = {0888-7543},
journal = {Genomics},
keywords = {Animals,Caenorhabditis,Caenorhabditis: genetics,Chromosome Mapping,Cloning,Escherichia coli,Escherichia coli: genetics,Mathematics,Molecular,Nucleotide Mapping,Saccharomyces cerevisiae,Saccharomyces cerevisiae: genetics},
month = apr,
number = {3},
pages = {231--9},
pmid = {3294162},
title = {{Genomic mapping by fingerprinting random clones: a mathematical analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3294162},
volume = {2},
year = {1988}
}
@article{Purcell2005,
abstract = {We present the first non-blocking hashtable based on open addressing that provides the following benefits: it combines good cache locality, accessing a single cacheline if there are no collisions, with short straight-line code; it needs no storage overhead for pointers and memory allocator schemes, having instead an overhead of two words per bucket; it does not need to periodically reorganise or replicate the table; and it does not need garbage collection, even with arbitrary-sized keys. Open problems include resizing the table and replacing, rather than erasing, entries. The result is a highly-concurrent set algorithm that approaches or outperforms the best externally- chained implementations we tested, with fixed memory costs and no need to select or fine-tune a garbage collector or locking strategy.},
author = {Purcell, Chris and Harris, Tim},
file = {:home/gus/Documents/Mendeley Desktop/Purcell, Harris\_2005\_Non-blocking hatables with open addressing.pdf:pdf},
journal = {University of Cambridge Technical Report},
keywords = {hash table,lock free},
number = {639},
title = {{Non-blocking hatables with open addressing}},
year = {2005}
}
@article{Zerbino2008,
abstract = {We have developed a new set of algorithms, collectively called "Velvet," to manipulate de Bruijn graphs for genomic sequence assembly. A de Bruijn graph is a compact representation based on short words (k-mers) that is ideal for high coverage, very short read (25-50 bp) data sets. Applying Velvet to very short reads and paired-ends information only, one can produce contigs of significant length, up to 50-kb N50 length in simulations of prokaryotic data and 3-kb N50 on simulated mammalian BACs. When applied to real Solexa data sets without read pairs, Velvet generated contigs of approximately 8 kb in a prokaryote and 2 kb in a mammalian BAC, in close agreement with our simulated results without read-pair information. Velvet represents a new approach to assembly that can leverage very short reads in combination with read pairs to produce useful assemblies.},
author = {Zerbino, Daniel R and Birney, Ewan},
doi = {10.1101/gr.074492.107},
file = {:home/gus/Documents/Mendeley Desktop/Zerbino, Birney\_2008\_Velvet algorithms for de novo short read assembly using de Bruijn graphs.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Animals,Artificial,Bacterial,Chromosomes,Computational Biology,Computational Biology: methods,Computer Simulation,DNA,DNA: methods,DNA: standards,Genome,Genomics,Human,Humans,Mammals,Mammals: genetics,Sequence Analysis,Streptococcus,Streptococcus: genetics},
month = may,
number = {5},
pages = {821--9},
pmid = {18349386},
title = {{Velvet: algorithms for de novo short read assembly using de Bruijn graphs}},
url = {http://genome.cshlp.org/content/18/5/821.short http://genome.cshlp.org/cgi/content/abstract/18/5/821},
volume = {18},
year = {2008}
}
@article{Salzberg2011,
abstract = {New sequencing technology has dramatically altered the landscape of whole-genome sequencing, allowing scientists to initiate numerous projects to decode the genomes of previously unsequenced organisms. The lowest-cost technology can generate deep coverage of most species, including mammals, in just a few days. The sequence data generated by one of these projects consists of millions or billions of short DNA sequences (reads) that range from 50-150 nucleotides in length. These sequences must then be assembled de novo before most genome analyses can begin. Unfortunately, genome assembly remains a very difficult problem, made more difficult by shorter reads and unreliable long-range linking information. In this study, we evaluated several of the leading de novo assembly algorithms on four different short-read data sets, all generated by Illumina sequencers. Our results describe the relative performance of the different assemblers as well as other significant differences in assembly difficulty that appear to be inherent in the genomes themselves. Three overarching conclusions are apparent: first, that data quality, rather than the assembler itself, has a dramatic affect on the quality of an assembled genome; second, that the degree of contiguity of an assembly varies enormously among different assemblers and different genomes; and third, that the correctness of an assembly also varies widely, and is not well correlated with statistics on contiguity. In order to enable others to replicate our results, all of our data and methods are freely available, as are all assemblers used in this study.},
annote = {
        From Duplicate 2 ( 
        
        
          GAGE: A critical evaluation of genome assemblies and assembly algorithms
        
        
         - Salzberg, S. L.; Phillippy, A. M.; Zimin, A. V.; Puiu, D.; Magoc, T.; Koren, S.; Treangen, T.; Schatz, M. C.; Delcher, A. L.; Roberts, M.; Marcais, G.; Pop, M.; Yorke, J. A. )

        
        

        

        

      },
author = {Salzberg, S. L. and Phillippy, A. M. and Zimin, A. V. and Puiu, D. and Magoc, T. and Koren, S. and Treangen, T. and Schatz, M. C. and Delcher, A. L. and Roberts, M. and Marcais, G. and Pop, M. and Yorke, J. A.},
doi = {10.1101/gr.131383.111},
file = {:home/gus/Documents/Mendeley Desktop/Salzberg et al.\_2011\_GAGE A critical evaluation of genome assemblies and assembly algorithms.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Salzberg et al.\_2011\_GAGE A critical evaluation of genome assemblies and assembly algorithms(2).pdf:pdf},
issn = {1088-9051},
journal = {Genome Research},
month = dec,
pages = {gr.131383.111--},
title = {{GAGE: A critical evaluation of genome assemblies and assembly algorithms}},
url = {http://genome.cshlp.org/cgi/content/abstract/gr.131383.111v1},
year = {2011}
}
@article{Maccallum:2009vn,
abstract = {We demonstrate that genome sequences approaching finished quality can be generated from short paired reads. Using 36 base (fragment) and 26 base (jumping) reads from five microbial genomes of varied GC composition and sizes up to 40 Mb, ALLPATHS2 generated assemblies with long, accurate contigs and scaffolds. Velvet and EULER-SR were less accurate. For example, for Escherichia coli, the fraction of 10-kb stretches that were perfect was 99.8\% (ALLPATHS2), 68.7\% (Velvet), and 42.1\% (EULER-SR).},
author = {Maccallum, Iain and Przybylski, Dariusz and Gnerre, Sante and Burton, Joshua and Shlyakhter, Ilya and Gnirke, Andreas and Malek, Joel and McKernan, Kevin and Ranade, Swati and Shea, Terrance P and Williams, Louise and Young, Sarah and Nusbaum, Chad and Jaffe, David B},
doi = {10.1186/gb-2009-10-10-r103},
file = {:home/gus/Documents/Mendeley Desktop//Maccallum et al.\_2009\_ALLPATHS 2 small genomes assembled accurately and with high continuity from short paired reads.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Bacteria,Bacteria: genetics,Base Pairing,Base Pairing: genetics,Fungi,Fungi: genetics,Genome,Genome: genetics,Genomics,Genomics: methods,Reproducibility of Results,Software},
month = jan,
number = {10},
pages = {R103},
pmid = {19796385},
title = {{ALLPATHS 2: small genomes assembled accurately and with high continuity from short paired reads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2784318\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Melsted2011,
abstract = {BACKGROUND:Counting k-mers (substrings of length k in DNA sequence data) is an essential component of many methods in bioinformatics, including for genome and transcriptome assembly, for metagenomic sequencing, and for error correction of sequence reads. Although simple in principle, counting k-mers in large modern sequence data sets can easily overwhelm the memory capacity of standard computers. In current data sets, a large fraction - often more than 50\% - of the storage capacity may be spent on storing k-mers that contain sequencing errors and which are typically observed only a single time in the data. These singleton k-mers are uninformative for many algorithms without some kind of error correction.RESULTS:We present a new method that identifies all the k-mers that occur more than once in a DNA sequence data set. Our method does this using a Bloom filter, a probabilistic data structure that stores all the observed k-mers implicitly in memory with greatly reduced memory requirements. We then make a second sweep through the data to provide exact counts of all nonunique k-mers. For example data sets, we report up to 50\% savings in memory usage compared to current software, with modest costs in computational speed. This approach may reduce memory requirements for any algorithm that starts by counting k-mers in sequence data with errors.CONCLUSIONS:A reference implementation for this methodology, BFCounter, is written in C++ and is GPL licensed. It is available for free download at http://pritch.bsd.uchicago.edu/software.html},
author = {Melsted, Pall and Pritchard, Jonathan K},
doi = {10.1186/1471-2105-12-333},
file = {:home/gus/Documents/Mendeley Desktop/Melsted, Pritchard\_2011\_Efficient counting of k-mers in DNA sequences using a Bloom Filter.pdf:pdf},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {333},
title = {{Efficient counting of k-mers in DNA sequences using a Bloom Filter}},
url = {http://www.biomedcentral.com/1471-2105/12/333},
volume = {12},
year = {2011}
}
@article{Chaisson2008,
abstract = {In the last year, high-throughput sequencing technologies have progressed from proof-of-concept to production quality. While these methods produce high-quality reads, they have yet to produce reads comparable in length to Sanger-based sequencing. Current fragment assembly algorithms have been implemented and optimized for mate-paired Sanger-based reads, and thus do not perform well on short reads produced by short read technologies. We present a new Eulerian assembler that generates nearly optimal short read assemblies of bacterial genomes and describe an approach to assemble reads in the case of the popular hybrid protocol when short and long Sanger-based reads are combined.},
author = {Chaisson, Mark J and Pevzner, Pavel A},
doi = {10.1101/gr.7088808},
file = {:home/gus/Documents/Mendeley Desktop/Chaisson, Pevzner\_2008\_Short read fragment assembly of bacterial genomes.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Bacterial,Bacterial: genetics,Contig Mapping,Contig Mapping: methods,DNA,DNA: methods,Genome,Genomics,Genomics: methods,Research Design,Sequence Analysis},
month = feb,
number = {2},
pages = {324--30},
pmid = {18083777},
title = {{Short read fragment assembly of bacterial genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2203630\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@article{Li01022010,
abstract = {Next-generation massively parallel DNA sequencing technologies provide ultrahigh throughput at a substantially lower unit data cost; however, the data are very short read length sequences, making de novo assembly extremely challenging. Here, we describe a novel method for de novo assembly of large genomes from short read sequences. We successfully assembled both the Asian and African human genome sequences, achieving an N50 contig size of 7.4 and 5.9 kilobases (kb) and scaffold of 446.3 and 61.9 kb, respectively. The development of this de novo short read assembly method creates new opportunities for building reference sequences and carrying out accurate analyses of unexplored genomes in a cost-effective way.},
author = {Li, Ruiqiang and Zhu, Hongmei and Ruan, Jue and Qian, Wubin and Fang, Xiaodong and Shi, Zhongbin and Li, Yingrui and Li, Shengting and Shan, Gao and Kristiansen, Karsten and Li, Songgang and Yang, Huanming and Wang, Jian and Wang, Jun},
doi = {10.1101/gr.097261.109},
file = {:home/gus/Documents/Mendeley Desktop/Li et al.\_2010\_De novo assembly of human genomes with massively parallel short read sequencing.pdf:pdf},
journal = {Genome Research},
number = {2},
pages = {265--272},
title = {{De novo assembly of human genomes with massively parallel short read sequencing}},
url = {http://genome.cshlp.org/content/20/2/265.abstract},
volume = {20},
year = {2010}
}
@article{Schatz:2010uq,
abstract = {Second-generation sequencing technology can now be used to sequence an entire human genome in a matter of days and at low cost. Sequence read lengths, initially very short, have rapidly increased since the technology first appeared, and we now are seeing a growing number of efforts to sequence large genomes de novo from these short reads. In this Perspective, we describe the issues associated with short-read assembly, the different types of data produced by second-gen sequencers, and the latest assembly algorithms designed for these data. We also review the genomes that have been assembled recently from short reads and make recommendations for sequencing strategies that will yield a high-quality assembly.},
author = {Schatz, Michael C and Delcher, Arthur L and Salzberg, Steven L},
doi = {10.1101/gr.101360.109},
file = {:home/gus/Documents/Mendeley Desktop/Schatz, Delcher, Salzberg\_2010\_Assembly of large genomes using second-generation sequencing.pdf:pdf},
issn = {1549-5469},
journal = {Genome research},
keywords = {Algorithms,Base Sequence,DNA,DNA: methods,Genome,Genomics,Genomics: methods,Human,Humans,Sequence Analysis},
month = sep,
number = {9},
pages = {1165--73},
pmid = {20508146},
title = {{Assembly of large genomes using second-generation sequencing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2928494\&tool=pmcentrez\&rendertype=abstract},
volume = {20},
year = {2010}
}
@article{Maccallum:2009vn,
abstract = {We demonstrate that genome sequences approaching finished quality can be generated from short paired reads. Using 36 base (fragment) and 26 base (jumping) reads from five microbial genomes of varied GC composition and sizes up to 40 Mb, ALLPATHS2 generated assemblies with long, accurate contigs and scaffolds. Velvet and EULER-SR were less accurate. For example, for Escherichia coli, the fraction of 10-kb stretches that were perfect was 99.8\% (ALLPATHS2), 68.7\% (Velvet), and 42.1\% (EULER-SR).},
author = {Maccallum, Iain and Przybylski, Dariusz and Gnerre, Sante and Burton, Joshua and Shlyakhter, Ilya and Gnirke, Andreas and Malek, Joel and McKernan, Kevin and Ranade, Swati and Shea, Terrance P and Williams, Louise and Young, Sarah and Nusbaum, Chad and Jaffe, David B},
doi = {10.1186/gb-2009-10-10-r103},
file = {:home/gus/Documents/Mendeley Desktop//Maccallum et al.\_2009\_ALLPATHS 2 small genomes assembled accurately and with high continuity from short paired reads.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Bacteria,Bacteria: genetics,Base Pairing,Base Pairing: genetics,Fungi,Fungi: genetics,Genome,Genome: genetics,Genomics,Genomics: methods,Reproducibility of Results,Software},
month = jan,
number = {10},
pages = {R103},
pmid = {19796385},
title = {{ALLPATHS 2: small genomes assembled accurately and with high continuity from short paired reads.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2784318\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Venter2001,
author = {Venter, JC and Adams, MD and Myers, EW},
file = {:home/gus/Documents/Mendeley Desktop/Venter, Adams, Myers\_2001\_The sequence of the human genome.pdf:pdf},
journal = {Science's \ldots},
title = {{The sequence of the human genome}},
url = {http://stke.sciencemag.org/cgi/content/abstract/sci;291/5507/1304},
year = {2001}
}
@article{Li2009,
abstract = {The enormous amount of short reads generated by the new DNA sequencing technologies call for the development of fast and accurate read alignment programs. A first generation of hash table-based methods has been developed, including MAQ, which is accurate, feature rich and fast enough to align short reads from a single individual. However, MAQ does not support gapped alignment for single-end reads, which makes it unsuitable for alignment of longer reads where indels may occur frequently. The speed of MAQ is also a concern when the alignment is scaled up to the resequencing of hundreds of individuals.},
author = {Li, Heng and Durbin, Richard},
doi = {10.1093/bioinformatics/btp324},
file = {:home/gus/Documents/Mendeley Desktop/Li, Durbin\_2009\_Fast and accurate short read alignment with Burrows-Wheeler transform.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Genomics,Genomics: methods,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
month = jul,
number = {14},
pages = {1754--60},
pmid = {19451168},
title = {{Fast and accurate short read alignment with Burrows-Wheeler transform.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/25/14/1754},
volume = {25},
year = {2009}
}
@article{salmela2011correcting,
author = {Salmela, L. and Schr\"{o}der, J.},
file = {:home/gus/Documents/Mendeley Desktop/Salmela, Schr\"{o}der\_2011\_Correcting errors in short reads by multiple alignments.pdf:pdf},
journal = {Bioinformatics},
number = {11},
pages = {1455},
publisher = {Oxford Univ Press},
title = {{Correcting errors in short reads by multiple alignments}},
url = {http://bioinformatics.oxfordjournals.org/content/27/11/1455.short},
volume = {27},
year = {2011}
}
@article{Li2011,
author = {Li, Yingrui and Zheng, Hancheng and Luo, Ruibang and Wu, Honglong and Zhu, Hongmei and Li, Ruiqiang and Cao, Hongzhi and Wu, Boxin and Huang, Shujia and Shao, Haojing and Ma, Hanzhou and Zhang, Fan and Feng, Shuijian and Zhang, Wei and Du, Hongli and Tian, Geng and Li, Jingxiang and Zhang, Xiuqing and Li, Songgang and Bolund, Lars and Kristiansen, Karsten and de Smith, Adam J and Blakemore, Alexandra I F and Coin, Lachlan J M and Yang, Huanming and Wang, Jian and Wang, Jun},
doi = {10.1038/nbt.1904},
file = {:home/gus/Documents/Mendeley Desktop/Li et al.\_2011\_Structural variation in two human genomes mapped at single-nucleotide resolution by whole genome de novo assembly.pdf:pdf},
issn = {1087-0156},
journal = {Nature Biotechnology},
month = jul,
number = {8},
pages = {723--730},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nat Biotech},
title = {{Structural variation in two human genomes mapped at single-nucleotide resolution by whole genome de novo assembly}},
url = {http://dx.doi.org/10.1038/nbt.1904},
volume = {29},
year = {2011}
}
@article{Kawrykow2012,
abstract = {Background
          Comparative genomics, or the study of the relationships of genome structure and function across different species, offers a powerful tool for studying evolution, annotating genomes, and understanding the causes of various genetic disorders. However, aligning multiple sequences of DNA, an essential intermediate step for most types of analyses, is a difficult computational task. In parallel, citizen science, an approach that takes advantage of the fact that the human brain is exquisitely tuned to solving specific types of problems, is becoming increasingly popular. There, instances of hard computational problems are dispatched to a crowd of non-expert human game players and solutions are sent back to a central server.
        
        
          Methodology/Principal Findings
          We introduce Phylo, a human-based computing framework applying crowd sourcing techniques to solve the Multiple Sequence Alignment (MSA) problem. The key idea of Phylo is to convert the MSA problem into a casual game that can be played by ordinary web users with a minimal prior knowledge of the biological context. We applied this strategy to improve the alignment of the promoters of disease-related genes from up to 44 vertebrate species. Since the launch in November 2010, we received more than 350,000 solutions submitted from more than 12,000 registered users. Our results show that solutions submitted contributed to improving the accuracy of up to 70\% of the alignment blocks considered.
        
        
          Conclusions/Significance
          We demonstrate that, combined with classical algorithms, crowd computing techniques can be successfully used to help improving the accuracy of MSA. More importantly, we show that an NP-hard computational problem can be embedded in casual game that can be easily played by people without significant scientific training. This suggests that citizen science approaches can be used to exploit the billions of human-brain peta-flops of computation that are spent every day playing games. Phylo is available at: http://phylo.cs.mcgill.ca.},
author = {Kawrykow, Alexander and Roumanis, Gary and Kam, Alfred and Kwak, Daniel and Leung, Clarence and Wu, Chu and Zarour, Eleyine and Sarmenta, Luis and Blanchette, Mathieu and Waldisp\"{u}hl, J\'{e}r\^{o}me},
doi = {10.1371/journal.pone.0031362},
editor = {Michalak, Pawel},
file = {:home/gus/Documents/Mendeley Desktop/Kawrykow et al.\_2012\_Phylo A Citizen Science Approach for Improving Multiple Sequence Alignment.pdf:pdf},
issn = {1932-6203},
journal = {PLoS ONE},
keywords = {Biology,Computational Biology,Computer Science,Computer applications,Computing methods,Evolutionary Biology,Genetics,Genetics and Genomics,Genomics,Organismal evolution,Research Article},
month = mar,
number = {3},
pages = {e31362},
publisher = {Public Library of Science},
title = {{Phylo: A Citizen Science Approach for Improving Multiple Sequence Alignment}},
url = {http://dx.plos.org/10.1371/journal.pone.0031362},
volume = {7},
year = {2012}
}
@article{Kurtz2008,
abstract = {The challenges of accurate gene prediction and enumeration are further aggravated in large genomes that contain highly repetitive transposable elements (TEs). Yet TEs play a substantial role in genome evolution and are themselves an important subject of study. Repeat annotation, based on counting occurrences of k-mers, has been previously used to distinguish TEs from low-copy genic regions; but currently available software solutions are impractical due to high memory requirements or specialization for specific user-tasks.},
author = {Kurtz, Stefan and Narechania, Apurva and Stein, Joshua C and Ware, Doreen},
doi = {10.1186/1471-2164-9-517},
file = {:home/gus/Documents/Mendeley Desktop/Kurtz et al.\_2008\_A new method to compute K-mer frequencies and its application to annotate large repetitive plant genomes.pdf:pdf},
issn = {1471-2164},
journal = {BMC genomics},
keywords = {Computational Biology,Computational Biology: methods,DNA Transposable Elements,Genome, Plant,Genomics,Genomics: methods,Methods,Oryza sativa,Software,Sorghum,Zea mays},
month = jan,
pages = {517},
pmid = {18976482},
title = {{A new method to compute K-mer frequencies and its application to annotate large repetitive plant genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2613927\&tool=pmcentrez\&rendertype=abstract},
volume = {9},
year = {2008}
}
@inproceedings{Gao2004,
author = {Gao, H. and Groote, J.F. F and Hesselink, W.H. H},
booktitle = {18th International Parallel and Distributed Processing Symposium},
doi = {10.1109/IPDPS.2004.1302969},
file = {:home/gus/Documents/Mendeley Desktop/Gao, Groote, Hesselink\_Unknown\_Almost wait-free resizable hashtables.pdf:pdf},
isbn = {0769521320},
number = {C},
pages = {50--58},
publisher = {Ieee},
title = {{Almost wait-free resizable hashtables}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1302969},
volume = {00},
year = {2004}
}
@article{Chaisson2008,
abstract = {In the last year, high-throughput sequencing technologies have progressed from proof-of-concept to production quality. While these methods produce high-quality reads, they have yet to produce reads comparable in length to Sanger-based sequencing. Current fragment assembly algorithms have been implemented and optimized for mate-paired Sanger-based reads, and thus do not perform well on short reads produced by short read technologies. We present a new Eulerian assembler that generates nearly optimal short read assemblies of bacterial genomes and describe an approach to assemble reads in the case of the popular hybrid protocol when short and long Sanger-based reads are combined.},
author = {Chaisson, Mark J and Pevzner, Pavel A},
doi = {10.1101/gr.7088808},
file = {:home/gus/Documents/Mendeley Desktop/Chaisson, Pevzner\_2008\_Short read fragment assembly of bacterial genomes.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Bacterial,Bacterial: genetics,Contig Mapping,Contig Mapping: methods,DNA,DNA: methods,Genome,Genomics,Genomics: methods,Research Design,Sequence Analysis},
month = feb,
number = {2},
pages = {324--30},
pmid = {18083777},
title = {{Short read fragment assembly of bacterial genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2203630\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@article{Ding2012,
author = {Ding, Li and Ley, Timothy J. and Larson, David E. and Miller, Christopher A. and Koboldt, Daniel C. and Welch, John S. and Ritchey, Julie K. and Young, Margaret A. and Lamprecht, Tamara and McLellan, Michael D. and McMichael, Joshua F. and Wallis, John W. and Lu, Charles and Shen, Dong and Harris, Christopher C. and Dooling, David J. and Fulton, Robert S. and Fulton, Lucinda L. and Chen, Ken and Schmidt, Heather and Kalicki-Veizer, Joelle and Magrini, Vincent J. and Cook, Lisa and McGrath, Sean D. and Vickery, Tammi L. and Wendl, Michael C. and Heath, Sharon and Watson, Mark A. and Link, Daniel C. and Tomasson, Michael H. and Shannon, William D. and Payton, Jacqueline E. and Kulkarni, Shashikant and Westervelt, Peter and Walter, Matthew J. and Graubert, Timothy A. and Mardis, Elaine R. and Wilson, Richard K. and DiPersio, John F.},
doi = {10.1038/nature10738},
file = {:home/gus/Documents/Mendeley Desktop/Ding et al.\_2012\_Clonal evolution in relapsed acute myeloid leukaemia revealed by whole-genome sequencing.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
month = jan,
number = {7382},
pages = {506--510},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nature},
title = {{Clonal evolution in relapsed acute myeloid leukaemia revealed by whole-genome sequencing}},
url = {http://dx.doi.org/10.1038/nature10738},
volume = {481},
year = {2012}
}
@article{Jaffe2003,
abstract = {We previously described the whole-genome assembly program Arachne, presenting assemblies of simulated data for small to mid-sized genomes. Here we describe algorithmic adaptations to the program, allowing for assembly of mammalian-size genomes, and also improving the assembly of smaller genomes. Three principal changes were simultaneously made and applied to the assembly of the mouse genome, during a six-month period of development: (1) Supercontigs (scaffolds) were iteratively broken and rejoined using several criteria, yielding a 64-fold increase in length (N50), and apparent elimination of all global misjoins; (2) gaps between contigs in supercontigs were filled (partially or completely) by insertion of reads, as suggested by pairing within the supercontig, increasing the N50 contig length by 50\%; (3) memory usage was reduced fourfold. The outcome of this mouse assembly and its analysis are described in (Mouse Genome Sequencing Consortium 2002).},
author = {Jaffe, David B and Butler, Jonathan and Gnerre, Sante and Mauceli, Evan and Lindblad-Toh, Kerstin and Mesirov, Jill P and Zody, Michael C and Lander, Eric S},
doi = {10.1101/gr.828403},
file = {:home/gus/Documents/Mendeley Desktop/Jaffe et al.\_2003\_Whole-genome sequence assembly for mammalian genomes Arachne 2.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Animals,Computational Biology,Computational Biology: methods,Contig Mapping,Contig Mapping: methods,Genome,Humans,Mice,Software},
month = jan,
number = {1},
pages = {91--6},
pmid = {12529310},
title = {{Whole-genome sequence assembly for mammalian genomes: Arachne 2.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=430950\&tool=pmcentrez\&rendertype=abstract},
volume = {13},
year = {2003}
}
@article{Simpson2010,
abstract = {MOTIVATION: Sequence assembly is a difficult problem whose importance has grown again recently as the cost of sequencing has dramatically dropped. Most new sequence assembly software has started by building a de Bruijn graph, avoiding the overlap-based methods used previously because of the computational cost and complexity of these with very large numbers of short reads. Here, we show how to use suffix array-based methods that have formed the basis of recent very fast sequence mapping algorithms to find overlaps and generate assembly string graphs asymptotically faster than previously described algorithms. RESULTS: Standard overlap assembly methods have time complexity O(N(2)), where N is the sum of the lengths of the reads. We use the Ferragina-Manzini index (FM-index) derived from the Burrows-Wheeler transform to find overlaps of length at least tau among a set of reads. As well as an approach that finds all overlaps then implements transitive reduction to produce a string graph, we show how to output directly only the irreducible overlaps, significantly shrinking memory requirements and reducing compute time to O(N), independent of depth. Overlap-based assembly methods naturally handle mixed length read sets, including capillary reads or long reads promised by the third generation sequencing technologies. The algorithms we present here pave the way for overlap-based assembly approaches to be developed that scale to whole vertebrate genome de novo assembly.},
author = {Simpson, Jared T and Durbin, Richard},
doi = {10.1093/bioinformatics/btq217},
file = {:home/gus/Documents/Mendeley Desktop/Simpson, Durbin\_2010\_Efficient construction of an assembly string graph using the FM-index.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Genomics,Genomics: methods,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Software},
month = jun,
number = {12},
pages = {i367--73},
pmid = {20529929},
title = {{Efficient construction of an assembly string graph using the FM-index.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/26/12/i367},
volume = {26},
year = {2010}
}
@article{Ladan-Mozes2007,
author = {Ladan-Mozes, Edya and Shavit, Nir},
doi = {10.1007/s00446-007-0050-0},
file = {:home/gus/Documents/Mendeley Desktop/Ladan-Mozes, Shavit\_2007\_An optimistic approach to lock-free FIFO queues.pdf:pdf},
issn = {0178-2770},
journal = {Distributed Computing},
month = nov,
number = {5},
pages = {323--341},
title = {{An optimistic approach to lock-free FIFO queues}},
url = {http://www.springerlink.com/index/10.1007/s00446-007-0050-0},
volume = {20},
year = {2007}
}
@article{dalloul2010multi,
author = {Dalloul, R.A. and Long, J.A. and Zimin, A.V. and Aslam, L. and Beal, K. and Bouffard, P. and Burt, D.W. and Crasta, O. and Crooijmans, R.P.M.A. and Cooper, K. and Others},
file = {:home/gus/Documents/Mendeley Desktop/Dalloul et al.\_2010\_Multi-platform next-generation sequencing of the domestic turkey (Meleagris gallopavo) genome assembly and analysis.pdf:pdf},
journal = {PLoS biology},
number = {9},
pages = {e1000475},
title = {{Multi-platform next-generation sequencing of the domestic turkey (Meleagris gallopavo): genome assembly and analysis}},
url = {http://dx.plos.org/10.1371/journal.pbio.1000475},
volume = {8},
year = {2010}
}
@article{Bloom1970,
author = {Bloom, Burton H.},
doi = {10.1145/362686.362692},
file = {:home/gus/Documents/Mendeley Desktop/Bloom\_1970\_Spacetime trade-offs in hash coding with allowable errors.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {Bloom filter,hash addressing,hash coding,retrieval efficiency,retrieval trade-offs,scatter storage,searching,storage efficiency,storage layout},
mendeley-tags = {Bloom filter},
month = jul,
number = {7},
pages = {422--426},
title = {{Space/time trade-offs in hash coding with allowable errors}},
url = {http://dl.acm.org/citation.cfm?id=362686.362692},
volume = {13},
year = {1970}
}
@inproceedings{michael2002safe,
author = {Michael, M.M.},
booktitle = {Proceedings of the twenty-first annual symposium on Principles of distributed computing},
file = {:home/gus/Documents/Mendeley Desktop/Michael\_2002\_Safe memory reclamation for dynamic lock-free objects using atomic reads and writes.pdf:pdf},
pages = {21--30},
publisher = {ACM},
title = {{Safe memory reclamation for dynamic lock-free objects using atomic reads and writes}},
url = {http://portal.acm.org/citation.cfm?id=571829},
year = {2002}
}
@article{Miller1998,
author = {Miller, Peter},
file = {:home/gus/Documents/Mendeley Desktop/Miller\_1998\_Recursive make considered harmful.pdf:pdf},
journal = {AUUGN Journal of AUUG Inc},
number = {1},
pages = {14--25},
title = {{Recursive make considered harmful}},
url = {http://www.unix-ag.uni-kl.de/svn/kbibtex/kbibtex/tags/release-0.1/admin/unsermake/doc/auug97.pdf},
volume = {19},
year = {1998}
}
@article{Chaisson2008,
abstract = {In the last year, high-throughput sequencing technologies have progressed from proof-of-concept to production quality. While these methods produce high-quality reads, they have yet to produce reads comparable in length to Sanger-based sequencing. Current fragment assembly algorithms have been implemented and optimized for mate-paired Sanger-based reads, and thus do not perform well on short reads produced by short read technologies. We present a new Eulerian assembler that generates nearly optimal short read assemblies of bacterial genomes and describe an approach to assemble reads in the case of the popular hybrid protocol when short and long Sanger-based reads are combined.},
author = {Chaisson, Mark J and Pevzner, Pavel A},
doi = {10.1101/gr.7088808},
file = {:home/gus/Documents/Mendeley Desktop/Chaisson, Pevzner\_2008\_Short read fragment assembly of bacterial genomes.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Bacterial,Bacterial: genetics,Contig Mapping,Contig Mapping: methods,DNA,DNA: methods,Genome,Genomics,Genomics: methods,Research Design,Sequence Analysis},
month = feb,
number = {2},
pages = {324--30},
pmid = {18083777},
title = {{Short read fragment assembly of bacterial genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2203630\&tool=pmcentrez\&rendertype=abstract},
volume = {18},
year = {2008}
}
@article{Li2003,
abstract = {In shotgun sequencing projects, the genome or BAC length is not always known. We approach estimating genome length by first estimating the repeat structure of the genome or BAC, sometimes of interest in its own right, on the basis of a set of random reads from a genome project. Moreover, we can find the consensus for repeat families before assembly. Our methods are based on the l-tuple content of the reads.},
author = {Li, Xiaoman and Waterman, Michael S},
doi = {10.1101/gr.1251803},
file = {:home/gus/Documents/Mendeley Desktop/Li, Waterman\_2003\_Estimating the repeat structure and length of DNA sequences using L-tuples.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Artificial,Bacterial,Bacterial: genetics,Base Composition,Chromosome Mapping,Chromosome Mapping: statistics \& numerical data,Chromosomes,Computational Biology,Computational Biology: statistics \& numerical data,Computer Simulation,Computer Simulation: statistics \& numerical data,Consensus Sequence,DNA,Mathematical Computing,Nucleic Acid,Nucleic Acid: genetics,Poisson Distribution,Repetitive Sequences,Software},
month = aug,
number = {8},
pages = {1916--22},
pmid = {12902383},
title = {{Estimating the repeat structure and length of DNA sequences using L-tuples.}},
url = {http://genome.cshlp.org/cgi/content/abstract/13/8/1916},
volume = {13},
year = {2003}
}
@article{Medvedev2011,
abstract = {The continuing improvements to high-throughput sequencing (HTS) platforms have begun to unfold a myriad of new applications. As a result, error correction of sequencing reads remains an important problem. Though several tools do an excellent job of correcting datasets where the reads are sampled close to uniformly, the problem of correcting reads coming from drastically non-uniform datasets, such as those from single-cell sequencing, remains open.},
author = {Medvedev, Paul and Scott, Eric and Kakaradov, Boyko and Pevzner, Pavel},
doi = {10.1093/bioinformatics/btr208},
file = {:home/gus/Documents/Mendeley Desktop/Medvedev et al.\_2011\_Error correction of high-throughput sequencing datasets with non-uniform coverage.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = jul,
number = {13},
pages = {i137--i141},
pmid = {21685062},
title = {{Error correction of high-throughput sequencing datasets with non-uniform coverage.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3117386\&tool=pmcentrez\&rendertype=abstract},
volume = {27},
year = {2011}
}
@inproceedings{Chazelle:2004:BFE:982792.982797,
address = {Philadelphia, PA, USA},
author = {Chazelle, Bernard and Kilian, Joe and Rubinfeld, Ronitt and Tal, Ayellet},
booktitle = {Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms},
file = {:home/gus/Documents/Mendeley Desktop/Chazelle et al.\_2004\_The Bloomier filter an efficient data structure for static support lookup tables.pdf:pdf},
isbn = {0-89871-558-X},
pages = {30--39},
publisher = {Society for Industrial and Applied Mathematics},
series = {SODA '04},
title = {{The Bloomier filter: an efficient data structure for static support lookup tables}},
url = {http://dl.acm.org/citation.cfm?id=982797 http://dl.acm.org/citation.cfm?id=982792.982797},
year = {2004}
}
@article{Michael1996,
address = {New York, New York, USA},
author = {Michael, Maged M. and Scott, Michael L.},
doi = {10.1145/248052.248106},
file = {:home/gus/Documents/Mendeley Desktop/Michael, Scott\_1996\_Simple, fast, and practical non-blocking and blocking concurrent queue algorithms.pdf:pdf},
isbn = {0897918002},
journal = {Proceedings of the fifteenth annual ACM symposium on Principles of distributed computing - PODC '96},
keywords = {compare and swap,concurrent queue,lock-free,multiprogramming,non-blocking},
pages = {267--275},
publisher = {ACM Press},
title = {{Simple, fast, and practical non-blocking and blocking concurrent queue algorithms}},
url = {http://portal.acm.org/citation.cfm?doid=248052.248106},
year = {1996}
}
@article{Pugh1990,
author = {Pugh, William},
doi = {10.1145/78973.78977},
file = {:home/gus/Documents/Mendeley Desktop//Pugh\_1990\_Skip lists a probabilistic alternative to balanced trees.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {data structures,searching,trees},
month = jun,
number = {6},
pages = {668--676},
publisher = {ACM},
title = {{Skip lists: a probabilistic alternative to balanced trees}},
url = {http://dl.acm.org/citation.cfm?id=78973.78977 http://dl.acm.org/citation.cfm?id=78977,},
volume = {33},
year = {1990}
}
@article{Pevzner:2001uq,
abstract = {For the last 20 years, fragment assembly in DNA sequencing followed the "overlap-layout-consensus" paradigm that is used in all currently available assembly tools. Although this approach proved useful in assembling clones, it faces difficulties in genomic shotgun assembly. We abandon the classical "overlap-layout-consensus" approach in favor of a new euler algorithm that, for the first time, resolves the 20-year-old "repeat problem" in fragment assembly. Our main result is the reduction of the fragment assembly to a variation of the classical Eulerian path problem that allows one to generate accurate solutions of large-scale sequencing problems. euler, in contrast to the celera assembler, does not mask such repeats but uses them instead as a powerful fragment assembly tool.},
author = {Pevzner, P A and Tang, H and Waterman, M S},
doi = {10.1073/pnas.171285098},
file = {:home/gus/Documents/Mendeley Desktop/Pevzner, Tang, Waterman\_2001\_An Eulerian path approach to DNA fragment assembly.pdf:pdf},
journal = {Proc Natl Acad Sci U S A},
month = aug,
number = {17},
pages = {9748--9753},
pmid = {11504945},
title = {{An Eulerian path approach to DNA fragment assembly}},
volume = {98},
year = {2001}
}
@article{Venter2001,
author = {Venter, JC and Adams, MD and Myers, EW},
file = {:home/gus/Documents/Mendeley Desktop/Venter, Adams, Myers\_2001\_The sequence of the human genome.pdf:pdf},
journal = {Science's \ldots},
title = {{The sequence of the human genome}},
url = {http://stke.sciencemag.org/cgi/content/abstract/sci;291/5507/1304},
year = {2001}
}
@article{Schroder2009,
abstract = {MOTIVATION: Second-generation sequencing technologies produce a massive amount of short reads in a single experiment. However, sequencing errors can cause major problems when using this approach for de novo sequencing applications. Moreover, existing error correction methods have been designed and optimized for shotgun sequencing. Therefore, there is an urgent need for the design of fast and accurate computational methods and tools for error correction of large amounts of short read data. RESULTS: We present SHREC, a new algorithm for correcting errors in short-read data that uses a generalized suffix trie on the read data as the underlying data structure. Our results show that the method can identify erroneous reads with sensitivity and specificity of over 99\% and 96\% for simulated data with error rates of up to 3\% as well as for real data. Furthermore, it achieves an error correction accuracy of over 80\% for simulated data and over 88\% for real data. These results are clearly superior to previously published approaches. SHREC is available as an efficient open-source Java implementation that allows processing of 10 million of short reads on a standard workstation.},
author = {Schr\"{o}der, Jan and Schr\"{o}der, Heiko and Puglisi, Simon J and Sinha, Ranjan and Schmidt, Bertil},
doi = {10.1093/bioinformatics/btp379},
file = {:home/gus/Documents/Mendeley Desktop/Schr\"{o}der et al.\_2009\_SHREC a short-read error correction method.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Schr\"{o}der et al.\_2009\_SHREC a short-read error correction method(2).pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,DNA,DNA: genetics,DNA: methods,Databases,Error correction,Genome,Genome: genetics,Nucleic Acid,Research Design,Sequence Analysis,Time Factors},
mendeley-tags = {Error correction},
month = sep,
number = {17},
pages = {2157--63},
pmid = {19542152},
title = {{SHREC: a short-read error correction method.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/25/17/2157},
volume = {25},
year = {2009}
}
@article{Richter2008,
abstract = {The new research field of metagenomics is providing exciting insights into various, previously unclassified ecological systems. Next-generation sequencing technologies are producing a rapid increase of environmental data in public databases. There is great need for specialized software solutions and statistical methods for dealing with complex metagenome data sets.},
author = {Richter, Daniel C and Ott, Felix and Auch, Alexander F and Schmid, Ramona and Huson, Daniel H},
doi = {10.1371/journal.pone.0003373},
editor = {Field, Dawn},
file = {:home/gus/Documents/Mendeley Desktop/Richter et al.\_2008\_MetaSim a sequencing simulator for genomics and metagenomics.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
keywords = {Databases, Genetic,Genomics,Models, Theoretical,User-Computer Interface},
month = jan,
number = {10},
pages = {e3373},
pmid = {18841204},
publisher = {Public Library of Science},
title = {{MetaSim: a sequencing simulator for genomics and metagenomics.}},
url = {http://dx.plos.org/10.1371/journal.pone.0003373},
volume = {3},
year = {2008}
}
@article{Brown2012,
abstract = {Deep shotgun sequencing and analysis of genomes, transcriptomes, amplified single-cell genomes, and metagenomes enable the sensitive investigation of a wide range of biological phenomena. However, it is increasingly difficult to deal with the volume of data emerging from deep short-read sequencers, in part because of random and systematic sampling variation as well as a high sequencing error rate. These challenges have led to the development of entire new classes of short-read mapping tools, as well as new \{$\backslash$em de novo\} assemblers. Even newer assembly strategies for dealing with transcriptomes, single-cell genomes, and metagenomes have also emerged. Despite these advances, algorithms and compute capacity continue to be challenged by the continued improvements in sequencing technology throughput. We here describe an approach we term digital normalization, a single-pass computational algorithm that discards redundant data and both sampling variation and the number of errors present in deep sequencing data sets. Digital normalization substantially reduces the size of data sets and accordingly decreases the memory and time requirements for \{$\backslash$em de novo\} sequence assembly, all without significantly impacting content of the generated contigs. In doing so, it converts high random coverage to low systematic coverage. Digital normalization is an effective and efficient approach to normalizing coverage, removing errors, and reducing data set size for shotgun sequencing data sets. It is particularly useful for reducing the compute requirements for \{$\backslash$em de novo\} sequence assembly. We demonstrate this for the assembly of microbial genomes, amplified single-cell genomic data, and transcriptomic data. The software is freely available for use and modification.},
archivePrefix = {arXiv},
arxivId = {1203.4802},
author = {Brown, C. Titus and Howe, Adina and Zhang, Qingpeng and Pyrkosz, Alexis B. and Brom, Timothy H.},
eprint = {1203.4802},
file = {:home/gus/Documents/Mendeley Desktop/Brown et al.\_2012\_A single pass approach to reducing sampling variation, removing errors, and scaling \{em de novo\} assembly of shotgun sequences.pdf:pdf},
month = mar,
title = {{A single pass approach to reducing sampling variation, removing errors, and scaling \{$\backslash$em de novo\} assembly of shotgun sequences}},
url = {http://arxiv.org/abs/1203.4802},
year = {2012}
}
@article{Salzberg2011,
abstract = {New sequencing technology has dramatically altered the landscape of whole-genome sequencing, allowing scientists to initiate numerous projects to decode the genomes of previously unsequenced organisms. The lowest-cost technology can generate deep coverage of most species, including mammals, in just a few days. The sequence data generated by one of these projects consists of millions or billions of short DNA sequences (reads) that range from 50-150 nucleotides in length. These sequences must then be assembled de novo before most genome analyses can begin. Unfortunately, genome assembly remains a very difficult problem, made more difficult by shorter reads and unreliable long-range linking information. In this study, we evaluated several of the leading de novo assembly algorithms on four different short-read data sets, all generated by Illumina sequencers. Our results describe the relative performance of the different assemblers as well as other significant differences in assembly difficulty that appear to be inherent in the genomes themselves. Three overarching conclusions are apparent: first, that data quality, rather than the assembler itself, has a dramatic affect on the quality of an assembled genome; second, that the degree of contiguity of an assembly varies enormously among different assemblers and different genomes; and third, that the correctness of an assembly also varies widely, and is not well correlated with statistics on contiguity. In order to enable others to replicate our results, all of our data and methods are freely available, as are all assemblers used in this study.},
author = {Salzberg, S. L. and Phillippy, A. M. and Zimin, A. V. and Puiu, D. and Magoc, T. and Koren, S. and Treangen, T. and Schatz, M. C. and Delcher, A. L. and Roberts, M. and Marcais, G. and Pop, M. and Yorke, J. A.},
doi = {10.1101/gr.131383.111},
file = {:home/gus/Documents/Mendeley Desktop/Salzberg et al.\_2011\_GAGE A critical evaluation of genome assemblies and assembly algorithms(2).pdf:pdf},
issn = {1088-9051},
journal = {Genome Research},
month = dec,
pages = {gr.131383.111--},
title = {{GAGE: A critical evaluation of genome assemblies and assembly algorithms}},
url = {http://genome.cshlp.org/cgi/content/abstract/gr.131383.111v1},
year = {2011}
}
@article{Nagaraj2007,
abstract = {Expressed sequence tag (EST) sequencing projects are underway for numerous organisms, generating millions of short, single-pass nucleotide sequence reads, accumulating in EST databases. Extensive computational strategies have been developed to organize and analyse both small- and large-scale EST data for gene discovery, transcript and single nucleotide polymorphism analysis as well as functional annotation of putative gene products. We provide an overview of the significance of ESTs in the genomic era, their properties and the applications of ESTs. Methods adopted for each step of EST analysis by various research groups have been compared. Challenges that lie ahead in organizing and analysing the ever increasing EST data have also been identified. The most appropriate software tools for EST pre-processing, clustering and assembly, database matching and functional annotation have been compiled (available online from http://biolinfo.org/EST). We propose a road map for EST analysis to accelerate the effective analyses of EST data sets. An investigation of EST analysis platforms reveals that they all terminate prior to downstream functional annotation including gene ontologies, motif/pattern analysis and pathway mapping.},
author = {Nagaraj, Shivashankar H and Gasser, Robin B and Ranganathan, Shoba},
doi = {10.1093/bib/bbl015},
file = {:home/gus/Documents/Mendeley Desktop/Nagaraj, Gasser, Ranganathan\_2007\_A hitchhiker's guide to expressed sequence tag (EST) analysis.pdf:pdf;:home/gus/Documents/Mendeley Desktop/Nagaraj, Gasser, Ranganathan\_2007\_A hitchhiker's guide to expressed sequence tag (EST) analysis(2).pdf:pdf},
issn = {1467-5463},
journal = {Briefings in bioinformatics},
keywords = {Animals,Complementary,Complementary: chemistry,Consensus Sequence,Consensus Sequence: genetics,DNA,Databases,Expressed Sequence Tags,Genetic,Genetic: standards,Genomics,Genomics: methods,Humans,Nucleic Acid,Nucleic Acid: standards,RNA,Software,Untranslated Regions,Untranslated Regions: chemistry},
mendeley-tags = {RNA},
month = jan,
number = {1},
pages = {6--21},
pmid = {16772268},
title = {{A hitchhiker's guide to expressed sequence tag (EST) analysis.}},
url = {http://bib.oxfordjournals.org/cgi/content/abstract/8/1/6},
volume = {8},
year = {2007}
}
@article{Zimin2009,
abstract = {The genome of the domestic cow, Bos taurus, was sequenced using a mixture of hierarchical and whole-genome shotgun sequencing methods.},
author = {Zimin, Aleksey V and Delcher, Arthur L and Florea, Liliana and Kelley, David R and Schatz, Michael C and Puiu, Daniela and Hanrahan, Finnian and Pertea, Geo and {Van Tassell}, Curtis P and Sonstegard, Tad S and Mar\c{c}ais, Guillaume and Roberts, Michael and Subramanian, Poorani and Yorke, James a and Salzberg, Steven L},
doi = {10.1186/gb-2009-10-4-r42},
file = {:home/gus/Documents/Mendeley Desktop/Zimin et al.\_2009\_A whole-genome assembly of the domestic cow, Bos taurus.pdf:pdf},
issn = {1465-6914},
journal = {Genome biology},
keywords = {Animals,Cattle,Cattle: genetics,Chromosome Mapping,Female,Genome,Genome, Human,Genome, Human: genetics,Genome: genetics,Genomics,Humans,Male,Sequence Analysis, DNA,Sequence Analysis, DNA: methods,Sequence Analysis, DNA: statistics \& numerical dat,Synteny,Y Chromosome,Y Chromosome: genetics},
month = jan,
number = {4},
pages = {R42},
pmid = {19393038},
title = {{A whole-genome assembly of the domestic cow, Bos taurus.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2688933\&tool=pmcentrez\&rendertype=abstract},
volume = {10},
year = {2009}
}
@article{Kelley:2010fk,
abstract = {ABSTRACT : We introduce Quake, a program to detect and correct errors in DNA sequencing reads. Using a maximum likelihood approach incorporating quality values and nucleotide specific miscall rates, Quake achieves the highest accuracy on realistically simulated reads. We further demonstrate substantial improvements in de novo assembly and SNP detection after using Quake. Quake can be used for any size project, including more than one billion human reads, and is freely available as open source software from http://www.cbcb.umd.edu/software/quake.},
author = {Kelley, David R and Schatz, Michael C and Salzberg, Steven L},
doi = {10.1186/gb-2010-11-11-r116},
file = {:home/gus/Documents/Mendeley Desktop/Kelley, Schatz, Salzberg\_2010\_Quake quality-aware detection and correction of sequencing errors.pdf:pdf},
journal = {Genome Biol},
number = {11},
pages = {R116},
pmid = {21114842},
title = {{Quake: quality-aware detection and correction of sequencing errors}},
volume = {11},
year = {2010}
}
@article{shi2010parallel,
author = {Shi, H. and Schmidt, B. and Liu, W. and M\"{u}ller-Wittig, W.},
file = {:home/gus/Documents/Mendeley Desktop/Shi et al.\_2010\_A parallel algorithm for error correction in high-throughput short-read data on CUDA-enabled graphics hardware.pdf:pdf},
journal = {Journal of Computational Biology},
number = {4},
pages = {603--615},
publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
title = {{A parallel algorithm for error correction in high-throughput short-read data on CUDA-enabled graphics hardware}},
url = {http://www.liebertonline.com/doi/abs/10.1089/cmb.2009.0062},
volume = {17},
year = {2010}
}
@article{Gibbs2012,
author = {Gibbs, Richard A and Rogers, Jeffrey},
doi = {10.1038/483164a},
file = {:home/gus/Documents/Mendeley Desktop/Gibbs, Rogers\_2012\_Genomics Gorilla gorilla gorilla.pdf:pdf},
issn = {1476-4687},
journal = {Nature},
month = mar,
number = {7388},
pages = {164--5},
pmid = {22398552},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
shorttitle = {Nature},
title = {{Genomics: Gorilla gorilla gorilla.}},
url = {http://dx.doi.org/10.1038/483164a},
volume = {483},
year = {2012}
}
@article{Ruffalo2011,
abstract = {Motivation: The advent of next-generation sequencing (NGS) techniques presents many novel opportunities for many applications in life sciences. The vast number of short reads produced by these techniques, however, pose significant computational challenges. The first step in many types of genomic analysis is the mapping of short reads to a reference genome, and several groups have developed dedicated algorithms and software packages to perform this function. As the developers of these packages optimize their algorithms with respect to various considerations, the relative merits of different software packages remain unclear. However, for scientists who generate and use NGS data for their specific research projects, an important consideration is choosing the software that is most suitable for their application. Results: With a view to comparing existing short read alignment software, we develop a simulation and evaluation suite, SEAL, which simulates NGS runs for different configurations of various factors, including sequencing error, indels and coverage. We also develop criteria to compare the performances of software with disparate output structure (e.g. some packages return a single alignment while some return multiple possible alignments). Using these criteria, we comprehensively evaluate the performances of Bowtie, BWA, mr- and mrsFAST, Novoalign, SHRiMP and SOAPv2, with regard to accuracy and runtime. Conclusion: We expect that the results presented here will be useful to investigators in choosing the alignment software that is most suitable for their specific research aims. Our results also provide insights into the factors that should be considered to use alignment results effectively. SEAL can also be used to evaluate the performance of algorithms that use deep sequencing data for various purposes (e.g. identification of genomic variants). Availability: SEAL is available as open source at http://compbio.case.edu/seal/. Contact: matthew.ruffalo@case.edu Supplementary information: Supplementary data are available at Bioinformatics online.},
author = {Ruffalo, M. and LaFramboise, T. and Koyuturk, M.},
doi = {10.1093/bioinformatics/btr477},
file = {:home/gus/Documents/Mendeley Desktop/Ruffalo, LaFramboise, Koyuturk\_2011\_Comparative analysis of algorithms for next-generation sequencing read alignment.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = aug,
number = {20},
pages = {2790--2796},
title = {{Comparative analysis of algorithms for next-generation sequencing read alignment}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/27/20/2790},
volume = {27},
year = {2011}
}
@article{Panda2010,
annote = {10.1038/nature08696},
author = {Li, Ruiqiang and Fan, Wei and Tian, Geng and Zhu, Hongmei and He, Lin and Cai, Jing and Huang, Quanfei and Cai, Qingle and Li, Bo and Bai, Yinqi and Zhang, Zhihe and Zhang, Yaping and Wang, Wen and Li, Jun and Wei, Fuwen and Li, Heng and Jian, Min and Li, Jianwen and Zhang, Zhaolei and Nielsen, Rasmus and Li, Dawei and Gu, Wanjun and Yang, Zhentao and Xuan, Zhaoling and Ryder, Oliver A and Leung, Frederick Chi-Ching and Zhou, Yan and Cao, Jianjun and Sun, Xiao and Fu, Yonggui and Fang, Xiaodong and Guo, Xiaosen and Wang, Bo and Hou, Rong and Shen, Fujun and Mu, Bo and Ni, Peixiang and Lin, Runmao and Qian, Wubin and Wang, Guodong and Yu, Chang and Nie, Wenhui and Wang, Jinhuan and Wu, Zhigang and Liang, Huiqing and Min, Jiumeng and Wu, Qi and Cheng, Shifeng and Ruan, Jue and Wang, Mingwei and Shi, Zhongbin and Wen, Ming and Liu, Binghang and Ren, Xiaoli and Zheng, Huisong and Dong, Dong and Cook, Kathleen and Shan, Gao and Zhang, Hao and Kosiol, Carolin and Xie, Xueying and Lu, Zuhong and Zheng, Hancheng and Li, Yingrui and Steiner, Cynthia C and Lam, Tommy Tsan-Yuk and Lin, Siyuan and Zhang, Qinghui and Li, Guoqing and Tian, Jing and Gong, Timing and Liu, Hongde and Zhang, Dejin and Fang, Lin and Ye, Chen and Zhang, Juanbin and Hu, Wenbo and Xu, Anlong and Ren, Yuanyuan and Zhang, Guojie and Bruford, Michael W and Li, Qibin and Ma, Lijia and Guo, Yiran and An, Na and Hu, Yujie and Zheng, Yang and Shi, Yongyong and Li, Zhiqiang and Liu, Qing and Chen, Yanling and Zhao, Jing and Qu, Ning and Zhao, Shancen and Tian, Feng and Wang, Xiaoling and Wang, Haiyin and Xu, Lizhi and Liu, Xiao and Vinar, Tomas and Wang, Yajun and Lam, Tak-Wah and Yiu, Siu-Ming and Liu, Shiping and Zhang, Hemin and Li, Desheng and Huang, Yan and Wang, Xia and Yang, Guohua and Jiang, Zhi and Wang, Junyi and Qin, Nan and Li, Li and Li, Jingxiang and Bolund, Lars and Kristiansen, Karsten and Wong, Gane Ka-Shu and Olson, Maynard and Zhang, Xiuqing and Li, Songgang and Yang, Huanming and Wang, Jian and Wang, Jun},
doi = {http://dx.doi.org/10.1038/nature08696},
issn = {0028-0836},
journal = {Nature},
number = {7279},
pages = {311--317},
publisher = {Macmillan Publishers Limited. All rights reserved},
title = {{The sequence and de novo assembly of the giant panda genome}},
volume = {463},
year = {2010}
}
@article{Liu2012,
abstract = {SUMMARY: SOAP3 is the first short read alignment tool that leverages the multi-processors in a graphic processing unit (GPU) to achieve a drastic improvement in speed. We adapted the compressed full-text index (BWT) used by SOAP2 in view of the advantages and disadvantages of GPU. When tested with millions of Illumina Hiseq 2000 length-100bp reads, SOAP3 takes less than 30 seconds to align a million read pairs onto the human reference genome and is at least 7.5 and 20 times faster than BWA and Bowtie, respectively. For aligning reads with up to 4 mismatches, SOAP3 aligns slightly more reads than BWA and Bowtie; this is because SOAP3, unlike BWA and Bowtie, is not heuristic-based and always reports all answers. AVAILABILITY: SOAP3 is available at: http://www.cs.hku.hk/2bwt-tools/soap3; and http://soap.genomics.org.cn/soap3.html. CONTACT: liruiqiang@gmail.com, twlam@cs.hku.hk.},
author = {Liu, Chi-Man and Wong, Thomas and Wu, Edward and Luo, Ruibang and Yiu, Siu-Ming and Li, Yingrui and Wang, B and Yu, C and Chu, X and Zhao, K and Li, Ruiqiang and Lam, Tak-Wah},
doi = {10.1093/bioinformatics/bts061},
file = {:home/gus/Documents/Mendeley Desktop/Liu et al.\_2012\_SOAP3 Ultra-fast GPU-based parallel alignment tool for short reads.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = jan,
number = {6},
pages = {878--879},
pmid = {22285832},
title = {{SOAP3: Ultra-fast GPU-based parallel alignment tool for short reads.}},
url = {http://bioinformatics.oxfordjournals.org/cgi/content/abstract/28/6/878},
volume = {28},
year = {2012}
}
@article{David2011,
abstract = {We report on a major update (version 2) of the original SHort Read Mapping Program (SHRiMP). SHRiMP2 primarily targets mapping sensitivity, and is able to achieve high accuracy at a very reasonable speed. SHRiMP2 supports both letter space and color space (AB/SOLiD) reads, enables for direct alignment of paired reads and uses parallel computation to fully utilize multi-core architectures. AVAILABILITY: SHRiMP2 executables and source code are freely available at: http://compbio.cs.toronto.edu/shrimp/.},
author = {David, Matei and Dzamba, Misko and Lister, Dan and Ilie, Lucian and Brudno, Michael},
doi = {10.1093/bioinformatics/btr046},
file = {:home/gus/Documents/Mendeley Desktop/David et al.\_2011\_SHRiMP2 sensitive yet practical SHort Read Mapping.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Chromosome Mapping,DNA,Genetic,Genomics,Genomics: methods,Polymorphism,Sequence Alignment,Sequence Analysis,Software},
month = apr,
number = {7},
pages = {1011--2},
pmid = {21278192},
title = {{SHRiMP2: sensitive yet practical SHort Read Mapping.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21278192},
volume = {27},
year = {2011}
}
